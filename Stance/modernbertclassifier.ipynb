{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0589258e",
   "metadata": {},
   "source": [
    "# Package Installation\n",
    "\n",
    "This cell installs all required Python packages and dependencies for the notebook. It includes:\n",
    "\n",
    "- **ipywidgets**: Interactive widgets for Jupyter notebooks\n",
    "- **google-genai**: Google's Generative AI SDK for accessing Gemini models\n",
    "- **transformers, accelerate, peft**: Hugging Face libraries for transformer models, acceleration, and Parameter-Efficient Fine-Tuning (LoRA)\n",
    "- **langdetect**: Language detection library for filtering non-English posts\n",
    "- **tqdm**: Progress bars for long-running operations\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **huggingface_hub**: Interface to Hugging Face model hub\n",
    "- **optuna**: Hyperparameter optimization framework\n",
    "- **bitsandbytes**: Quantization library for efficient model loading (4-bit, 8-bit)\n",
    "- **flash_attn**: Flash Attention implementation for faster training\n",
    "\n",
    "All packages are installed with the `-U` flag to ensure the latest versions are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac37768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 60984,
     "status": "ok",
     "timestamp": 1769035523470,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "2ac37768",
    "outputId": "9752ffbb-4595-448c-f4cc-1c8fc6b8c293"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install -U ipywidgets\n",
    "!pip install -U google-genai\n",
    "!pip install -q transformers accelerate peft langdetect tqdm pandas huggingface_hub\n",
    "!pip install -U optuna\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U flash_attn\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdeebe25",
   "metadata": {},
   "source": [
    "# Hugging Face Token Setup and Warnings Configuration\n",
    "\n",
    "This cell performs two important setup tasks:\n",
    "\n",
    "1. **Hugging Face Authentication**: Sets up authentication with Hugging Face Hub using `HfFolder.save_token()`. You need to provide your Hugging Face API token here to download models and push trained models to the hub.\n",
    "\n",
    "2. **Warning Suppression**: Configures Python's warning system to ignore specific warnings from `torch.utils.checkpoint`, which can be verbose during training and don't affect functionality. This keeps the output cleaner and easier to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e36a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1769035523724,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "757e36a2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(\"\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    module=\"torch.utils.checkpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed002e0",
   "metadata": {},
   "source": [
    "# Core Library Imports\n",
    "\n",
    "This cell imports all the essential libraries needed for the data processing and Gemini API interaction parts of the pipeline:\n",
    "\n",
    "- **Standard library**: `re` (regex), `json`, `logging`, `Path` (file paths), `Sequence`, `List` (type hints), `ProcessPoolExecutor` (parallel processing)\n",
    "- **Data processing**: `pandas` for DataFrame operations, `tqdm` for progress bars\n",
    "- **Language detection**: `langdetect` with error handling - if not installed, sets a flag to disable language filtering\n",
    "- **Google GenAI**: The Google Generative AI client library for interacting with Gemini models\n",
    "\n",
    "The language detection import includes a try-except block to gracefully handle cases where the library isn't installed, allowing the notebook to continue with a warning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VkUn0nxf_Thd",
   "metadata": {
    "executionInfo": {
     "elapsed": 4803,
     "status": "ok",
     "timestamp": 1769035528530,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "VkUn0nxf_Thd"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re, json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Sequence, List\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "try:\n",
    "    from langdetect import detect, LangDetectException\n",
    "    HAS_LANGDETECT = True\n",
    "except ImportError:\n",
    "    HAS_LANGDETECT = False\n",
    "    logging.warning(\"langdetect not installed. Language filtering disabled.\")\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e230351",
   "metadata": {},
   "source": [
    "# Gemini Classification System Prompt\n",
    "\n",
    "This cell defines the comprehensive system instruction (prompt) for the Gemini model to classify social media posts about the Gaza War. The prompt includes:\n",
    "\n",
    "1. **Task Definition**: Clear instructions to classify English-language posts into one of five categories\n",
    "2. **Allowed Labels**: Pro-Palestinian, Pro-Israeli, Anti-War_Pro-Peace, Other, Off-topic\n",
    "3. **Detailed Definitions**: Specific guidance for each category with examples of cues, emojis, and language patterns\n",
    "4. **Classification Rules**: Tie-break rules and checklists to ensure consistent classification\n",
    "5. **Output Format**: Strict requirement to return only the label on a single line\n",
    "\n",
    "The prompt is designed to be very specific to minimize ambiguity and ensure the model returns exactly the expected format. This is critical for automated processing of large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c49600",
   "metadata": {
    "id": "74c49600"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Gaza-stance classification (Gemini) â€“ v6, prompt â€œStanceClassifier-Gaza-v2â€\n",
    "# --------------------------------------------------------------------------\n",
    "# â€¢ Reconstruit le texte complet depuis les CSV Facebook & Instagram.\n",
    "# â€¢ Filtre les posts non anglophones puis Ã©chantillonne 0,5 %.\n",
    "# â€¢ Envoie chaque post Ã  Gemini-2.5-flash-preview avec le prompt ci-dessous.\n",
    "# â€¢ La rÃ©ponse doit Ãªtre **exactement** un label sur une ligne.\n",
    "# â€¢ Sauvegarde CSV : text, gpt_category\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\"import logging, os, sys\n",
    "logging.basicConfig(level=logging.WARNING,           # root silencieux\n",
    "                    format=\"[%(levelname)s] %(message)s\",\n",
    "                    force=True)                      # â† reset complet\n",
    "\n",
    "log = logging.getLogger(\"pipeline\")                  # ton logger\n",
    "log.setLevel(logging.INFO)                           # un seul handler (root)\n",
    "# pas dâ€™ajout de handler, on laisse propager au root\n",
    "\n",
    "for name in (\"google\", \"google_genai\", \"google.api_core\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)  # stop INFO/DEBUG\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV  = \"/home/lisst_ai/DATA/FACEBOOK_concat.csv\"\n",
    "#INSTAGRAM_CSV = \"/home/lisst_ai/DATA/INSTAGRAM_concat.csv\"\n",
    "\n",
    "#GOOGLE_API_KEY  = \"\"\n",
    "#MODEL_NAME      = \"gemini-2.5-flash-preview-04-17\"\n",
    "OUTPUT_FILENAME = \"/home/lisst_ai/DATA/gaza_stance_sampled_classified.csv\"\n",
    "\n",
    "SAMPLE_FRAC      = 0.005   # 0,5 %\n",
    "RANDOM_STATE     = 42\n",
    "TARGET_LANG      = \"en\"\n",
    "NUM_WORDS_SAMPLE = 100\n",
    "NUM_WORKERS      = None\n",
    "\n",
    "SEP             = \" - \"\n",
    "CONSTRUCTED_COL = \"_text\"\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\",\n",
    "    \"Anti-War_Pro-Peace\",\n",
    "    \"Other\",\n",
    "    \"Off-topic\",\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SYSTEM_INSTRUCTION = \"\"\"\n",
    "You are â€œStanceClassifier-Gaza-v2â€.\n",
    "Task: read one English-language social-media post about the 2023-2025 Gaza War and\n",
    "output **exactly one** label from the list below.\n",
    "\n",
    "Think step-by-step **silently** (donâ€™t reveal your reasoning).\n",
    "Return only the label on a single line, nothing else.\n",
    "\n",
    "### Allowed labels\n",
    "1. Pro-Palestinian\n",
    "2. Pro-Israeli\n",
    "3. Anti-War_Pro-Peace\n",
    "4. Other\n",
    "5. Off-topic          â† use when the post is not about the Gaza War at all\n",
    "\n",
    "### Definitions & guidance\n",
    "\n",
    "**Pro-Palestinian**\n",
    "Any stance that primarily supports Palestinians or blames Israel.\n",
    "âœ” Detect even subtle cues: ğŸ‡µğŸ‡¸ emoji, â€œfrom the river to the seaâ€, focus on Palestinian victims, words like â€œgenocideâ€, â€œoccupationâ€, â€œapartheidâ€, or praise for resistance.\n",
    "âœ˜ A single humanitarian mention of both sides â†’ see Anti-War_Pro-Peace unless Israel is clearly blamed.\n",
    "\n",
    "**Pro-Israeli**\n",
    "Primarily supports Israel or blames Hamas/Palestinian side.\n",
    "âœ” Cues: ğŸ‡®ğŸ‡±, â€œright to self-defenceâ€, â€œterrorist organisationâ€, hostage hashtags, emphasis on Hamas using human shields.\n",
    "âœ˜ Balanced calls for restraint â†’ Anti-War_Pro-Peace unless Hamas is clearly condemned more than Israel.\n",
    "\n",
    "**Anti-War_Pro-Peace**\n",
    "Core message = stop the violence / protect civilians on *both* sides, with balanced language.\n",
    "\n",
    "**Other** (use sparingly)\n",
    "Mentions the war but no discernible leaning **after** checking for:\n",
    "â€£ sarcasm or irony\n",
    "â€£ vocabulary that allocates guilt (even implicitly)\n",
    "â€£ spotlight on one populationâ€™s suffering\n",
    "â€£ selective historical references\n",
    "\n",
    "**Off-topic**\n",
    "Post lacks substantive reference to the Gaza War (generic memes, ads, ambiguous â€œPray for themâ€ with no context, etc.).\n",
    "\n",
    "### Checklist before choosing â€œOtherâ€\n",
    "1. Does the wording, emoji, or hashtag tilt sympathy toward one side?\n",
    "2. Are perpetrators or victims named asymmetrically?\n",
    "3. Is there an implied moral judgment?\n",
    "If **any** answer is yes, assign the corresponding stance; else, Other.\n",
    "\n",
    "### Tie-break rules\n",
    "* Stance + peace call â†’ keep the stance.\n",
    "* Balanced criticism but one side more severe â†’ assign that side.\n",
    "* Unsure between Anti-War and partisan â†’ choose the partisan stance.\n",
    "\n",
    "### Output format\n",
    "Return exactly one of:\n",
    "Pro-Palestinian | Pro-Israeli | Anti-War_Pro-Peace | Other | Off-topic\n",
    "\n",
    "\"\"\".strip()\n",
    "\"\"\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. GEMINI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log.info(\"Initialisation API Geminiâ€¦\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "GEN_CFG = types.GenerateContentConfig(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    temperature=0.1,\n",
    "    safety_settings=[\n",
    "        types.SafetySetting(category=c, threshold=types.HarmBlockThreshold.BLOCK_NONE)\n",
    "        for c in (\n",
    "            types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "            types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "            types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "            types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        )\n",
    "    ],\n",
    "    thinking_config=genai.types.ThinkingConfig(\n",
    "      thinking_budget=0\n",
    "    )\n",
    ")\n",
    "log.info(\"ModÃ¨le prÃªt : %s\", MODEL_NAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. UTILITAIRES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ZERO_WIDTH_RE = re.compile(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF]\")\n",
    "\n",
    "def strip_invisible(t: str) -> str:\n",
    "    return ZERO_WIDTH_RE.sub(\"\", t)\n",
    "\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for v in values:\n",
    "        if isinstance(v, str):\n",
    "            v = v.strip()\n",
    "            if v and v.lower() not in sep.join(parts).lower():\n",
    "                parts.append(v)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    log.info(\"Lecture CSV : %s\", path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    log.info(\" â†’ %s lignes\", f\"{len(df):,}\")\n",
    "    return df\n",
    "\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dÃ©tection langue (multiprocessing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _detect(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _lang_worker(args):\n",
    "    i, txt = args\n",
    "    sample = \" \".join(txt.split()[:NUM_WORDS_SAMPLE])\n",
    "    return i, (_detect(sample) == TARGET_LANG)\n",
    "\n",
    "def filter_english(texts: list[str]) -> list[bool]:\n",
    "    workers = NUM_WORKERS or (os.cpu_count() or 4)\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for i, ok in ex.map(_lang_worker, enumerate(texts), chunksize=512):\n",
    "            flags[i] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. CLASSIFY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def classify(post: str) -> str:\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=post,\n",
    "            config=GEN_CFG,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.error(\"Gemini error : %s â€“ exit.\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not resp.text:\n",
    "        return \"INVALID\"\n",
    "\n",
    "    label = resp.text.splitlines()[0].strip()\n",
    "    return label if label in ALLOWED_CATEGORIES else \"INVALID\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    df_fb = process_facebook(safe_read_csv(FACEBOOK_CSV))\n",
    "    #df_ig = process_instagram(safe_read_csv(INSTAGRAM_CSV))\n",
    "\n",
    "    log.info(\"Filtre de langue : %d %d\", len(df_fb), len(df_ig))  # avant le filtrage\n",
    "    for df in (df_fb, df_ig):\n",
    "        mask = filter_english(df[CONSTRUCTED_COL].tolist())\n",
    "        df.drop(index=df.index[~pd.Series(mask)], inplace=True)\n",
    "\n",
    "    log.info(\"AprÃ¨s filtre de langue : %d %d\", len(df_fb), len(df_ig))\n",
    "\n",
    "    df_sampled = pd.concat([\n",
    "        df_fb.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE),\n",
    "        df_ig.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "    ], ignore_index=True)\n",
    "    log.info(\"Ã‰chantillon : %s posts\", f\"{len(df_sampled):,}\")\n",
    "\n",
    "    texts = df_sampled[CONSTRUCTED_COL].apply(strip_invisible).tolist()\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"Aucun texte anglais aprÃ¨s filtrage.\")\n",
    "\n",
    "    #log.info(\"Classification Geminiâ€¦\")\n",
    "    #cats = [classify(t) for t in tqdm(texts, desc=\"Gemini\", unit=\"post\")]\n",
    "\n",
    "   #out = pd.DataFrame({\"text\": texts, \"gpt_category\": cats})\n",
    "    #n_inv = (out[\"gpt_category\"] == \"INVALID\").sum()\n",
    "    #if n_inv:\n",
    "    #    log.warning(\"%d rÃ©ponses INVALID\", n_inv)\n",
    "\n",
    "    #out.query(\"gpt_category != 'INVALID'\").to_csv(OUTPUT_FILENAME, index=False)\n",
    "    log.info(\"Fichier enregistrÃ© : %s\", OUTPUT_FILENAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EntrÃ©e script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    log.info(\"Lancement pipelineâ€¦\")\n",
    "    main()\n",
    "    log.info(\"TerminÃ©.\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e51cbb",
   "metadata": {},
   "source": [
    "# Tokenization, Prompt Building, and Custom Classes\n",
    "\n",
    "This cell sets up the core components for text processing and model training:\n",
    "\n",
    "1. **Prompt Building Function (`build_prompt`)**: \n",
    "   - Creates instruction-following prompts in the Mistral chat template format\n",
    "   - Includes system message with classification task and category list\n",
    "   - Uses the tokenizer's chat template to ensure proper formatting\n",
    "\n",
    "2. **Encoding Function (`enc`)**:\n",
    "   - Converts text to tokenized format\n",
    "   - Truncates to 512 tokens maximum\n",
    "   - Handles special tokens appropriately\n",
    "\n",
    "3. **Tokenizer Setup**:\n",
    "   - Loads the Ministral tokenizer with left padding (safer for generation)\n",
    "   - Adds a proper padding token if missing\n",
    "   - Records vocabulary size for model embedding resizing\n",
    "\n",
    "4. **Custom Dataset Class (`TokenizedDS`)**:\n",
    "   - PyTorch Dataset wrapper for tokenized text and labels\n",
    "   - Returns dictionaries with input_ids, attention_mask, and labels\n",
    "\n",
    "5. **Custom Trainer Class (`WeightedCETrainer`)**:\n",
    "   - Extends Hugging Face Trainer with class-weighted cross-entropy loss\n",
    "   - Handles imbalanced datasets by weighting classes inversely to their frequency\n",
    "   - Computes loss using the weighted approach\n",
    "\n",
    "6. **Metrics Functions**:\n",
    "   - `cat_metrics`: Computes accuracy, F1-macro, F1-micro, and Matthews Correlation Coefficient (MCC)\n",
    "   - `compute_metrics`: Wrapper for Hugging Face evaluation that formats metrics with \"eval_\" prefix\n",
    "\n",
    "7. **Test Set Encoding**: Pre-encodes the test set for later evaluation\n",
    "\n",
    "This cell establishes all the infrastructure needed for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90976cab",
   "metadata": {},
   "source": [
    "# Model Training Setup and Data Loading\n",
    "\n",
    "This cell performs the initial setup for model training:\n",
    "\n",
    "1. **Configuration Constants**:\n",
    "   - `EFF_BATCH_TARGET = 32`: Target effective batch size for gradient accumulation\n",
    "   - Model architecture: Ministral-8B-Instruct with LoRA fine-tuning\n",
    "   - Training parameters: 40 epochs, early stopping patience of 6, timeout of 3 hours\n",
    "   - Cross-validation: 5-fold CV with 3 Optuna trials\n",
    "   - Maximum sequence length: 512 tokens\n",
    "\n",
    "2. **Google Colab Drive Mount**: Mounts Google Drive to access data files stored there\n",
    "\n",
    "3. **Library Imports**: Imports all necessary libraries for model training including PyTorch, Transformers, PEFT, Optuna, and scikit-learn\n",
    "\n",
    "4. **Random Seed Setting**: Sets seeds for reproducibility across random number generators (Python, NumPy, PyTorch)\n",
    "\n",
    "5. **Data Loading and Preprocessing**:\n",
    "   - Loads the classified CSV file\n",
    "   - Merges minor classes (Other, Off-topic, Anti-War_Pro-Peace) into a single \"Neutral\" class\n",
    "   - Creates label mappings (label2id, id2label)\n",
    "   - Splits data into train/validation (90%) and test (10%) sets with stratification\n",
    "\n",
    "This prepares the foundation for the entire training pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481dfac1",
   "metadata": {},
   "source": [
    "# Google Colab Widget Manager (Enable)\n",
    "\n",
    "This cell enables the custom widget manager in Google Colab. This is necessary for certain interactive widgets and visualizations to work properly in the Colab environment. The widget manager provides support for third-party widgets that enhance the notebook experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rxJm-koN9xr9",
   "metadata": {
    "id": "rxJm-koN9xr9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb487b57",
   "metadata": {},
   "source": [
    "# Google Colab Widget Manager (Disable)\n",
    "\n",
    "This cell disables the custom widget manager. This is typically done at the end of a session or when widgets are no longer needed. Disabling it can free up resources and clean up the notebook environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8043ccf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49481,
     "status": "ok",
     "timestamp": 1769035591543,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "b8043ccf",
    "outputId": "d0f7c1d9-b34b-463b-c2a3-cb7bc144dcc5"
   },
   "outputs": [],
   "source": [
    "EFF_BATCH_TARGET = 32\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Llama-3-70B-Instruct â€¢ LoRA â€¢ 4-bit NF4 â€¢ Optuna â€¢ Cosine LR â€¢ MCC\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#   â€¢ EntrÃ©e tronquÃ©e Ã  512 tokens\n",
    "#   â€¢ Quantisation bitsandbytes 4-bit NF4\n",
    "#   â€¢ Fusion des classes : Other + Off-topic + Anti-War_Pro-Peace â†’ Neutral\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import os, gc, random, threading, _thread, json, math, copy\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef\n",
    "\n",
    "import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "SEED = 42\n",
    "set_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "MODEL_NAME = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "NUM_EPOCHS, EARLY_STOP_PATIENCE, TIMEOUT_TRAIN = 40, 6, 3 * 36000\n",
    "KFOLD, N_TRIALS = 5, 3\n",
    "artifact_dir = Path(\"./outputs\"); artifact_dir.mkdir(exist_ok=True, parents=True)\n",
    "SINGLE_FOLD_ONLY = False\n",
    "MAX_LENGTH = 512\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lecture du CSV + fusion -------------------------\n",
    "CSV_PATH = \"/content/drive/MyDrive/UTJ2/Memoire/Scripts/Stance/gaza_stance_sampled_classified.csv\"\n",
    "df = pd.read_csv(CSV_PATH)          # colonnes: text, gpt_category / cat\n",
    "\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "# â”€â”€â”€ Fusion des classes mineures â†’ Neutral\n",
    "FUSE_MAP = {\n",
    "    \"Pro-Palestinian\":        \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\":            \"Pro-Israeli\",\n",
    "    \"Other\":                  \"Neutral\",\n",
    "    \"Off-topic\":              \"Neutral\",\n",
    "    \"Anti-War_Pro-Peace\":     \"Neutral\",\n",
    "}\n",
    "df[\"cat\"] = df[\"cat\"].map(FUSE_MAP)             # applique la fusion\n",
    "df.dropna(subset=[\"cat\"], inplace=True)         # sÃ©curitÃ© : lignes non mappÃ©es\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Mappings label â†” id ------------------------------\n",
    "cats      = sorted(df[\"cat\"].unique())          # ['Neutral', 'Pro-Israeli', 'Pro-Palestinian']\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Diagnostics --------------------------------------\n",
    "print(f\"{len(df):,} exemples chargÃ©s (classes fusionnÃ©es).\")\n",
    "print(\"RÃ©partition des classes :\", df[\"label_id\"].value_counts().to_dict())\n",
    "print(\"label2id :\", label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Listes utiles / constantes -----------------------\n",
    "messages   = df[\"text\"].tolist()\n",
    "labels     = df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)                           # 3\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Split train / val / test -------------------------\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d058c",
   "metadata": {},
   "source": [
    "# Training Timeout Helper Function\n",
    "\n",
    "This cell defines a utility function `timeout_train()` that implements a timeout mechanism for model training. This is crucial for hyperparameter optimization with Optuna, where:\n",
    "\n",
    "- **Purpose**: Prevents individual training runs from running indefinitely, which could block the entire optimization process\n",
    "- **Mechanism**: Uses threading to run training in a separate thread and interrupts it after the specified timeout period\n",
    "- **Use Case**: When Optuna is testing many hyperparameter combinations, some might cause training to hang or take too long. This function ensures each trial completes within the timeout window (3 hours in this case)\n",
    "\n",
    "The function starts training in a background thread, waits for the specified duration, and if training is still running, it interrupts the main thread to stop the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f90a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d859487e62ae4f1da1284846bc7abcb6",
      "0dbf1d6e218c4930b5beaa843c985fe0",
      "4adc0eadab4b441c9346d96e50a21f6a",
      "b637c5cd40dc450cbb7573e2a8935857",
      "4aacc61fdb374caebed92f9096b21944",
      "25f3c9542b2b4467b8995ba923a5fefd",
      "e12099fda04b4db4ae4405ea17776f74",
      "956baf5852b44eb097ea07b913f0b9cb",
      "364f174037914d769f413641bdc67f08",
      "9715b7d58c334f8b9cb16e17483fdc43",
      "0b7ec87b9c15483f85a55e7610ac2642",
      "1497d4b7748348b6ba6cd32593194e1e",
      "8b0838891b6f46c68400e1ed3c912a81",
      "093949c9ff6c47109c73936559022568",
      "b5341442f83e4b30ac45d26a068caa96",
      "33659f055420477d96ed9536fc933440",
      "1c34568ec32d4ba4a38e495705928d90",
      "70891aac81c64db488a3b38c99d26f74",
      "2debc053862a4c6ea56fd8bd79a73459",
      "b29b016319b14adab629eb2647d3a355",
      "e80a79c956d7489db2f4be66166ce017",
      "b1faa0869bf54d5399433efe4e083f8a",
      "94c5951d08d0450485c607bc737a4770",
      "a08c06618f61423783448d2d965d65db",
      "37f74260cc4b4311b1edf58a3bd6c8d5",
      "1b3d07ab5e6849ccaf1c54ed915db588",
      "d6d08beaf22b43128ecb74b24ae2a353",
      "9134ea58f63345d696bddbfeacf72b37",
      "873e6d9585f0426b88eb064c4c43c73e",
      "89c6b12b18704bf8a1f9a49c32a1cd80",
      "6566f97e6225481f8bb153dd3151dcf6",
      "280b5d5f5b5d4358ba6a4fe7caa8d565",
      "0d3cba73f4fc4fa68037c32ca2035efb"
     ]
    },
    "executionInfo": {
     "elapsed": 5730,
     "status": "ok",
     "timestamp": 1769035603294,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "c02f90a2",
    "outputId": "0052c59b-9099-40d1-b0ab-38dfe90f0137"
   },
   "outputs": [],
   "source": [
    "def enc(txts: List[str]):\n",
    "    prompts = [build_prompt(t) for t in txts]\n",
    "    return tok(                     # â†Â important\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=False    # plus de doubleÂ BOS\n",
    "    )\n",
    "\n",
    "def build_prompt(txt: str) -> str:\n",
    "    \"\"\"Prompt Mistral officiel, sÃ»r pour l'encodeur HF.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": f\"You are an expert assistant. Classify the following text into one of these \"\n",
    "                    f\"categories: {cats_string}. Respond with the category label only.\"},\n",
    "        {\"role\": \"user\", \"content\": txt}\n",
    "    ]\n",
    "    return tok.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,                # on renvoie une string\n",
    "        add_generation_prompt=False     # ajoute automatiquement le tag assistant\n",
    "    ).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt instructif -----------------------------------------\n",
    "cats_string = \", \".join(cats)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tokenizer --------------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")  # â† leftâ€‘pad plus sÃ»r\n",
    "if tok.pad_token_id is None or tok.pad_token_id == tok.eos_token_id:\n",
    "    # 1) on crÃ©e un vrai token pad\n",
    "    tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "    # 2) on pointe officiellement dessus\n",
    "    tok.pad_token = '<pad>'\n",
    "\n",
    "# IMPORTANT : redimensionner lâ€™embedding AVANT dâ€™injecter LoRA\n",
    "VOCAB = len(tok)\n",
    "\n",
    "class TokenizedDS(Dataset):\n",
    "    def __init__(self, e, y):\n",
    "        self.e, self.y = e, y\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        d = {k: torch.tensor(v[i]) for k, v in self.e.items()}\n",
    "        d[\"labels\"] = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return d\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WeightedCETrainer ----------------------------------------\n",
    "class WeightedCETrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.float()\n",
    "\n",
    "    # â–¿ ajouter le nouvel argument (ou **kwargs) â–¿\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: int | None = None,   # â† NEW\n",
    "        **kwargs,                                # â† compatibilitÃ© future\n",
    "    ):\n",
    "        labels  = inputs[\"labels\"]\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits  = outputs.logits\n",
    "\n",
    "        weight = self.class_weights.to(logits.device)\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits,\n",
    "            labels,\n",
    "            weight=weight,    # pondÃ©ration des classes\n",
    "            reduction=\"mean\"  # moyenne directe sur le batch\n",
    "        )\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics ----------------------------------------------------\n",
    "\n",
    "def cat_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def compute_metrics(ev: EvalPrediction):\n",
    "    y_hat = np.argmax(ev.predictions, axis=1)\n",
    "    g = cat_metrics(ev.label_ids, y_hat)\n",
    "    return {\n",
    "        \"eval_accuracy\": g[\"accuracy\"],\n",
    "        \"eval_f1_macro\": g[\"f1_macro\"],\n",
    "        \"eval_f1_micro\": g[\"f1_micro\"],\n",
    "        \"eval_mcc\": g[\"mcc\"],\n",
    "    }\n",
    "\n",
    "\n",
    "te_enc = enc(msg_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f5781",
   "metadata": {},
   "source": [
    "# Optuna Hyperparameter Optimization\n",
    "\n",
    "This is the core hyperparameter optimization cell that uses Optuna to find the best training configuration:\n",
    "\n",
    "1. **Setup**:\n",
    "   - Sets random seeds for reproducibility\n",
    "   - Creates artifacts directory for saving results\n",
    "   - Reloads and prepares the dataset with label mappings\n",
    "\n",
    "2. **Model Building Function (`build_model`)**:\n",
    "   - Constructs a quantized model (4-bit NF4) using BitsAndBytes\n",
    "   - Applies LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "   - Configures flash attention for faster training\n",
    "   - Enables gradient checkpointing to save memory\n",
    "\n",
    "3. **Optuna Objective Function**:\n",
    "   - **Hyperparameter Search Space**:\n",
    "     - Learning rate: 5e-5 to 5e-4 (log scale)\n",
    "     - Batch size: 16 or 32\n",
    "     - LoRA rank: 8, 16, 24, or 32\n",
    "     - LoRA dropout: 0.01 to 0.1 (log scale)\n",
    "   - **Cross-Validation**: Uses 5-fold stratified CV, but only uses folds 0 and 2 for faster optimization\n",
    "   - **Training Process**: For each fold, trains a model with the suggested hyperparameters, evaluates on validation set, and tracks MCC score\n",
    "   - **Best Model Tracking**: Saves the best model state across all folds for each trial\n",
    "\n",
    "4. **Optuna Study Execution**:\n",
    "   - Creates a study to maximize MCC (Matthews Correlation Coefficient)\n",
    "   - Runs 3 trials to find optimal hyperparameters\n",
    "   - Extracts best parameters and model state\n",
    "\n",
    "5. **Best Model Reconstruction**:\n",
    "   - Rebuilds the model with best hyperparameters\n",
    "   - Loads the best state from the optimization\n",
    "   - Evaluates on test set\n",
    "   - Saves the model, tokenizer, and best parameters to disk\n",
    "\n",
    "This cell performs the actual hyperparameter search and produces the optimized model configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eNXHCkl1AhnO",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1769035607515,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "eNXHCkl1AhnO"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b43caa",
   "metadata": {},
   "source": [
    "# Load and Display Best Parameters\n",
    "\n",
    "This simple cell loads the saved best hyperparameters from the Optuna optimization and displays them. This allows you to see what configuration was found to be optimal:\n",
    "- Learning rate\n",
    "- Batch size  \n",
    "- LoRA rank (r)\n",
    "- LoRA dropout\n",
    "\n",
    "These parameters will be used in subsequent cells for final model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNKzJGhgAhnO",
   "metadata": {
    "id": "RNKzJGhgAhnO"
   },
   "source": [
    "Support for third party widgets will remain active for the duration of the session. To disable support:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867298b",
   "metadata": {},
   "source": [
    "# Final Model Training with Optimal Hyperparameters\n",
    "\n",
    "This cell performs the final training run using the best hyperparameters found by Optuna:\n",
    "\n",
    "1. **Data Reloading**: Reloads the CSV from scratch to ensure a clean state, recreates label mappings\n",
    "\n",
    "2. **Data Splitting**: Creates train/validation/test splits:\n",
    "   - 90% for train+validation, 10% for test\n",
    "   - Further splits train+validation into 90% train, 10% validation\n",
    "\n",
    "3. **Text Encoding**: Encodes all three splits (train, validation, test) using the prompt building and tokenization functions\n",
    "\n",
    "4. **Hyperparameter Setup**: Uses the best parameters from Optuna (or manually specified values):\n",
    "   - Learning rate: 0.000315\n",
    "   - Batch size: 32\n",
    "   - LoRA rank: 64\n",
    "   - LoRA dropout: 0.1\n",
    "   - Calculates gradient accumulation steps to reach effective batch size of 32\n",
    "\n",
    "5. **Model Initialization**: \n",
    "   - Creates 4-bit quantized base model\n",
    "   - Applies LoRA with optimal configuration\n",
    "   - Sets up for efficient training with flash attention\n",
    "\n",
    "6. **Class Weighting**: Calculates class weights to handle imbalanced dataset\n",
    "\n",
    "7. **Training Configuration**: Sets up TrainingArguments with:\n",
    "   - Cosine learning rate schedule\n",
    "   - Early stopping based on validation MCC\n",
    "   - Model checkpointing\n",
    "   - Best model loading at end\n",
    "\n",
    "8. **Training Execution**: Trains the model with timeout protection\n",
    "\n",
    "9. **Final Evaluation**: Evaluates on test set and displays metrics\n",
    "\n",
    "10. **Model Saving**: Saves the final trained model, tokenizer, and configuration\n",
    "\n",
    "This produces the final production-ready model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2-2UaaEAhnP",
   "metadata": {
    "id": "A2-2UaaEAhnP"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.disable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f600b82",
   "metadata": {},
   "source": [
    "# Display Test Results\n",
    "\n",
    "This cell displays the test set evaluation results from the final model training. The output shows comprehensive metrics including:\n",
    "- **eval_accuracy**: Overall classification accuracy\n",
    "- **eval_f1_macro**: Macro-averaged F1 score (average across classes)\n",
    "- **eval_f1_micro**: Micro-averaged F1 score (overall F1)\n",
    "- **eval_mcc**: Matthews Correlation Coefficient (balanced metric for imbalanced datasets)\n",
    "- **eval_loss**: Final loss value\n",
    "- **eval_runtime**: Time taken for evaluation\n",
    "- Training statistics (samples/second, steps/second, final epoch)\n",
    "\n",
    "These metrics provide a complete picture of model performance on the held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf21ec",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1769035610082,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "c6cf21ec"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Timeout helper --------------------------------------------\n",
    "\n",
    "def timeout_train(trainer, seconds):\n",
    "    t = threading.Thread(target=trainer.train)\n",
    "    t.start(); t.join(seconds)\n",
    "    if t.is_alive():\n",
    "        _thread.interrupt_main(); t.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ddc670",
   "metadata": {},
   "source": [
    "# Detailed Classification Report and Confusion Matrix\n",
    "\n",
    "This cell generates comprehensive performance analysis for the trained model:\n",
    "\n",
    "1. **Predictions Extraction**: \n",
    "   - Gets raw predictions from the model on the test set\n",
    "   - Extracts true labels and predicted labels\n",
    "\n",
    "2. **Classification Report**:\n",
    "   - Generates per-class metrics: precision, recall, F1-score, and support (number of samples)\n",
    "   - Shows both per-class and overall (macro/micro averaged) metrics\n",
    "   - Outputs as a formatted DataFrame for easy reading\n",
    "\n",
    "3. **Confusion Matrix**:\n",
    "   - Creates a confusion matrix showing true vs predicted labels\n",
    "   - Formatted as a DataFrame with labeled rows (true labels) and columns (predicted labels)\n",
    "   - Helps identify which classes are being confused with each other\n",
    "\n",
    "This analysis is crucial for understanding model behavior, identifying problematic class confusions, and assessing whether the model meets the requirements for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d465b4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5ad0ee1341244193a6743bd2a2207833",
      "83db65deecc9408d99574c0c5c74a552",
      "279fba77a40a4a73b5f7ddc30dfcc9e0",
      "4ef56778e4f54c179af64e0ec72750db",
      "8fe1094fb7d546b5977f1f4b75972e16",
      "236a9c2d5ebd485ebce91732a03c2c5f",
      "2e10d96515ad451b87d50265639c2552",
      "a6cbb34af4d04414a0bc2dc11a191298",
      "de97cc7484b74b92905f2d67c184cd0a",
      "6af59cb80b32457ba43ff1135128c83d",
      "c7d43a6ed71f469e88b448a2e681920e",
      "5b15e0aef23345918af904b64a911ed6",
      "cd6a0872a8c8439ca657648a7858834d",
      "f3dd43f426a541ad936365e395cc9dc5",
      "2c102e3e0c4f4d6f99abbf728b8e9e28",
      "179aead8a0d84108bd3d6b68c68c89d8",
      "c62fff4e2c1f4cbd8b376d740e066325",
      "c5f17f05acdb496c869d63a4103bc3ec",
      "7241fcdcbd5d4679866243d592b29094",
      "82bfbabc74e04380a49b6083c8388813",
      "5dd368b1abe74b54a8ef5c23ae2cfeb5",
      "b0f0ccfbe9214be3b4613e6d25d735ce"
     ]
    },
    "id": "8d465b4c",
    "outputId": "041e5c02-95c2-485c-c3a6-1f4889ddee3f"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©-requis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, gc, copy, json, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import optuna\n",
    "\n",
    "# (on suppose que tous les imports HF, LoRA, votre collator, compute_metrics,\n",
    "#  WeightedCETrainer, enc(), TokenizedDS, timeout_train(), etc. sont dÃ©jÃ  prÃ©sents)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "artifact_dir = \"./artifacts\"; os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chargement et dÃ©coupage du corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df = df.rename(columns={\"gpt_category\": \"cat\"})\n",
    "\n",
    "cats       = sorted(df[\"cat\"].unique())\n",
    "label2id   = {c: i for i, c in enumerate(cats)}\n",
    "id2label   = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS       = len(cats)\n",
    "\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(r_lora, lora_dropout, use_bf16):\n",
    "    \"\"\"Construit un modÃ¨le QLoRA + classification, sans entraÃ®nement.\"\"\"\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    )\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id      = tok.pad_token_id\n",
    "    base.config.use_cache         = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    l_cfg = LoraConfig(\n",
    "        r=r_lora,\n",
    "        lora_alpha=2 * r_lora,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    model = get_peft_model(base, l_cfg)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad: p.data = p.data.float()\n",
    "    return model\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optuna objective  (validation croisÃ©e) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def objective(trial):\n",
    "    # Hyper-paramÃ¨tres\n",
    "    lr            = trial.suggest_float(\"lr\", 5e-5, 5e-4, log=True)\n",
    "    bsz           = trial.suggest_categorical(\"bsz\", [16, 32])\n",
    "    r_lora        = trial.suggest_int(\"lora_r\", 8, 32, step=8)\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.01, 0.1, log=True)\n",
    "\n",
    "    grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = not use_bf16\n",
    "    skf      = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_mcc, best_fold_state, best_fold_metric = [], None, -1.0\n",
    "    VAL_FOLDS_TO_USE = [0, 2]\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(skf.split(msg_tv, lab_tv)):\n",
    "        if fold_idx not in VAL_FOLDS_TO_USE:\n",
    "            continue\n",
    "\n",
    "        # jeux train / val encodÃ©s\n",
    "        msg_tr = [msg_tv[i] for i in tr_idx]; lab_tr = [lab_tv[i] for i in tr_idx]\n",
    "        msg_va = [msg_tv[i] for i in va_idx]; lab_va = [lab_tv[i] for i in va_idx]\n",
    "        train_ds = TokenizedDS(enc(msg_tr), lab_tr)\n",
    "        val_ds   = TokenizedDS(enc(msg_va), lab_va)\n",
    "\n",
    "        # pondÃ©ration des classes\n",
    "        counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "        cls_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "        model = build_model(r_lora, lora_dropout, use_bf16)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"{artifact_dir}/trial_{trial.number}/fold_{fold_idx}\",\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            per_device_train_batch_size=bsz,\n",
    "            per_device_eval_batch_size=32,\n",
    "            gradient_accumulation_steps=grad_acc,\n",
    "            warmup_ratio=0.08,\n",
    "            learning_rate=lr,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=use_fp16,\n",
    "            gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",               # pas besoin de sauvegarde intermÃ©diaire\n",
    "            seed=SEED,\n",
    "            logging_strategy=\"epoch\",\n",
    "            label_names=[\"labels\"],\n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"eval_mcc\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "\n",
    "        trainer = WeightedCETrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "            class_weights=cls_weights,\n",
    "        )\n",
    "\n",
    "        # entraÃ®nement (protÃ©gÃ© par timeout)\n",
    "        timeout_train(trainer, TIMEOUT_TRAIN)\n",
    "        mcc = trainer.evaluate(val_ds)[\"eval_mcc\"]\n",
    "        fold_mcc.append(mcc)\n",
    "\n",
    "        if mcc >= best_fold_metric:\n",
    "            best_fold_metric = mcc\n",
    "            best_fold_state  = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        del trainer, model\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    mean_mcc = float(np.mean(fold_mcc)) if fold_mcc else -1.0\n",
    "    trial.set_user_attr(\"best_state\", best_fold_state)\n",
    "    return mean_mcc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lancement de la recherche Optuna â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params\n",
    "best_state   = best_trial.user_attrs[\"best_state\"]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Reconstruction & chargement du meilleur modÃ¨le â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "best_model   = build_model(\n",
    "    r_lora       = best_params[\"lora_r\"],\n",
    "    lora_dropout = best_params[\"lora_dropout\"],\n",
    "    use_bf16     = use_bf16,\n",
    ")\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©paration du jeu test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_ds = TokenizedDS(enc(msg_te), lab_te)\n",
    "trainer = WeightedCETrainer(\n",
    "    model=best_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"{artifact_dir}/best_model_mcc\",\n",
    "        per_device_eval_batch_size=32,\n",
    "        dataloader_drop_last=False,\n",
    "        seed=SEED,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test MCC :\", round(test_metrics[\"eval_mcc\"], 4))\n",
    "print(\"Test F1  :\", round(test_metrics[\"eval_f1_macro\"], 4))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sauvegarde finale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_dir = f\"{artifact_dir}/best_model_mcc\"\n",
    "best_model.save_pretrained(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "with open(os.path.join(save_dir, \"best_params.json\"), \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(f\"ModÃ¨le et tokenizer sauvegardÃ©s dans Â« {save_dir} Â».\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8daa730",
   "metadata": {},
   "source": [
    "# Stochastic Weight Averaging (SWA) and Full Training Pipeline\n",
    "\n",
    "This cell implements an advanced training strategy using Stochastic Weight Averaging (SWA) to improve model robustness and generalization:\n",
    "\n",
    "1. **Setup and Imports**: \n",
    "   - Imports necessary libraries for SWA implementation\n",
    "   - Loads best hyperparameters from Optuna optimization\n",
    "\n",
    "2. **Helper Functions**:\n",
    "   - `set_all_seeds()`: Sets all random seeds for reproducibility\n",
    "   - `average_states()`: Averages model state dictionaries (core of SWA)\n",
    "   - `load_lora_model()`: Helper to load saved LoRA models\n",
    "\n",
    "3. **Phase A - Train+Validation with Multiple Seeds**:\n",
    "   - Trains models with 2 different random seeds (0 and 13) on train+validation set\n",
    "   - Each model is trained with early stopping on a small validation split\n",
    "   - Saves the best state from each seed\n",
    "   - Averages the two model states to create an SWA model\n",
    "   - Evaluates the SWA model on test set\n",
    "\n",
    "4. **Phase C - Full Training for Production**:\n",
    "   - Trains on 100% of data (train+validation+test combined)\n",
    "   - Again uses 2 seeds and creates SWA average\n",
    "   - This final model uses all available data for maximum performance\n",
    "   - Saves the production-ready SWA model\n",
    "\n",
    "**Why SWA?** Averaging weights from multiple training runs with different seeds reduces variance and often improves generalization. This is a common technique in competitive machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17559",
   "metadata": {
    "id": "98c17559"
   },
   "outputs": [],
   "source": [
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42308a5c",
   "metadata": {
    "id": "42308a5c"
   },
   "outputs": [],
   "source": [
    "  Parameters: lr=3.15e-04, bsz=32, lora_r=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b52b47",
   "metadata": {
    "id": "09b52b47"
   },
   "outputs": [],
   "source": [
    "{'lr': 0.000315, 'bsz': 32, 'lora_r': 16, 'lora_dropout': 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c166c60",
   "metadata": {
    "id": "7c166c60"
   },
   "outputs": [],
   "source": [
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Phase finale  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# 1)   Re-chargement du CSV  (Ã©tat vierge)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "cats      = sorted(df[\"cat\"].unique())\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)\n",
    "\n",
    "# 2)   DÃ©coupage train / val / test\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.10, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "\n",
    "# 3)   Encodage\n",
    "tr_enc, va_enc, te_enc = enc(msg_tr), enc(msg_va), enc(msg_te)\n",
    "train_ds = TokenizedDS(tr_enc, lab_tr)\n",
    "val_ds   = TokenizedDS(va_enc, lab_va)\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "\n",
    "# 4)   Hyper-paramÃ¨tres Optuna retenus\n",
    "best_params = json.load(open(artifact_dir / \"best_params.json\"))\n",
    "best_params = {'lr': 0.000315, 'bsz': 32, 'lora_r': 64, 'lora_dropout': 0.1}\n",
    "\n",
    "lr            = best_params[\"lr\"]\n",
    "bsz           = best_params[\"bsz\"]\n",
    "r_lora        = best_params[\"lora_r\"]\n",
    "lora_dropout  = best_params[\"lora_dropout\"]\n",
    "\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = not use_bf16\n",
    "\n",
    "# 5)   Instanciation modÃ¨le + LoRA\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "base.config.use_cache = False\n",
    "base.config.use_paged_attention = True\n",
    "if USE_GRADIENT_CHECKPOINTING:\n",
    "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "l_cfg = LoraConfig(\n",
    "    r=r_lora,\n",
    "    lora_alpha=2 * r_lora,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(base, l_cfg)\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "# 6)   PondÃ©ration des classes\n",
    "counts       = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "class_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "# 7)   Arguments dâ€™entraÃ®nement finaux\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{artifact_dir}/final_run\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=bsz,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    warmup_ratio=0.08,\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=use_bf16,\n",
    "    fp16=use_fp16,\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    logging_strategy=\"epoch\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = WeightedCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=DataCollatorWithPadding(tok),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "# 8)   EntraÃ®nement + Ã©valuation finale\n",
    "timeout_train(trainer, TIMEOUT_TRAIN)          # <-- garde la mÃªme limite que plus haut\n",
    "print(\"Best val MCC:\", trainer.state.best_metric)\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test set :\", {k: round(v, 4) for k, v in test_metrics.items()})\n",
    "\n",
    "# (facultatif) Sauvegarde du modÃ¨le affinÃ©\n",
    "trainer.save_model(f\"{artifact_dir}/final_model\")\n",
    "tok.save_pretrained(f\"{artifact_dir}/final_model\")\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b5e0e",
   "metadata": {
    "id": "819b5e0e"
   },
   "outputs": [],
   "source": [
    "Test set : {'eval_accuracy': 0.7943, 'eval_f1_macro': 0.6463, 'eval_f1_micro': 0.7943, 'eval_mcc': 0.6533, 'eval_loss': 1.4161, 'eval_runtime': 55.0543, 'eval_samples_per_second': 11.57, 'eval_steps_per_second': 0.363, 'epoch': 16.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c070b8",
   "metadata": {
    "id": "88c070b8"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scores dÃ©taillÃ©s par classe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) PrÃ©dictions brutes sur le test set\n",
    "pred_res = trainer.predict(test_ds)\n",
    "y_true   = pred_res.label_ids\n",
    "y_pred   = np.argmax(pred_res.predictions, axis=1)\n",
    "\n",
    "# 2) Rapport complet (precision / recall / f1 / support)\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(NUM_LABELS)],\n",
    "    digits=4,\n",
    "    output_dict=True          # â† pour lâ€™avoir aussi sous forme dict / DataFrame\n",
    ")\n",
    "print(\"\\n=== Classification report ===\")\n",
    "print(pd.DataFrame(report).T)\n",
    "\n",
    "# 3) Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{id2label[i]}\"  for i in range(NUM_LABELS)],\n",
    "                     columns=[f\"pred_{id2label[i]}\" for i in range(NUM_LABELS)])\n",
    "print(\"\\n=== Confusion matrix ===\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc36e5c",
   "metadata": {
    "id": "9cc36e5c"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Post-Optuna : 2 seeds + SWA, test, puis full-train final\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, gc, math, copy, random, numpy as np, torch\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dossiers / fichiers Optuna dÃ©jÃ  crÃ©Ã©s\n",
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "\n",
    "# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def average_states(paths):\n",
    "    avg = OrderedDict()\n",
    "    for p in paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(paths)).to(torch.float16)\n",
    "    return avg\n",
    "\n",
    "# â”€â”€â”€ Configs rÃ©utilisables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / best_params[\"bsz\"]))\n",
    "l_cfg = LoraConfig(\n",
    "    r=best_params[\"lora_r\"],\n",
    "    lora_alpha=2 * best_params[\"lora_r\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  A.   train+val  (90 %)  â†’  deux seeds + SWA\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase A : 2 seeds sur train+val ==========\")\n",
    "\n",
    "# mini-val interne 10 % pour lâ€™early-stop\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.1, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "tr_enc, va_enc = enc(msg_tr), enc(msg_va)\n",
    "train_ds, val_ds = TokenizedDS(tr_enc, lab_tr), TokenizedDS(va_enc, lab_va)\n",
    "\n",
    "cls_counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "cls_weights = torch.tensor(\n",
    "    cls_counts.sum() / (NUM_LABELS * (cls_counts + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Re-train seed {seed} (train+val) â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_val\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        eval_on_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc\",\n",
    "        greater_is_better=True,\n",
    "        seed=seed,\n",
    "        logging_strategy=\"epoch\",\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "        class_weights=cls_weights,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_val.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths.append(state_path)\n",
    "\n",
    "    del trainer, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd4f9f",
   "metadata": {
    "id": "4bdd4f9f"
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a710",
   "metadata": {
    "id": "2d93a710"
   },
   "outputs": [],
   "source": [
    "def eval_loader(loader, model):\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(model.device)\n",
    "            logits = model(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            ).logits\n",
    "            preds.extend(torch.argmax(logits, -1).cpu())\n",
    "            true.extend(batch[\"labels\"].cpu())\n",
    "    return preds, true, cat_metrics(true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589f347",
   "metadata": {
    "id": "3589f347"
   },
   "outputs": [],
   "source": [
    "import copy, torch, os\n",
    "\n",
    "def load_lora_model(state_path: str):\n",
    "    \"\"\"\n",
    "    Reconstruit le backbone + LoRA et recharge les poids sauvegardÃ©s.\n",
    "    Rien dâ€™autre nâ€™est modifiÃ©.\n",
    "    \"\"\"\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "\n",
    "    state = torch.load(state_path, map_location=\"cpu\")\n",
    "    # seules les tÃªtes LoRA ont Ã©tÃ© sauvegardÃ©es ; on les recharge sans toucher au reste\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Ã‰valuation -------------------------------------------------------------\n",
    "\n",
    "results = {}\n",
    "\n",
    "for state_path in state_paths:          # state_paths = [\"...seed0_val.pt\", \"...seed13_val.pt\"]\n",
    "    model = load_lora_model(state_path)\n",
    "    _, _, metrics = eval_loader(test_dl, model)   # ou val_loader/test_loader, selon ton besoin\n",
    "    results[os.path.basename(state_path)] = metrics\n",
    "\n",
    "# Petit rÃ©capitulatif lisible :\n",
    "for name, m in results.items():\n",
    "    print(f\"{name:25s} | MCC={m['mcc']:.4f} | Acc={m['accuracy']:.4f} | F1={m['f1_macro']:.4f} | F1={m['f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574743b3",
   "metadata": {
    "id": "574743b3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# â€” SWA hors-ligne â€”\n",
    "swa_state = average_states(state_paths)\n",
    "torch.save(swa_state, os.path.join(artifact_dir, \"best_state_swa_val.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:               # câ€™est le cas le plus frÃ©quent\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# â€” Ã©valuation sur test â€”\n",
    "print(\"\\n=========== Phase B : Ã©valuation SWA sur test =========\")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(VOCAB)\n",
    "\n",
    "\n",
    "\n",
    "base.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_swa = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "model_swa.load_state_dict(torch.load(os.path.join(artifact_dir, \"best_state_swa_val.pt\"), map_location=\"cpu\"), strict=False)\n",
    "\n",
    "\n",
    "\n",
    "test_ds = TokenizedDS(te_enc, lab_te)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, test_metrics = eval_loader(test_dl, model_swa)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  C.   FULL TRAIN (100 %)  â†’  deux seeds + SWA  â†’  production\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase C : full-train 100 % pour la prod =========\")\n",
    "\n",
    "full_enc = enc(msg_tv + msg_te)\n",
    "full_ds  = TokenizedDS(full_enc, lab_tv + lab_te)\n",
    "\n",
    "cls_counts_all = np.bincount(lab_tv + lab_te, minlength=NUM_LABELS)\n",
    "cls_weights_all = torch.tensor(\n",
    "    cls_counts_all.sum() / (NUM_LABELS * (cls_counts_all + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths_full = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Full-train seed {seed} â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    # pas de validation â†’ epochs fixes\n",
    "    args_prod = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_full\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_strategy=\"epoch\",\n",
    "        seed=seed,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer_prod = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args_prod,\n",
    "        train_dataset=full_ds,\n",
    "        data_collator=collator,\n",
    "        class_weights=cls_weights_all,\n",
    "    )\n",
    "    trainer_prod.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_full.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths_full.append(state_path)\n",
    "\n",
    "    del trainer_prod, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "swa_state_full = average_states(state_paths_full)\n",
    "torch.save(swa_state_full, os.path.join(artifact_dir, \"best_state_swa_full.pt\"))\n",
    "print(\"\\nâœ“ ModÃ¨le SWA full-train enregistrÃ© : best_state_swa_full.pt\")\n",
    "\n",
    "# Optionnel : sauvegarde du tokenizer / meta-infos\n",
    "tok.save_pretrained(os.path.join(artifact_dir, \"tokenizer_final\"))\n",
    "json.dump(\n",
    "    {\n",
    "        \"best_params\": best_params,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"notes\": \"adapter SWA train+val, puis SWA full-train pour production\",\n",
    "    },\n",
    "    open(os.path.join(artifact_dir, \"run_summary.json\"), \"w\"),\n",
    "    indent=2,\n",
    ")\n",
    "print(\"RÃ©sumÃ© sauvegardÃ©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51290589",
   "metadata": {
    "id": "51290589"
   },
   "outputs": [],
   "source": [
    "# helpers_swa.py --------------------------------------------------------------\n",
    "import os, copy, torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "def load_base(model_name, num_labels, tok, bnb_cfg, lora_cfg):\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(tok.vocab_size)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    return get_peft_model(base, copy.deepcopy(lora_cfg))\n",
    "\n",
    "def average_states(state_paths):\n",
    "    from collections import OrderedDict\n",
    "    avg = OrderedDict()\n",
    "    for p in state_paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(state_paths)).to(torch.float16)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde7145",
   "metadata": {
    "id": "bdde7145"
   },
   "outputs": [],
   "source": [
    "# run_swa_eval.py -------------------------------------------------------------\n",
    "import os, torch, copy\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig\n",
    "# -----------------------------------------------------------------------------\n",
    "artifact_dir = \"./outputs\"\n",
    "state_paths = [\n",
    "    os.path.join(artifact_dir, \"best_state_seed0_val.pt\"),\n",
    "    os.path.join(artifact_dir, \"best_state_seed13_val.pt\"),\n",
    "]\n",
    "\n",
    "# â”€ config rÃ©utilisÃ©e (exactement celle de ton script initial) â”€\n",
    "l_cfg        = LoraConfig(           # mÃªmes valeurs quâ€™avant\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    bias=\"none\", task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg      = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1. SWA hors-ligne\n",
    "swa_state = average_states(state_paths)\n",
    "swa_path  = os.path.join(artifact_dir, \"best_state_swa_val.pt\")\n",
    "torch.save(swa_state, swa_path)\n",
    "print(f\"âœ“ SWA sauvegardÃ© : {swa_path}\")\n",
    "\n",
    "# 2. Ã‰valuation\n",
    "model = load_base(MODEL_NAME, NUM_LABELS, tok, bnb_cfg, l_cfg)\n",
    "model.load_state_dict(torch.load(swa_path, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "test_dl  = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, metrics = eval_loader(test_dl)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96b2e",
   "metadata": {
    "id": "08d96b2e"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MHgOovRy9dDW",
   "metadata": {
    "id": "MHgOovRy9dDW"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Process Facebook & Instagram CSV exports (v2)\n",
    "\n",
    "DiffÃ©rences principales :\n",
    "1. Chemin du modÃ¨le â†’ dossier `outputs/final_model/` issu de trainer.save_model().\n",
    "2. Prompt identique Ã  lâ€™entraÃ®nement : add_generation_prompt=False.\n",
    "3. BitsAndBytes : dtype = bfloat16 si dispo, sinon float16.\n",
    "4. Tout le reste (reprise, sauvegarde incrÃ©mentale, dÃ©tection de langue)\n",
    "   reste inchangÃ©.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Early env tweaks\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"        # supprime lâ€™avertissement\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Standard lib\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Third-party\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USER CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV   = \"/home/lisst_ai/FACEBOOK_concat.csv\"\n",
    "INSTAGRAM_CSV  = \"/home/lisst_ai/INSTAGRAM_concat.csv\"\n",
    "\n",
    "# â€¼ï¸ nouveau rÃ©pertoire modÃ¨le sauvegardÃ© par trainer.save_model()\n",
    "MODEL_DIR      = \"/home/lisst_ai/qlora_project/outputs/final_model\"\n",
    "\n",
    "OUT_FACEBOOK   = \"/home/lisst_ai/gaza_facebook_cleaned.csv\"\n",
    "OUT_INSTAGRAM  = \"/home/lisst_ai/gaza_instagram_cleaned.csv\"\n",
    "\n",
    "BATCH_SIZE          = 64\n",
    "BATCH_SAVE_EVERY    = 100          # sauvegarde toutes les 100 batchs\n",
    "NUM_WORKERS: int | None = None     # None â‡’ tous les cÅ“urs CPU\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONSTANTS & LOGGING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEP               = \" - \"\n",
    "NUM_WORDS_SAMPLE  = 20\n",
    "TARGET_LANG       = \"en\"\n",
    "CONSTRUCTED_COL   = \"constructed_text\"\n",
    "PRED_COL          = \"predicted_category\"\n",
    "MAX_LENGTH        = 512\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(levelname)-8s | %(message)s\")\n",
    "log = logging.getLogger(\"process_sm\")\n",
    "\n",
    "TRAIN_CSV = \"/home/lisst_ai/gaza_classified.csv\"      # fallback si id2label absent\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for val in values:\n",
    "        if not isinstance(val, str):\n",
    "            continue\n",
    "        val_clean = val.strip()\n",
    "        if not val_clean:\n",
    "            continue\n",
    "        current = sep.join(parts).lower()\n",
    "        if val_clean.lower() in current:\n",
    "            continue\n",
    "        parts.append(val_clean)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def detect_lang(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "# â€” worker pour ProcessPoolExecutor (picklable) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def _detect_lang_worker(args):\n",
    "    idx, txt = args\n",
    "    words = txt.split()\n",
    "    sample = \" \".join(words[:NUM_WORDS_SAMPLE])\n",
    "    if not sample.strip():\n",
    "        return idx, False\n",
    "    return idx, detect_lang(sample) == TARGET_LANG\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "def filter_english(texts: list[str],\n",
    "                   *, workers: int | None = NUM_WORKERS) -> list[bool]:\n",
    "    workers = workers or os.cpu_count() or 4\n",
    "    log.info(\"Detecting language on %d texts with %d workersâ€¦\",\n",
    "             len(texts), workers)\n",
    "\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for idx, ok in tqdm(ex.map(_detect_lang_worker,\n",
    "                                   enumerate(texts),\n",
    "                                   chunksize=512),\n",
    "                            total=len(texts),\n",
    "                            desc=\"Lang-detect\",\n",
    "                            unit=\"post\"):\n",
    "            flags[idx] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Model loading & inference\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_classifier(model_dir: str, *, train_csv: str = TRAIN_CSV):\n",
    "    p_cfg     = PeftConfig.from_pretrained(model_dir)\n",
    "    base_name = p_cfg.base_model_name_or_path\n",
    "\n",
    "    # 1. id2label / num_labels â€“ on tente la config dâ€™abord\n",
    "    id2label = getattr(p_cfg, \"id2label\", None)\n",
    "    if id2label:\n",
    "        num_labels = len(id2label)\n",
    "    else:\n",
    "        try:\n",
    "            with open(Path(model_dir) / \"adapter_config.json\") as f:\n",
    "                raw = json.load(f)\n",
    "            num_labels = raw.get(\"num_labels\")\n",
    "            id2label   = {int(k): v for k, v in raw.get(\"id2label\", {}).items()}\n",
    "        except FileNotFoundError:\n",
    "            num_labels = None\n",
    "            id2label   = None\n",
    "\n",
    "    # 2. Fallback : reconstruction Ã  partir du CSV dâ€™entraÃ®nement\n",
    "    if not id2label:\n",
    "        df         = pd.read_csv(train_csv, header=None, names=[\"text\", \"cat\"])\n",
    "        cats       = sorted(df[\"cat\"].unique())\n",
    "        id2label   = {i: c for i, c in enumerate(cats)}\n",
    "        num_labels = len(id2label)\n",
    "        log.info(\"id2label reconstruit depuis %s\", train_csv)\n",
    "\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(\n",
    "        base_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_name,\n",
    "        config=cfg,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_name, padding_side=\"left\")\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "        tok.pad_token = '<pad>'\n",
    "        base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(base, model_dir)\n",
    "    except RuntimeError as e:\n",
    "        log.warning(\"TÃªte LoRA incompatible (%s) â†’ ignore_mismatched_sizes=True\", e)\n",
    "        model = PeftModel.from_pretrained(base, model_dir,\n",
    "                                          ignore_mismatched_sizes=True)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    cats_str = \", \".join(id2label.values())\n",
    "    def build_prompt(txt: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\"You are an expert assistant. \"\n",
    "                         \"Classify the following text into one of these \"\n",
    "                         f\"categories: {cats_str}. \"\n",
    "                         \"Respond with the category label only.\")},\n",
    "            {\"role\": \"user\", \"content\": txt},\n",
    "        ]\n",
    "        # â€¼ï¸ add_generation_prompt=False pour coller Ã  lâ€™entraÃ®nement\n",
    "        return tok.apply_chat_template(messages,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=False).strip()\n",
    "\n",
    "    return tok, model, build_prompt\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Batch prediction with incremental saving\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def incremental_predict(df: pd.DataFrame,\n",
    "                        tok, model, build_prompt,\n",
    "                        *,\n",
    "                        text_col: str = CONSTRUCTED_COL,\n",
    "                        batch_size: int = BATCH_SIZE,\n",
    "                        save_every: int = BATCH_SAVE_EVERY,\n",
    "                        out_path: str | Path | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Remplit df[PRED_COL] in-place pour les lignes oÃ¹ il manque,\n",
    "    en sauvegardant toutes les `save_every` batchs.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # indices Ã  traiter\n",
    "    to_process = df.index[df[PRED_COL].isna() | (df[PRED_COL] == \"\")].tolist()\n",
    "    if not to_process:\n",
    "        log.info(\"Aucune ligne Ã  catÃ©goriser (dÃ©jÃ  complet).\")\n",
    "        return\n",
    "\n",
    "    total_batches = math.ceil(len(to_process) / batch_size)\n",
    "    batch_counter = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(to_process), batch_size),\n",
    "                  desc=\"Batch-predict\",\n",
    "                  total=total_batches,\n",
    "                  unit=\"batch\",\n",
    "                  ncols=300):\n",
    "        batch_idx   = to_process[i: i + batch_size]\n",
    "        batch_texts = df.loc[batch_idx, text_col].tolist()\n",
    "        prompts     = [build_prompt(t) for t in batch_texts]\n",
    "\n",
    "        enc = tok(prompts,\n",
    "                  return_tensors=\"pt\",\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  max_length=MAX_LENGTH)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "        ids = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "        labels = [\n",
    "            model.config.id2label[i]\n",
    "            if isinstance(model.config.id2label, dict)\n",
    "            else model.config.id2label[i]\n",
    "            for i in ids\n",
    "        ]\n",
    "        df.loc[batch_idx, PRED_COL] = labels\n",
    "\n",
    "        batch_counter += 1\n",
    "        if out_path and batch_counter % save_every == 0:\n",
    "            log.info(\"Interim save â†’ %s\", out_path)\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "    if out_path:\n",
    "        log.info(\"Final save â†’ %s\", out_path)\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Per-platform helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Robust CSV loader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError as err:\n",
    "        log.warning(\"Standard parser failed for %s (%s). \"\n",
    "                    \"Retrying with engine='python'â€¦\", path, err)\n",
    "        return pd.read_csv(path,\n",
    "                           engine=\"python\",\n",
    "                           on_bad_lines=\"skip\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Platform pipeline with resume capability\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pipeline(name: str,\n",
    "                 src_csv: str | Path,\n",
    "                 out_csv: str | Path,\n",
    "                 builder_fn) -> None:\n",
    "    out_csv = Path(out_csv)\n",
    "    src_csv = Path(src_csv)\n",
    "\n",
    "    if out_csv.exists():\n",
    "        log.info(\"%s: found existing output (%s) â€“ resume mode.\", name, out_csv)\n",
    "        df = safe_read_csv(out_csv)\n",
    "        # reconstruit le texte si absent (rare)\n",
    "        if CONSTRUCTED_COL not in df.columns:\n",
    "            raw = safe_read_csv(src_csv)\n",
    "            df_texts = builder_fn(raw)[[CONSTRUCTED_COL]]\n",
    "            df = df.join(df_texts)\n",
    "    else:\n",
    "        log.info(\"%s: loading raw CSVâ€¦\", name)\n",
    "        raw = safe_read_csv(src_csv)\n",
    "        df = builder_fn(raw)\n",
    "        df[PRED_COL] = pd.NA                         # nouvelle colonne\n",
    "\n",
    "    # filtrage langue pour les lignes non catÃ©gorisÃ©es\n",
    "    mask_uncat = df[PRED_COL].isna() | (df[PRED_COL] == \"\")\n",
    "    to_check   = df.loc[mask_uncat, CONSTRUCTED_COL].tolist()\n",
    "    if to_check:\n",
    "        log.info(\"%s: language filtering (%d lignes)â€¦\", name, len(to_check))\n",
    "        flags = filter_english(to_check)\n",
    "        df = df.loc[~mask_uncat | pd.Series(flags,\n",
    "                                            index=df.loc[mask_uncat].index)\n",
    "                    ].reset_index(drop=True)\n",
    "\n",
    "    # classification\n",
    "    tok, model, build_prompt = load_classifier(MODEL_DIR)\n",
    "    incremental_predict(df, tok, model, build_prompt,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        save_every=BATCH_SAVE_EVERY,\n",
    "                        out_path=out_csv)\n",
    "\n",
    "    log.info(\"%s: completed (%d rows total).\", name, len(df))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main routine\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    run_pipeline(\"Facebook\",\n",
    "                 FACEBOOK_CSV,\n",
    "                 OUT_FACEBOOK,\n",
    "                 process_facebook)\n",
    "\n",
    "    run_pipeline(\"Instagram\",\n",
    "                 INSTAGRAM_CSV,\n",
    "                 OUT_INSTAGRAM,\n",
    "                 process_instagram)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be0a7a",
   "metadata": {
    "id": "93be0a7a"
   },
   "outputs": [],
   "source": [
    "sklearn.model_selection.cross_val_score(..., return_estimator=True) â†’ bootstrap des scores pour IC 95 % ; on publie toujours : Â« MCC 0.71 Â± 0.03 Â».\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "093949c9ff6c47109c73936559022568": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_2debc053862a4c6ea56fd8bd79a73459",
      "max": 17078136,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b29b016319b14adab629eb2647d3a355",
      "tabbable": null,
      "tooltip": null,
      "value": 17078136
     }
    },
    "0b7ec87b9c15483f85a55e7610ac2642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "0d3cba73f4fc4fa68037c32ca2035efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "0dbf1d6e218c4930b5beaa843c985fe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_25f3c9542b2b4467b8995ba923a5fefd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e12099fda04b4db4ae4405ea17776f74",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer_config.json:â€‡"
     }
    },
    "1497d4b7748348b6ba6cd32593194e1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b0838891b6f46c68400e1ed3c912a81",
       "IPY_MODEL_093949c9ff6c47109c73936559022568",
       "IPY_MODEL_b5341442f83e4b30ac45d26a068caa96"
      ],
      "layout": "IPY_MODEL_33659f055420477d96ed9536fc933440",
      "tabbable": null,
      "tooltip": null
     }
    },
    "179aead8a0d84108bd3d6b68c68c89d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b3d07ab5e6849ccaf1c54ed915db588": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_280b5d5f5b5d4358ba6a4fe7caa8d565",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0d3cba73f4fc4fa68037c32ca2035efb",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡414/414â€‡[00:00&lt;00:00,â€‡55.2kB/s]"
     }
    },
    "1c34568ec32d4ba4a38e495705928d90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "236a9c2d5ebd485ebce91732a03c2c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f3c9542b2b4467b8995ba923a5fefd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "279fba77a40a4a73b5f7ddc30dfcc9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a6cbb34af4d04414a0bc2dc11a191298",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de97cc7484b74b92905f2d67c184cd0a",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "280b5d5f5b5d4358ba6a4fe7caa8d565": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c102e3e0c4f4d6f99abbf728b8e9e28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "2debc053862a4c6ea56fd8bd79a73459": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e10d96515ad451b87d50265639c2552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "33659f055420477d96ed9536fc933440": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364f174037914d769f413641bdc67f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37f74260cc4b4311b1edf58a3bd6c8d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_89c6b12b18704bf8a1f9a49c32a1cd80",
      "max": 414,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6566f97e6225481f8bb153dd3151dcf6",
      "tabbable": null,
      "tooltip": null,
      "value": 414
     }
    },
    "4aacc61fdb374caebed92f9096b21944": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4adc0eadab4b441c9346d96e50a21f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_956baf5852b44eb097ea07b913f0b9cb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_364f174037914d769f413641bdc67f08",
      "tabbable": null,
      "tooltip": null,
      "value": 1
     }
    },
    "4ef56778e4f54c179af64e0ec72750db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_6af59cb80b32457ba43ff1135128c83d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7d43a6ed71f469e88b448a2e681920e",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡4/4â€‡[00:16&lt;00:00,â€‡â€‡5.69s/it]"
     }
    },
    "5ad0ee1341244193a6743bd2a2207833": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83db65deecc9408d99574c0c5c74a552",
       "IPY_MODEL_279fba77a40a4a73b5f7ddc30dfcc9e0",
       "IPY_MODEL_4ef56778e4f54c179af64e0ec72750db"
      ],
      "layout": "IPY_MODEL_8fe1094fb7d546b5977f1f4b75972e16",
      "tabbable": null,
      "tooltip": null
     }
    },
    "5b15e0aef23345918af904b64a911ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd368b1abe74b54a8ef5c23ae2cfeb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6566f97e6225481f8bb153dd3151dcf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6af59cb80b32457ba43ff1135128c83d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70891aac81c64db488a3b38c99d26f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "7241fcdcbd5d4679866243d592b29094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5b15e0aef23345918af904b64a911ed6",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd6a0872a8c8439ca657648a7858834d",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "82bfbabc74e04380a49b6083c8388813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_179aead8a0d84108bd3d6b68c68c89d8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c62fff4e2c1f4cbd8b376d740e066325",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡4/4â€‡[00:16&lt;00:00,â€‡â€‡5.76s/it]"
     }
    },
    "83db65deecc9408d99574c0c5c74a552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_236a9c2d5ebd485ebce91732a03c2c5f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2e10d96515ad451b87d50265639c2552",
      "tabbable": null,
      "tooltip": null,
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "873e6d9585f0426b88eb064c4c43c73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "89c6b12b18704bf8a1f9a49c32a1cd80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b0838891b6f46c68400e1ed3c912a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1c34568ec32d4ba4a38e495705928d90",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70891aac81c64db488a3b38c99d26f74",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "8fe1094fb7d546b5977f1f4b75972e16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9134ea58f63345d696bddbfeacf72b37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94c5951d08d0450485c607bc737a4770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a08c06618f61423783448d2d965d65db",
       "IPY_MODEL_37f74260cc4b4311b1edf58a3bd6c8d5",
       "IPY_MODEL_1b3d07ab5e6849ccaf1c54ed915db588"
      ],
      "layout": "IPY_MODEL_d6d08beaf22b43128ecb74b24ae2a353",
      "tabbable": null,
      "tooltip": null
     }
    },
    "956baf5852b44eb097ea07b913f0b9cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9715b7d58c334f8b9cb16e17483fdc43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a08c06618f61423783448d2d965d65db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_9134ea58f63345d696bddbfeacf72b37",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_873e6d9585f0426b88eb064c4c43c73e",
      "tabbable": null,
      "tooltip": null,
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "a6cbb34af4d04414a0bc2dc11a191298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f0ccfbe9214be3b4613e6d25d735ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5f17f05acdb496c869d63a4103bc3ec",
       "IPY_MODEL_7241fcdcbd5d4679866243d592b29094",
       "IPY_MODEL_82bfbabc74e04380a49b6083c8388813"
      ],
      "layout": "IPY_MODEL_5dd368b1abe74b54a8ef5c23ae2cfeb5",
      "tabbable": null,
      "tooltip": null
     }
    },
    "b1faa0869bf54d5399433efe4e083f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b29b016319b14adab629eb2647d3a355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5341442f83e4b30ac45d26a068caa96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e80a79c956d7489db2f4be66166ce017",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b1faa0869bf54d5399433efe4e083f8a",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡17.1M/17.1Mâ€‡[00:01&lt;00:00,â€‡13.3MB/s]"
     }
    },
    "b637c5cd40dc450cbb7573e2a8935857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_9715b7d58c334f8b9cb16e17483fdc43",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b7ec87b9c15483f85a55e7610ac2642",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡181k/?â€‡[00:00&lt;00:00,â€‡19.9MB/s]"
     }
    },
    "c5f17f05acdb496c869d63a4103bc3ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_f3dd43f426a541ad936365e395cc9dc5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2c102e3e0c4f4d6f99abbf728b8e9e28",
      "tabbable": null,
      "tooltip": null,
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "c62fff4e2c1f4cbd8b376d740e066325": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "c7d43a6ed71f469e88b448a2e681920e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "cd6a0872a8c8439ca657648a7858834d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6d08beaf22b43128ecb74b24ae2a353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d859487e62ae4f1da1284846bc7abcb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dbf1d6e218c4930b5beaa843c985fe0",
       "IPY_MODEL_4adc0eadab4b441c9346d96e50a21f6a",
       "IPY_MODEL_b637c5cd40dc450cbb7573e2a8935857"
      ],
      "layout": "IPY_MODEL_4aacc61fdb374caebed92f9096b21944",
      "tabbable": null,
      "tooltip": null
     }
    },
    "de97cc7484b74b92905f2d67c184cd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e12099fda04b4db4ae4405ea17776f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "e80a79c956d7489db2f4be66166ce017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3dd43f426a541ad936365e395cc9dc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
