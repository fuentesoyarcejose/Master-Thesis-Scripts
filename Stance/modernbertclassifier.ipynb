{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac37768",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U ipywidgets\n",
    "!pip install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(\"\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    module=\"torch.utils.checkpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c49600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Gaza-stance classification (Gemini) â€“ v6, prompt â€œStanceClassifier-Gaza-v2â€\n",
    "# --------------------------------------------------------------------------\n",
    "# â€¢ Reconstruit le texte complet depuis les CSV Facebook & Instagram.\n",
    "# â€¢ Filtre les posts non anglophones puis Ã©chantillonne 0,5 %.\n",
    "# â€¢ Envoie chaque post Ã  Gemini-2.5-flash-preview avec le prompt ci-dessous.\n",
    "# â€¢ La rÃ©ponse doit Ãªtre **exactement** un label sur une ligne.\n",
    "# â€¢ Sauvegarde CSV : text, gpt_category\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import logging, os, sys\n",
    "logging.basicConfig(level=logging.WARNING,           # root silencieux\n",
    "                    format=\"[%(levelname)s] %(message)s\",\n",
    "                    force=True)                      # â† reset complet\n",
    "\n",
    "log = logging.getLogger(\"pipeline\")                  # ton logger\n",
    "log.setLevel(logging.INFO)                           # un seul handler (root)\n",
    "# pas dâ€™ajout de handler, on laisse propager au root\n",
    "\n",
    "for name in (\"google\", \"google_genai\", \"google.api_core\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)  # stop INFO/DEBUG\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV  = \"/home/lisst_ai/DATA/FACEBOOK_concat.csv\"\n",
    "INSTAGRAM_CSV = \"/home/lisst_ai/DATA/INSTAGRAM_concat.csv\"\n",
    "\n",
    "GOOGLE_API_KEY  = \"AIzaSyA5DEVewU4AVikamHxbGwObLeyu1IfgveM\"\n",
    "MODEL_NAME      = \"gemini-2.5-flash-preview-04-17\"\n",
    "OUTPUT_FILENAME = \"/home/lisst_ai/DATA/gaza_stance_sampled_classified.csv\"\n",
    "\n",
    "SAMPLE_FRAC      = 0.005   # 0,5 %\n",
    "RANDOM_STATE     = 42\n",
    "TARGET_LANG      = \"en\"\n",
    "NUM_WORDS_SAMPLE = 100\n",
    "NUM_WORKERS      = None\n",
    "\n",
    "SEP             = \" - \"\n",
    "CONSTRUCTED_COL = \"_text\"\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\",\n",
    "    \"Anti-War_Pro-Peace\",\n",
    "    \"Other\",\n",
    "    \"Off-topic\",\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from typing import Sequence, List\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SYSTEM_INSTRUCTION = \"\"\"\n",
    "You are â€œStanceClassifier-Gaza-v2â€.\n",
    "Task: read one English-language social-media post about the 2023-2025 Gaza War and\n",
    "output **exactly one** label from the list below.\n",
    "\n",
    "Think step-by-step **silently** (donâ€™t reveal your reasoning).  \n",
    "Return only the label on a single line, nothing else.\n",
    "\n",
    "### Allowed labels\n",
    "1. Pro-Palestinian  \n",
    "2. Pro-Israeli  \n",
    "3. Anti-War_Pro-Peace  \n",
    "4. Other  \n",
    "5. Off-topic          â† use when the post is not about the Gaza War at all\n",
    "\n",
    "### Definitions & guidance\n",
    "\n",
    "**Pro-Palestinian**  \n",
    "Any stance that primarily supports Palestinians or blames Israel.  \n",
    "âœ” Detect even subtle cues: ğŸ‡µğŸ‡¸ emoji, â€œfrom the river to the seaâ€, focus on Palestinian victims, words like â€œgenocideâ€, â€œoccupationâ€, â€œapartheidâ€, or praise for resistance.  \n",
    "âœ˜ A single humanitarian mention of both sides â†’ see Anti-War_Pro-Peace unless Israel is clearly blamed.\n",
    "\n",
    "**Pro-Israeli**  \n",
    "Primarily supports Israel or blames Hamas/Palestinian side.  \n",
    "âœ” Cues: ğŸ‡®ğŸ‡±, â€œright to self-defenceâ€, â€œterrorist organisationâ€, hostage hashtags, emphasis on Hamas using human shields.  \n",
    "âœ˜ Balanced calls for restraint â†’ Anti-War_Pro-Peace unless Hamas is clearly condemned more than Israel.\n",
    "\n",
    "**Anti-War_Pro-Peace**  \n",
    "Core message = stop the violence / protect civilians on *both* sides, with balanced language.\n",
    "\n",
    "**Other** (use sparingly)  \n",
    "Mentions the war but no discernible leaning **after** checking for:  \n",
    "â€£ sarcasm or irony  \n",
    "â€£ vocabulary that allocates guilt (even implicitly)  \n",
    "â€£ spotlight on one populationâ€™s suffering  \n",
    "â€£ selective historical references  \n",
    "\n",
    "**Off-topic**  \n",
    "Post lacks substantive reference to the Gaza War (generic memes, ads, ambiguous â€œPray for themâ€ with no context, etc.).\n",
    "\n",
    "### Checklist before choosing â€œOtherâ€\n",
    "1. Does the wording, emoji, or hashtag tilt sympathy toward one side?  \n",
    "2. Are perpetrators or victims named asymmetrically?  \n",
    "3. Is there an implied moral judgment?  \n",
    "If **any** answer is yes, assign the corresponding stance; else, Other.\n",
    "\n",
    "### Tie-break rules\n",
    "* Stance + peace call â†’ keep the stance.  \n",
    "* Balanced criticism but one side more severe â†’ assign that side.  \n",
    "* Unsure between Anti-War and partisan â†’ choose the partisan stance.\n",
    "\n",
    "### Output format\n",
    "Return exactly one of:  \n",
    "Pro-Palestinian | Pro-Israeli | Anti-War_Pro-Peace | Other | Off-topic\n",
    "\"\"\".strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. GEMINI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log.info(\"Initialisation API Geminiâ€¦\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "GEN_CFG = types.GenerateContentConfig(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    temperature=0.1,\n",
    "    safety_settings=[\n",
    "        types.SafetySetting(category=c, threshold=types.HarmBlockThreshold.BLOCK_NONE)\n",
    "        for c in (\n",
    "            types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "            types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "            types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "            types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        )\n",
    "    ],\n",
    "    thinking_config=genai.types.ThinkingConfig(\n",
    "      thinking_budget=0\n",
    "    )\n",
    ")\n",
    "log.info(\"ModÃ¨le prÃªt : %s\", MODEL_NAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. UTILITAIRES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ZERO_WIDTH_RE = re.compile(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF]\")\n",
    "\n",
    "def strip_invisible(t: str) -> str:\n",
    "    return ZERO_WIDTH_RE.sub(\"\", t)\n",
    "\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for v in values:\n",
    "        if isinstance(v, str):\n",
    "            v = v.strip()\n",
    "            if v and v.lower() not in sep.join(parts).lower():\n",
    "                parts.append(v)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    log.info(\"Lecture CSV : %s\", path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    log.info(\" â†’ %s lignes\", f\"{len(df):,}\")\n",
    "    return df\n",
    "\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dÃ©tection langue (multiprocessing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _detect(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _lang_worker(args):\n",
    "    i, txt = args\n",
    "    sample = \" \".join(txt.split()[:NUM_WORDS_SAMPLE])\n",
    "    return i, (_detect(sample) == TARGET_LANG)\n",
    "\n",
    "def filter_english(texts: list[str]) -> list[bool]:\n",
    "    workers = NUM_WORKERS or (os.cpu_count() or 4)\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for i, ok in ex.map(_lang_worker, enumerate(texts), chunksize=512):\n",
    "            flags[i] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. CLASSIFY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def classify(post: str) -> str:\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=post,\n",
    "            config=GEN_CFG,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.error(\"Gemini error : %s â€“ exit.\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not resp.text:\n",
    "        return \"INVALID\"\n",
    "\n",
    "    label = resp.text.splitlines()[0].strip()\n",
    "    return label if label in ALLOWED_CATEGORIES else \"INVALID\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    df_fb = process_facebook(safe_read_csv(FACEBOOK_CSV))\n",
    "    df_ig = process_instagram(safe_read_csv(INSTAGRAM_CSV))\n",
    "\n",
    "    log.info(\"Filtre de langue : %d %d\", len(df_fb), len(df_ig))  # avant le filtrage\n",
    "    for df in (df_fb, df_ig):\n",
    "        mask = filter_english(df[CONSTRUCTED_COL].tolist())\n",
    "        df.drop(index=df.index[~pd.Series(mask)], inplace=True)\n",
    "\n",
    "    log.info(\"AprÃ¨s filtre de langue : %d %d\", len(df_fb), len(df_ig))\n",
    "    \n",
    "    df_sampled = pd.concat([\n",
    "        df_fb.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE),\n",
    "        df_ig.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "    ], ignore_index=True)\n",
    "    log.info(\"Ã‰chantillon : %s posts\", f\"{len(df_sampled):,}\")\n",
    "\n",
    "    texts = df_sampled[CONSTRUCTED_COL].apply(strip_invisible).tolist()\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"Aucun texte anglais aprÃ¨s filtrage.\")\n",
    "\n",
    "    log.info(\"Classification Geminiâ€¦\")\n",
    "    cats = [classify(t) for t in tqdm(texts, desc=\"Gemini\", unit=\"post\")]\n",
    "\n",
    "    out = pd.DataFrame({\"text\": texts, \"gpt_category\": cats})\n",
    "    n_inv = (out[\"gpt_category\"] == \"INVALID\").sum()\n",
    "    if n_inv:\n",
    "        log.warning(\"%d rÃ©ponses INVALID\", n_inv)\n",
    "\n",
    "    out.query(\"gpt_category != 'INVALID'\").to_csv(OUTPUT_FILENAME, index=False)\n",
    "    log.info(\"Fichier enregistrÃ© : %s\", OUTPUT_FILENAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EntrÃ©e script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    log.info(\"Lancement pipelineâ€¦\")\n",
    "    main()\n",
    "    log.info(\"TerminÃ©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8043ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFF_BATCH_TARGET = 32\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Llama-3-70B-Instruct â€¢ LoRA â€¢ 4-bit NF4 â€¢ Optuna â€¢ Cosine LR â€¢ MCC\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#   â€¢ EntrÃ©e tronquÃ©e Ã  512 tokens\n",
    "#   â€¢ Quantisation bitsandbytes 4-bit NF4\n",
    "#   â€¢ Fusion des classes : Other + Off-topic + Anti-War_Pro-Peace â†’ Neutral\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import os, gc, random, threading, _thread, json, math, copy\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef\n",
    "\n",
    "import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "SEED = 42\n",
    "set_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "MODEL_NAME = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "NUM_EPOCHS, EARLY_STOP_PATIENCE, TIMEOUT_TRAIN = 40, 6, 3 * 36000\n",
    "KFOLD, N_TRIALS = 5, 3\n",
    "artifact_dir = Path(\"./outputs\"); artifact_dir.mkdir(exist_ok=True, parents=True)\n",
    "SINGLE_FOLD_ONLY = False\n",
    "MAX_LENGTH = 512\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lecture du CSV + fusion -------------------------\n",
    "CSV_PATH = \"/home/lisst_ai/DATA/gaza_stance_sampled_classified.csv\"\n",
    "df = pd.read_csv(CSV_PATH)          # colonnes: text, gpt_category / cat\n",
    "\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "# â”€â”€â”€ Fusion des classes mineures â†’ Neutral\n",
    "FUSE_MAP = {\n",
    "    \"Pro-Palestinian\":        \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\":            \"Pro-Israeli\",\n",
    "    \"Other\":                  \"Neutral\",\n",
    "    \"Off-topic\":              \"Neutral\",\n",
    "    \"Anti-War_Pro-Peace\":     \"Neutral\",\n",
    "}\n",
    "df[\"cat\"] = df[\"cat\"].map(FUSE_MAP)             # applique la fusion\n",
    "df.dropna(subset=[\"cat\"], inplace=True)         # sÃ©curitÃ© : lignes non mappÃ©es\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Mappings label â†” id ------------------------------\n",
    "cats      = sorted(df[\"cat\"].unique())          # ['Neutral', 'Pro-Israeli', 'Pro-Palestinian']\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Diagnostics --------------------------------------\n",
    "print(f\"{len(df):,} exemples chargÃ©s (classes fusionnÃ©es).\")\n",
    "print(\"RÃ©partition des classes :\", df[\"label_id\"].value_counts().to_dict())\n",
    "print(\"label2id :\", label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Listes utiles / constantes -----------------------\n",
    "messages   = df[\"text\"].tolist()\n",
    "labels     = df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)                           # 3\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Split train / val / test -------------------------\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f90a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc(txts: List[str]):\n",
    "    prompts = [build_prompt(t) for t in txts]\n",
    "    return tok(                     # â†Â important\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=False    # plus de doubleÂ BOS\n",
    "    )\n",
    "\n",
    "def build_prompt(txt: str) -> str:\n",
    "    \"\"\"Prompt Mistral officiel, sÃ»r pour l'encodeur HF.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": f\"You are an expert assistant. Classify the following text into one of these \"\n",
    "                    f\"categories: {cats_string}. Respond with the category label only.\"},\n",
    "        {\"role\": \"user\", \"content\": txt}\n",
    "    ]\n",
    "    return tok.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,                # on renvoie une string\n",
    "        add_generation_prompt=False     # ajoute automatiquement le tag assistant\n",
    "    ).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt instructif -----------------------------------------\n",
    "cats_string = \", \".join(cats)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tokenizer --------------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")  # â† leftâ€‘pad plus sÃ»r\n",
    "if tok.pad_token_id is None or tok.pad_token_id == tok.eos_token_id:\n",
    "    # 1) on crÃ©e un vrai token pad\n",
    "    tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "    # 2) on pointe officiellement dessus\n",
    "    tok.pad_token = '<pad>'\n",
    "\n",
    "# IMPORTANT : redimensionner lâ€™embedding AVANT dâ€™injecter LoRA\n",
    "VOCAB = len(tok)\n",
    "\n",
    "class TokenizedDS(Dataset):\n",
    "    def __init__(self, e, y):\n",
    "        self.e, self.y = e, y\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        d = {k: torch.tensor(v[i]) for k, v in self.e.items()}\n",
    "        d[\"labels\"] = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return d\n",
    "    \n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WeightedCETrainer ----------------------------------------\n",
    "class WeightedCETrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.float()\n",
    "\n",
    "    # â–¿ ajouter le nouvel argument (ou **kwargs) â–¿\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: int | None = None,   # â† NEW\n",
    "        **kwargs,                                # â† compatibilitÃ© future\n",
    "    ):\n",
    "        labels  = inputs[\"labels\"]\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits  = outputs.logits\n",
    "\n",
    "        weight = self.class_weights.to(logits.device)\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits,\n",
    "            labels,\n",
    "            weight=weight,    # pondÃ©ration des classes\n",
    "            reduction=\"mean\"  # moyenne directe sur le batch\n",
    "        )\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics ----------------------------------------------------\n",
    "\n",
    "def cat_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def compute_metrics(ev: EvalPrediction):\n",
    "    y_hat = np.argmax(ev.predictions, axis=1)\n",
    "    g = cat_metrics(ev.label_ids, y_hat)\n",
    "    return {\n",
    "        \"eval_accuracy\": g[\"accuracy\"],\n",
    "        \"eval_f1_macro\": g[\"f1_macro\"],\n",
    "        \"eval_f1_micro\": g[\"f1_micro\"],\n",
    "        \"eval_mcc\": g[\"mcc\"],\n",
    "    }\n",
    "\n",
    "\n",
    "te_enc = enc(msg_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Timeout helper --------------------------------------------\n",
    "\n",
    "def timeout_train(trainer, seconds):\n",
    "    t = threading.Thread(target=trainer.train)\n",
    "    t.start(); t.join(seconds)\n",
    "    if t.is_alive():\n",
    "        _thread.interrupt_main(); t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d465b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©-requis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, gc, copy, json, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import optuna\n",
    "\n",
    "# (on suppose que tous les imports HF, LoRA, votre collator, compute_metrics,\n",
    "#  WeightedCETrainer, enc(), TokenizedDS, timeout_train(), etc. sont dÃ©jÃ  prÃ©sents)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "artifact_dir = \"./artifacts\"; os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chargement et dÃ©coupage du corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df = df.rename(columns={\"gpt_category\": \"cat\"})\n",
    "\n",
    "cats       = sorted(df[\"cat\"].unique())\n",
    "label2id   = {c: i for i, c in enumerate(cats)}\n",
    "id2label   = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS       = len(cats)\n",
    "\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(r_lora, lora_dropout, use_bf16):\n",
    "    \"\"\"Construit un modÃ¨le QLoRA + classification, sans entraÃ®nement.\"\"\"\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    )\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id      = tok.pad_token_id\n",
    "    base.config.use_cache         = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    l_cfg = LoraConfig(\n",
    "        r=r_lora,\n",
    "        lora_alpha=2 * r_lora,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    model = get_peft_model(base, l_cfg)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad: p.data = p.data.float()\n",
    "    return model\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optuna objective  (validation croisÃ©e) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def objective(trial):\n",
    "    # Hyper-paramÃ¨tres\n",
    "    lr            = trial.suggest_float(\"lr\", 5e-5, 5e-4, log=True)\n",
    "    bsz           = trial.suggest_categorical(\"bsz\", [16, 32])\n",
    "    r_lora        = trial.suggest_int(\"lora_r\", 8, 32, step=8)\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.01, 0.1, log=True)\n",
    "\n",
    "    grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = not use_bf16\n",
    "    skf      = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_mcc, best_fold_state, best_fold_metric = [], None, -1.0\n",
    "    VAL_FOLDS_TO_USE = [0, 2]\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(skf.split(msg_tv, lab_tv)):\n",
    "        if fold_idx not in VAL_FOLDS_TO_USE:\n",
    "            continue\n",
    "\n",
    "        # jeux train / val encodÃ©s\n",
    "        msg_tr = [msg_tv[i] for i in tr_idx]; lab_tr = [lab_tv[i] for i in tr_idx]\n",
    "        msg_va = [msg_tv[i] for i in va_idx]; lab_va = [lab_tv[i] for i in va_idx]\n",
    "        train_ds = TokenizedDS(enc(msg_tr), lab_tr)\n",
    "        val_ds   = TokenizedDS(enc(msg_va), lab_va)\n",
    "\n",
    "        # pondÃ©ration des classes\n",
    "        counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "        cls_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "        model = build_model(r_lora, lora_dropout, use_bf16)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"{artifact_dir}/trial_{trial.number}/fold_{fold_idx}\",\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            per_device_train_batch_size=bsz,\n",
    "            per_device_eval_batch_size=32,\n",
    "            gradient_accumulation_steps=grad_acc,\n",
    "            warmup_ratio=0.08,\n",
    "            learning_rate=lr,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=use_fp16,\n",
    "            gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",               # pas besoin de sauvegarde intermÃ©diaire\n",
    "            seed=SEED,\n",
    "            logging_strategy=\"epoch\",\n",
    "            label_names=[\"labels\"],\n",
    "            load_best_model_at_end=False,\n",
    "        )\n",
    "\n",
    "        trainer = WeightedCETrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "            class_weights=cls_weights,\n",
    "        )\n",
    "\n",
    "        # entraÃ®nement (protÃ©gÃ© par timeout)\n",
    "        timeout_train(trainer, TIMEOUT_TRAIN)\n",
    "        mcc = trainer.evaluate(val_ds)[\"eval_mcc\"]\n",
    "        fold_mcc.append(mcc)\n",
    "\n",
    "        if mcc >= best_fold_metric:\n",
    "            best_fold_metric = mcc\n",
    "            best_fold_state  = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        del trainer, model\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    mean_mcc = float(np.mean(fold_mcc)) if fold_mcc else -1.0\n",
    "    trial.set_user_attr(\"best_state\", best_fold_state)\n",
    "    return mean_mcc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lancement de la recherche Optuna â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params\n",
    "best_state   = best_trial.user_attrs[\"best_state\"]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Reconstruction & chargement du meilleur modÃ¨le â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "best_model   = build_model(\n",
    "    r_lora       = best_params[\"lora_r\"],\n",
    "    lora_dropout = best_params[\"lora_dropout\"],\n",
    "    use_bf16     = use_bf16,\n",
    ")\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©paration du jeu test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_ds = TokenizedDS(enc(msg_te), lab_te)\n",
    "trainer = WeightedCETrainer(\n",
    "    model=best_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"{artifact_dir}/best_model_mcc\",\n",
    "        per_device_eval_batch_size=32,\n",
    "        dataloader_drop_last=False,\n",
    "        seed=SEED,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test MCC :\", round(test_metrics[\"eval_mcc\"], 4))\n",
    "print(\"Test F1  :\", round(test_metrics[\"eval_f1_macro\"], 4))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sauvegarde finale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_dir = f\"{artifact_dir}/best_model_mcc\"\n",
    "best_model.save_pretrained(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "with open(os.path.join(save_dir, \"best_params.json\"), \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(f\"ModÃ¨le et tokenizer sauvegardÃ©s dans Â« {save_dir} Â».\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17559",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42308a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "  Parameters: lr=3.15e-04, bsz=32, lora_r=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b52b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'lr': 0.000315, 'bsz': 32, 'lora_r': 16, 'lora_dropout': 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c166c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Phase finale  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# 1)   Re-chargement du CSV  (Ã©tat vierge)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "cats      = sorted(df[\"cat\"].unique())\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)\n",
    "\n",
    "# 2)   DÃ©coupage train / val / test\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.10, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "\n",
    "# 3)   Encodage\n",
    "tr_enc, va_enc, te_enc = enc(msg_tr), enc(msg_va), enc(msg_te)\n",
    "train_ds = TokenizedDS(tr_enc, lab_tr)\n",
    "val_ds   = TokenizedDS(va_enc, lab_va)\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "\n",
    "# 4)   Hyper-paramÃ¨tres Optuna retenus\n",
    "best_params = json.load(open(artifact_dir / \"best_params.json\"))\n",
    "best_params = {'lr': 0.000315, 'bsz': 32, 'lora_r': 64, 'lora_dropout': 0.1}\n",
    "\n",
    "lr            = best_params[\"lr\"]\n",
    "bsz           = best_params[\"bsz\"]\n",
    "r_lora        = best_params[\"lora_r\"]\n",
    "lora_dropout  = best_params[\"lora_dropout\"]\n",
    "\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = not use_bf16\n",
    "\n",
    "# 5)   Instanciation modÃ¨le + LoRA\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "base.config.use_cache = False\n",
    "base.config.use_paged_attention = True\n",
    "if USE_GRADIENT_CHECKPOINTING:\n",
    "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "l_cfg = LoraConfig(\n",
    "    r=r_lora,\n",
    "    lora_alpha=2 * r_lora,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(base, l_cfg)\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "# 6)   PondÃ©ration des classes\n",
    "counts       = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "class_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "# 7)   Arguments dâ€™entraÃ®nement finaux\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{artifact_dir}/final_run\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=bsz,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    warmup_ratio=0.08,\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=use_bf16,\n",
    "    fp16=use_fp16,\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    logging_strategy=\"epoch\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = WeightedCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=DataCollatorWithPadding(tok),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "# 8)   EntraÃ®nement + Ã©valuation finale\n",
    "timeout_train(trainer, TIMEOUT_TRAIN)          # <-- garde la mÃªme limite que plus haut\n",
    "print(\"Best val MCC:\", trainer.state.best_metric)\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test set :\", {k: round(v, 4) for k, v in test_metrics.items()})\n",
    "\n",
    "# (facultatif) Sauvegarde du modÃ¨le affinÃ©\n",
    "trainer.save_model(f\"{artifact_dir}/final_model\")\n",
    "tok.save_pretrained(f\"{artifact_dir}/final_model\")\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test set : {'eval_accuracy': 0.7943, 'eval_f1_macro': 0.6463, 'eval_f1_micro': 0.7943, 'eval_mcc': 0.6533, 'eval_loss': 1.4161, 'eval_runtime': 55.0543, 'eval_samples_per_second': 11.57, 'eval_steps_per_second': 0.363, 'epoch': 16.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c070b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scores dÃ©taillÃ©s par classe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) PrÃ©dictions brutes sur le test set\n",
    "pred_res = trainer.predict(test_ds)\n",
    "y_true   = pred_res.label_ids\n",
    "y_pred   = np.argmax(pred_res.predictions, axis=1)\n",
    "\n",
    "# 2) Rapport complet (precision / recall / f1 / support)\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(NUM_LABELS)],\n",
    "    digits=4,\n",
    "    output_dict=True          # â† pour lâ€™avoir aussi sous forme dict / DataFrame\n",
    ")\n",
    "print(\"\\n=== Classification report ===\")\n",
    "print(pd.DataFrame(report).T)\n",
    "\n",
    "# 3) Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{id2label[i]}\"  for i in range(NUM_LABELS)],\n",
    "                     columns=[f\"pred_{id2label[i]}\" for i in range(NUM_LABELS)])\n",
    "print(\"\\n=== Confusion matrix ===\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc36e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Post-Optuna : 2 seeds + SWA, test, puis full-train final\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, gc, math, copy, random, numpy as np, torch\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dossiers / fichiers Optuna dÃ©jÃ  crÃ©Ã©s\n",
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "\n",
    "# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def average_states(paths):\n",
    "    avg = OrderedDict()\n",
    "    for p in paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(paths)).to(torch.float16)\n",
    "    return avg\n",
    "\n",
    "# â”€â”€â”€ Configs rÃ©utilisables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / best_params[\"bsz\"]))\n",
    "l_cfg = LoraConfig(\n",
    "    r=best_params[\"lora_r\"],\n",
    "    lora_alpha=2 * best_params[\"lora_r\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  A.   train+val  (90 %)  â†’  deux seeds + SWA\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase A : 2 seeds sur train+val ==========\")\n",
    "\n",
    "# mini-val interne 10 % pour lâ€™early-stop\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.1, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "tr_enc, va_enc = enc(msg_tr), enc(msg_va)\n",
    "train_ds, val_ds = TokenizedDS(tr_enc, lab_tr), TokenizedDS(va_enc, lab_va)\n",
    "\n",
    "cls_counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "cls_weights = torch.tensor(\n",
    "    cls_counts.sum() / (NUM_LABELS * (cls_counts + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Re-train seed {seed} (train+val) â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_val\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc\",\n",
    "        greater_is_better=True,\n",
    "        seed=seed,\n",
    "        logging_strategy=\"epoch\",\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "        class_weights=cls_weights,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_val.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths.append(state_path)\n",
    "\n",
    "    del trainer, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loader(loader, model):\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(model.device)\n",
    "            logits = model(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            ).logits\n",
    "            preds.extend(torch.argmax(logits, -1).cpu())\n",
    "            true.extend(batch[\"labels\"].cpu())\n",
    "    return preds, true, cat_metrics(true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch, os\n",
    "\n",
    "def load_lora_model(state_path: str):\n",
    "    \"\"\"\n",
    "    Reconstruit le backbone + LoRA et recharge les poids sauvegardÃ©s.\n",
    "    Rien dâ€™autre nâ€™est modifiÃ©.\n",
    "    \"\"\"\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "\n",
    "    state = torch.load(state_path, map_location=\"cpu\")\n",
    "    # seules les tÃªtes LoRA ont Ã©tÃ© sauvegardÃ©es ; on les recharge sans toucher au reste\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Ã‰valuation -------------------------------------------------------------\n",
    "\n",
    "results = {}\n",
    "\n",
    "for state_path in state_paths:          # state_paths = [\"...seed0_val.pt\", \"...seed13_val.pt\"]\n",
    "    model = load_lora_model(state_path)\n",
    "    _, _, metrics = eval_loader(test_dl, model)   # ou val_loader/test_loader, selon ton besoin\n",
    "    results[os.path.basename(state_path)] = metrics\n",
    "\n",
    "# Petit rÃ©capitulatif lisible :\n",
    "for name, m in results.items():\n",
    "    print(f\"{name:25s} | MCC={m['mcc']:.4f} | Acc={m['accuracy']:.4f} | F1={m['f1_macro']:.4f} | F1={m['f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574743b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â€” SWA hors-ligne â€”\n",
    "swa_state = average_states(state_paths)\n",
    "torch.save(swa_state, os.path.join(artifact_dir, \"best_state_swa_val.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:               # câ€™est le cas le plus frÃ©quent\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# â€” Ã©valuation sur test â€”\n",
    "print(\"\\n=========== Phase B : Ã©valuation SWA sur test =========\")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(VOCAB)\n",
    "\n",
    "\n",
    "\n",
    "base.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_swa = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "model_swa.load_state_dict(torch.load(os.path.join(artifact_dir, \"best_state_swa_val.pt\"), map_location=\"cpu\"), strict=False)\n",
    "\n",
    "\n",
    "\n",
    "test_ds = TokenizedDS(te_enc, lab_te)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, test_metrics = eval_loader(test_dl, model_swa)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  C.   FULL TRAIN (100 %)  â†’  deux seeds + SWA  â†’  production\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase C : full-train 100 % pour la prod =========\")\n",
    "\n",
    "full_enc = enc(msg_tv + msg_te)\n",
    "full_ds  = TokenizedDS(full_enc, lab_tv + lab_te)\n",
    "\n",
    "cls_counts_all = np.bincount(lab_tv + lab_te, minlength=NUM_LABELS)\n",
    "cls_weights_all = torch.tensor(\n",
    "    cls_counts_all.sum() / (NUM_LABELS * (cls_counts_all + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths_full = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Full-train seed {seed} â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    # pas de validation â†’ epochs fixes\n",
    "    args_prod = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_full\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_strategy=\"epoch\",\n",
    "        seed=seed,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer_prod = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args_prod,\n",
    "        train_dataset=full_ds,\n",
    "        data_collator=collator,\n",
    "        class_weights=cls_weights_all,\n",
    "    )\n",
    "    trainer_prod.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_full.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths_full.append(state_path)\n",
    "\n",
    "    del trainer_prod, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "swa_state_full = average_states(state_paths_full)\n",
    "torch.save(swa_state_full, os.path.join(artifact_dir, \"best_state_swa_full.pt\"))\n",
    "print(\"\\nâœ“ ModÃ¨le SWA full-train enregistrÃ© : best_state_swa_full.pt\")\n",
    "\n",
    "# Optionnel : sauvegarde du tokenizer / meta-infos\n",
    "tok.save_pretrained(os.path.join(artifact_dir, \"tokenizer_final\"))\n",
    "json.dump(\n",
    "    {\n",
    "        \"best_params\": best_params,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"notes\": \"adapter SWA train+val, puis SWA full-train pour production\",\n",
    "    },\n",
    "    open(os.path.join(artifact_dir, \"run_summary.json\"), \"w\"),\n",
    "    indent=2,\n",
    ")\n",
    "print(\"RÃ©sumÃ© sauvegardÃ©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ce11e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e88d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fd8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca01cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d162b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51290589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers_swa.py --------------------------------------------------------------\n",
    "import os, copy, torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "def load_base(model_name, num_labels, tok, bnb_cfg, lora_cfg):\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(tok.vocab_size)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    return get_peft_model(base, copy.deepcopy(lora_cfg))\n",
    "\n",
    "def average_states(state_paths):\n",
    "    from collections import OrderedDict\n",
    "    avg = OrderedDict()\n",
    "    for p in state_paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(state_paths)).to(torch.float16)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_swa_eval.py -------------------------------------------------------------\n",
    "import os, torch, copy\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig\n",
    "# -----------------------------------------------------------------------------\n",
    "artifact_dir = \"./outputs\"\n",
    "state_paths = [\n",
    "    os.path.join(artifact_dir, \"best_state_seed0_val.pt\"),\n",
    "    os.path.join(artifact_dir, \"best_state_seed13_val.pt\"),\n",
    "]\n",
    "\n",
    "# â”€ config rÃ©utilisÃ©e (exactement celle de ton script initial) â”€\n",
    "l_cfg        = LoraConfig(           # mÃªmes valeurs quâ€™avant\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    bias=\"none\", task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg      = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1. SWA hors-ligne\n",
    "swa_state = average_states(state_paths)\n",
    "swa_path  = os.path.join(artifact_dir, \"best_state_swa_val.pt\")\n",
    "torch.save(swa_state, swa_path)\n",
    "print(f\"âœ“ SWA sauvegardÃ© : {swa_path}\")\n",
    "\n",
    "# 2. Ã‰valuation\n",
    "model = load_base(MODEL_NAME, NUM_LABELS, tok, bnb_cfg, l_cfg)\n",
    "model.load_state_dict(torch.load(swa_path, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "test_dl  = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, metrics = eval_loader(test_dl)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c454a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6904e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905183f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00234a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527ef2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12991eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512820c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696928db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db101ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f903637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20abfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98539d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bfcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33f14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89318778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc995f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c6441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50a170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4a034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511134c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Process Facebook & Instagram CSV exports (v2)\n",
    "\n",
    "DiffÃ©rences principales :\n",
    "1. Chemin du modÃ¨le â†’ dossier `outputs/final_model/` issu de trainer.save_model().\n",
    "2. Prompt identique Ã  lâ€™entraÃ®nement : add_generation_prompt=False.\n",
    "3. BitsAndBytes : dtype = bfloat16 si dispo, sinon float16.\n",
    "4. Tout le reste (reprise, sauvegarde incrÃ©mentale, dÃ©tection de langue)\n",
    "   reste inchangÃ©.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Early env tweaks\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"        # supprime lâ€™avertissement\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Standard lib\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Third-party\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USER CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV   = \"/home/lisst_ai/FACEBOOK_concat.csv\"\n",
    "INSTAGRAM_CSV  = \"/home/lisst_ai/INSTAGRAM_concat.csv\"\n",
    "\n",
    "# â€¼ï¸ nouveau rÃ©pertoire modÃ¨le sauvegardÃ© par trainer.save_model()\n",
    "MODEL_DIR      = \"/home/lisst_ai/qlora_project/outputs/final_model\"\n",
    "\n",
    "OUT_FACEBOOK   = \"/home/lisst_ai/gaza_facebook_cleaned.csv\"\n",
    "OUT_INSTAGRAM  = \"/home/lisst_ai/gaza_instagram_cleaned.csv\"\n",
    "\n",
    "BATCH_SIZE          = 64\n",
    "BATCH_SAVE_EVERY    = 100          # sauvegarde toutes les 100 batchs\n",
    "NUM_WORKERS: int | None = None     # None â‡’ tous les cÅ“urs CPU\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONSTANTS & LOGGING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEP               = \" - \"\n",
    "NUM_WORDS_SAMPLE  = 20\n",
    "TARGET_LANG       = \"en\"\n",
    "CONSTRUCTED_COL   = \"constructed_text\"\n",
    "PRED_COL          = \"predicted_category\"\n",
    "MAX_LENGTH        = 512\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(levelname)-8s | %(message)s\")\n",
    "log = logging.getLogger(\"process_sm\")\n",
    "\n",
    "TRAIN_CSV = \"/home/lisst_ai/gaza_classified.csv\"      # fallback si id2label absent\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for val in values:\n",
    "        if not isinstance(val, str):\n",
    "            continue\n",
    "        val_clean = val.strip()\n",
    "        if not val_clean:\n",
    "            continue\n",
    "        current = sep.join(parts).lower()\n",
    "        if val_clean.lower() in current:\n",
    "            continue\n",
    "        parts.append(val_clean)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def detect_lang(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "# â€” worker pour ProcessPoolExecutor (picklable) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def _detect_lang_worker(args):\n",
    "    idx, txt = args\n",
    "    words = txt.split()\n",
    "    sample = \" \".join(words[:NUM_WORDS_SAMPLE])\n",
    "    if not sample.strip():\n",
    "        return idx, False\n",
    "    return idx, detect_lang(sample) == TARGET_LANG\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "def filter_english(texts: list[str],\n",
    "                   *, workers: int | None = NUM_WORKERS) -> list[bool]:\n",
    "    workers = workers or os.cpu_count() or 4\n",
    "    log.info(\"Detecting language on %d texts with %d workersâ€¦\",\n",
    "             len(texts), workers)\n",
    "\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for idx, ok in tqdm(ex.map(_detect_lang_worker,\n",
    "                                   enumerate(texts),\n",
    "                                   chunksize=512),\n",
    "                            total=len(texts),\n",
    "                            desc=\"Lang-detect\",\n",
    "                            unit=\"post\"):\n",
    "            flags[idx] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Model loading & inference\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_classifier(model_dir: str, *, train_csv: str = TRAIN_CSV):\n",
    "    p_cfg     = PeftConfig.from_pretrained(model_dir)\n",
    "    base_name = p_cfg.base_model_name_or_path\n",
    "\n",
    "    # 1. id2label / num_labels â€“ on tente la config dâ€™abord\n",
    "    id2label = getattr(p_cfg, \"id2label\", None)\n",
    "    if id2label:\n",
    "        num_labels = len(id2label)\n",
    "    else:\n",
    "        try:\n",
    "            with open(Path(model_dir) / \"adapter_config.json\") as f:\n",
    "                raw = json.load(f)\n",
    "            num_labels = raw.get(\"num_labels\")\n",
    "            id2label   = {int(k): v for k, v in raw.get(\"id2label\", {}).items()}\n",
    "        except FileNotFoundError:\n",
    "            num_labels = None\n",
    "            id2label   = None\n",
    "\n",
    "    # 2. Fallback : reconstruction Ã  partir du CSV dâ€™entraÃ®nement\n",
    "    if not id2label:\n",
    "        df         = pd.read_csv(train_csv, header=None, names=[\"text\", \"cat\"])\n",
    "        cats       = sorted(df[\"cat\"].unique())\n",
    "        id2label   = {i: c for i, c in enumerate(cats)}\n",
    "        num_labels = len(id2label)\n",
    "        log.info(\"id2label reconstruit depuis %s\", train_csv)\n",
    "\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(\n",
    "        base_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_name,\n",
    "        config=cfg,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_name, padding_side=\"left\")\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "        tok.pad_token = '<pad>'\n",
    "        base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(base, model_dir)\n",
    "    except RuntimeError as e:\n",
    "        log.warning(\"TÃªte LoRA incompatible (%s) â†’ ignore_mismatched_sizes=True\", e)\n",
    "        model = PeftModel.from_pretrained(base, model_dir,\n",
    "                                          ignore_mismatched_sizes=True)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    cats_str = \", \".join(id2label.values())\n",
    "    def build_prompt(txt: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\"You are an expert assistant. \"\n",
    "                         \"Classify the following text into one of these \"\n",
    "                         f\"categories: {cats_str}. \"\n",
    "                         \"Respond with the category label only.\")},\n",
    "            {\"role\": \"user\", \"content\": txt},\n",
    "        ]\n",
    "        # â€¼ï¸ add_generation_prompt=False pour coller Ã  lâ€™entraÃ®nement\n",
    "        return tok.apply_chat_template(messages,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=False).strip()\n",
    "\n",
    "    return tok, model, build_prompt\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Batch prediction with incremental saving\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def incremental_predict(df: pd.DataFrame,\n",
    "                        tok, model, build_prompt,\n",
    "                        *,\n",
    "                        text_col: str = CONSTRUCTED_COL,\n",
    "                        batch_size: int = BATCH_SIZE,\n",
    "                        save_every: int = BATCH_SAVE_EVERY,\n",
    "                        out_path: str | Path | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Remplit df[PRED_COL] in-place pour les lignes oÃ¹ il manque,\n",
    "    en sauvegardant toutes les `save_every` batchs.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # indices Ã  traiter\n",
    "    to_process = df.index[df[PRED_COL].isna() | (df[PRED_COL] == \"\")].tolist()\n",
    "    if not to_process:\n",
    "        log.info(\"Aucune ligne Ã  catÃ©goriser (dÃ©jÃ  complet).\")\n",
    "        return\n",
    "\n",
    "    total_batches = math.ceil(len(to_process) / batch_size)\n",
    "    batch_counter = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(to_process), batch_size),\n",
    "                  desc=\"Batch-predict\",\n",
    "                  total=total_batches,\n",
    "                  unit=\"batch\",\n",
    "                  ncols=300):\n",
    "        batch_idx   = to_process[i: i + batch_size]\n",
    "        batch_texts = df.loc[batch_idx, text_col].tolist()\n",
    "        prompts     = [build_prompt(t) for t in batch_texts]\n",
    "\n",
    "        enc = tok(prompts,\n",
    "                  return_tensors=\"pt\",\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  max_length=MAX_LENGTH)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "        ids = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "        labels = [\n",
    "            model.config.id2label[i]\n",
    "            if isinstance(model.config.id2label, dict)\n",
    "            else model.config.id2label[i]\n",
    "            for i in ids\n",
    "        ]\n",
    "        df.loc[batch_idx, PRED_COL] = labels\n",
    "\n",
    "        batch_counter += 1\n",
    "        if out_path and batch_counter % save_every == 0:\n",
    "            log.info(\"Interim save â†’ %s\", out_path)\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "    if out_path:\n",
    "        log.info(\"Final save â†’ %s\", out_path)\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Per-platform helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Robust CSV loader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError as err:\n",
    "        log.warning(\"Standard parser failed for %s (%s). \"\n",
    "                    \"Retrying with engine='python'â€¦\", path, err)\n",
    "        return pd.read_csv(path,\n",
    "                           engine=\"python\",\n",
    "                           on_bad_lines=\"skip\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Platform pipeline with resume capability\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pipeline(name: str,\n",
    "                 src_csv: str | Path,\n",
    "                 out_csv: str | Path,\n",
    "                 builder_fn) -> None:\n",
    "    out_csv = Path(out_csv)\n",
    "    src_csv = Path(src_csv)\n",
    "\n",
    "    if out_csv.exists():\n",
    "        log.info(\"%s: found existing output (%s) â€“ resume mode.\", name, out_csv)\n",
    "        df = safe_read_csv(out_csv)\n",
    "        # reconstruit le texte si absent (rare)\n",
    "        if CONSTRUCTED_COL not in df.columns:\n",
    "            raw = safe_read_csv(src_csv)\n",
    "            df_texts = builder_fn(raw)[[CONSTRUCTED_COL]]\n",
    "            df = df.join(df_texts)\n",
    "    else:\n",
    "        log.info(\"%s: loading raw CSVâ€¦\", name)\n",
    "        raw = safe_read_csv(src_csv)\n",
    "        df = builder_fn(raw)\n",
    "        df[PRED_COL] = pd.NA                         # nouvelle colonne\n",
    "\n",
    "    # filtrage langue pour les lignes non catÃ©gorisÃ©es\n",
    "    mask_uncat = df[PRED_COL].isna() | (df[PRED_COL] == \"\")\n",
    "    to_check   = df.loc[mask_uncat, CONSTRUCTED_COL].tolist()\n",
    "    if to_check:\n",
    "        log.info(\"%s: language filtering (%d lignes)â€¦\", name, len(to_check))\n",
    "        flags = filter_english(to_check)\n",
    "        df = df.loc[~mask_uncat | pd.Series(flags,\n",
    "                                            index=df.loc[mask_uncat].index)\n",
    "                    ].reset_index(drop=True)\n",
    "\n",
    "    # classification\n",
    "    tok, model, build_prompt = load_classifier(MODEL_DIR)\n",
    "    incremental_predict(df, tok, model, build_prompt,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        save_every=BATCH_SAVE_EVERY,\n",
    "                        out_path=out_csv)\n",
    "\n",
    "    log.info(\"%s: completed (%d rows total).\", name, len(df))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main routine\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    run_pipeline(\"Facebook\",\n",
    "                 FACEBOOK_CSV,\n",
    "                 OUT_FACEBOOK,\n",
    "                 process_facebook)\n",
    "\n",
    "    run_pipeline(\"Instagram\",\n",
    "                 INSTAGRAM_CSV,\n",
    "                 OUT_INSTAGRAM,\n",
    "                 process_instagram)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360076e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be0a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.model_selection.cross_val_score(..., return_estimator=True) â†’ bootstrap des scores pour IC 95 % ; on publie toujours : Â« MCC 0.71 Â± 0.03 Â». \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af11ce2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1149fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
