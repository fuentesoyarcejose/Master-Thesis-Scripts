{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac37768",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 60984,
     "status": "ok",
     "timestamp": 1769035523470,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "2ac37768",
    "outputId": "9752ffbb-4595-448c-f4cc-1c8fc6b8c293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (7.7.1)\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting comm>=0.1.3 (from ipywidgets)\n",
      "  Downloading comm-0.2.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (75.2.0)\n",
      "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.3-py3-none-any.whl (7.3 kB)\n",
      "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m112.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jedi, comm, ipywidgets\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.6.10\n",
      "    Uninstalling widgetsnbextension-3.6.10:\n",
      "      Successfully uninstalled widgetsnbextension-3.6.10\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.7.1\n",
      "    Uninstalling ipywidgets-7.7.1:\n",
      "      Successfully uninstalled ipywidgets-7.7.1\n",
      "Successfully installed comm-0.2.3 ipywidgets-8.1.8 jedi-0.19.2 widgetsnbextension-4.0.15\n",
      "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
      "Collecting google-genai\n",
      "  Downloading google_genai-1.60.0-py3-none-any.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.12.1)\n",
      "Collecting google-auth<3.0.0,>=2.47.0 (from google-auth[requests]<3.0.0,>=2.47.0->google-genai)\n",
      "  Downloading google_auth-2.47.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai) (0.6.2)\n",
      "Downloading google_genai-1.60.0-py3-none-any.whl (719 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m719.4/719.4 kB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.47.0-py3-none-any.whl (234 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: google-auth, google-genai\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.43.0\n",
      "    Uninstalling google-auth-2.43.0:\n",
      "      Successfully uninstalled google-auth-2.43.0\n",
      "  Attempting uninstall: google-genai\n",
      "    Found existing installation: google-genai 1.55.0\n",
      "    Uninstalling google-genai-1.55.0:\n",
      "      Successfully uninstalled google-genai-1.55.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-2.47.0 google-genai-1.60.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "7e965697cfdf4dbc967ba853e0d01c9b",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.1)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.7.0\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.9.0+cu126)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1\n",
      "Collecting flash_attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m141.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash_attn) (2.9.0+cu126)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash_attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash_attn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash_attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash_attn) (3.0.3)\n",
      "Building wheels for collected packages: flash_attn\n",
      "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for flash_attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=253780426 sha256=4e2f9e39313266b1544b68138b15b91ee6221eccf14f7902b7c6620351340810\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash_attn\n",
      "Installing collected packages: flash_attn\n",
      "Successfully installed flash_attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ipywidgets\n",
    "!pip install -U google-genai\n",
    "!pip install -q transformers accelerate peft langdetect tqdm pandas huggingface_hub\n",
    "!pip install -U optuna\n",
    "!pip install -U bitsandbytes\n",
    "!pip install -U flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e36a2",
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1769035523724,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "757e36a2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(\"\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=UserWarning,\n",
    "    module=\"torch.utils.checkpoint\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "VkUn0nxf_Thd",
   "metadata": {
    "executionInfo": {
     "elapsed": 4803,
     "status": "ok",
     "timestamp": 1769035528530,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "VkUn0nxf_Thd"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import re, json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Sequence, List\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "try:\n",
    "    from langdetect import detect, LangDetectException\n",
    "    HAS_LANGDETECT = True\n",
    "except ImportError:\n",
    "    HAS_LANGDETECT = False\n",
    "    logging.warning(\"langdetect not installed. Language filtering disabled.\")\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c49600",
   "metadata": {
    "id": "74c49600"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Gaza-stance classification (Gemini) â€“ v6, prompt â€œStanceClassifier-Gaza-v2â€\n",
    "# --------------------------------------------------------------------------\n",
    "# â€¢ Reconstruit le texte complet depuis les CSV Facebook & Instagram.\n",
    "# â€¢ Filtre les posts non anglophones puis Ã©chantillonne 0,5 %.\n",
    "# â€¢ Envoie chaque post Ã  Gemini-2.5-flash-preview avec le prompt ci-dessous.\n",
    "# â€¢ La rÃ©ponse doit Ãªtre **exactement** un label sur une ligne.\n",
    "# â€¢ Sauvegarde CSV : text, gpt_category\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\"\"\"import logging, os, sys\n",
    "logging.basicConfig(level=logging.WARNING,           # root silencieux\n",
    "                    format=\"[%(levelname)s] %(message)s\",\n",
    "                    force=True)                      # â† reset complet\n",
    "\n",
    "log = logging.getLogger(\"pipeline\")                  # ton logger\n",
    "log.setLevel(logging.INFO)                           # un seul handler (root)\n",
    "# pas dâ€™ajout de handler, on laisse propager au root\n",
    "\n",
    "for name in (\"google\", \"google_genai\", \"google.api_core\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)  # stop INFO/DEBUG\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV  = \"/home/lisst_ai/DATA/FACEBOOK_concat.csv\"\n",
    "#INSTAGRAM_CSV = \"/home/lisst_ai/DATA/INSTAGRAM_concat.csv\"\n",
    "\n",
    "#GOOGLE_API_KEY  = \"\"\n",
    "#MODEL_NAME      = \"gemini-2.5-flash-preview-04-17\"\n",
    "OUTPUT_FILENAME = \"/home/lisst_ai/DATA/gaza_stance_sampled_classified.csv\"\n",
    "\n",
    "SAMPLE_FRAC      = 0.005   # 0,5 %\n",
    "RANDOM_STATE     = 42\n",
    "TARGET_LANG      = \"en\"\n",
    "NUM_WORDS_SAMPLE = 100\n",
    "NUM_WORKERS      = None\n",
    "\n",
    "SEP             = \" - \"\n",
    "CONSTRUCTED_COL = \"_text\"\n",
    "ALLOWED_CATEGORIES = {\n",
    "    \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\",\n",
    "    \"Anti-War_Pro-Peace\",\n",
    "    \"Other\",\n",
    "    \"Off-topic\",\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. PROMPT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SYSTEM_INSTRUCTION = \"\"\"\n",
    "You are â€œStanceClassifier-Gaza-v2â€.\n",
    "Task: read one English-language social-media post about the 2023-2025 Gaza War and\n",
    "output **exactly one** label from the list below.\n",
    "\n",
    "Think step-by-step **silently** (donâ€™t reveal your reasoning).\n",
    "Return only the label on a single line, nothing else.\n",
    "\n",
    "### Allowed labels\n",
    "1. Pro-Palestinian\n",
    "2. Pro-Israeli\n",
    "3. Anti-War_Pro-Peace\n",
    "4. Other\n",
    "5. Off-topic          â† use when the post is not about the Gaza War at all\n",
    "\n",
    "### Definitions & guidance\n",
    "\n",
    "**Pro-Palestinian**\n",
    "Any stance that primarily supports Palestinians or blames Israel.\n",
    "âœ” Detect even subtle cues: ğŸ‡µğŸ‡¸ emoji, â€œfrom the river to the seaâ€, focus on Palestinian victims, words like â€œgenocideâ€, â€œoccupationâ€, â€œapartheidâ€, or praise for resistance.\n",
    "âœ˜ A single humanitarian mention of both sides â†’ see Anti-War_Pro-Peace unless Israel is clearly blamed.\n",
    "\n",
    "**Pro-Israeli**\n",
    "Primarily supports Israel or blames Hamas/Palestinian side.\n",
    "âœ” Cues: ğŸ‡®ğŸ‡±, â€œright to self-defenceâ€, â€œterrorist organisationâ€, hostage hashtags, emphasis on Hamas using human shields.\n",
    "âœ˜ Balanced calls for restraint â†’ Anti-War_Pro-Peace unless Hamas is clearly condemned more than Israel.\n",
    "\n",
    "**Anti-War_Pro-Peace**\n",
    "Core message = stop the violence / protect civilians on *both* sides, with balanced language.\n",
    "\n",
    "**Other** (use sparingly)\n",
    "Mentions the war but no discernible leaning **after** checking for:\n",
    "â€£ sarcasm or irony\n",
    "â€£ vocabulary that allocates guilt (even implicitly)\n",
    "â€£ spotlight on one populationâ€™s suffering\n",
    "â€£ selective historical references\n",
    "\n",
    "**Off-topic**\n",
    "Post lacks substantive reference to the Gaza War (generic memes, ads, ambiguous â€œPray for themâ€ with no context, etc.).\n",
    "\n",
    "### Checklist before choosing â€œOtherâ€\n",
    "1. Does the wording, emoji, or hashtag tilt sympathy toward one side?\n",
    "2. Are perpetrators or victims named asymmetrically?\n",
    "3. Is there an implied moral judgment?\n",
    "If **any** answer is yes, assign the corresponding stance; else, Other.\n",
    "\n",
    "### Tie-break rules\n",
    "* Stance + peace call â†’ keep the stance.\n",
    "* Balanced criticism but one side more severe â†’ assign that side.\n",
    "* Unsure between Anti-War and partisan â†’ choose the partisan stance.\n",
    "\n",
    "### Output format\n",
    "Return exactly one of:\n",
    "Pro-Palestinian | Pro-Israeli | Anti-War_Pro-Peace | Other | Off-topic\n",
    "\n",
    "\"\"\".strip()\n",
    "\"\"\"\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. GEMINI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log.info(\"Initialisation API Geminiâ€¦\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "GEN_CFG = types.GenerateContentConfig(\n",
    "    system_instruction=SYSTEM_INSTRUCTION,\n",
    "    temperature=0.1,\n",
    "    safety_settings=[\n",
    "        types.SafetySetting(category=c, threshold=types.HarmBlockThreshold.BLOCK_NONE)\n",
    "        for c in (\n",
    "            types.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "            types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "            types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "            types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        )\n",
    "    ],\n",
    "    thinking_config=genai.types.ThinkingConfig(\n",
    "      thinking_budget=0\n",
    "    )\n",
    ")\n",
    "log.info(\"ModÃ¨le prÃªt : %s\", MODEL_NAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. UTILITAIRES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ZERO_WIDTH_RE = re.compile(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF]\")\n",
    "\n",
    "def strip_invisible(t: str) -> str:\n",
    "    return ZERO_WIDTH_RE.sub(\"\", t)\n",
    "\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for v in values:\n",
    "        if isinstance(v, str):\n",
    "            v = v.strip()\n",
    "            if v and v.lower() not in sep.join(parts).lower():\n",
    "                parts.append(v)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    log.info(\"Lecture CSV : %s\", path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    log.info(\" â†’ %s lignes\", f\"{len(df):,}\")\n",
    "    return df\n",
    "\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(lambda r: concatenate_fields(\n",
    "        [r.get(c) for c in cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ dÃ©tection langue (multiprocessing) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _detect(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _lang_worker(args):\n",
    "    i, txt = args\n",
    "    sample = \" \".join(txt.split()[:NUM_WORDS_SAMPLE])\n",
    "    return i, (_detect(sample) == TARGET_LANG)\n",
    "\n",
    "def filter_english(texts: list[str]) -> list[bool]:\n",
    "    workers = NUM_WORKERS or (os.cpu_count() or 4)\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for i, ok in ex.map(_lang_worker, enumerate(texts), chunksize=512):\n",
    "            flags[i] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. CLASSIFY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def classify(post: str) -> str:\n",
    "    try:\n",
    "        resp = client.models.generate_content(\n",
    "            model=MODEL_NAME,\n",
    "            contents=post,\n",
    "            config=GEN_CFG,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        log.error(\"Gemini error : %s â€“ exit.\", e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not resp.text:\n",
    "        return \"INVALID\"\n",
    "\n",
    "    label = resp.text.splitlines()[0].strip()\n",
    "    return label if label in ALLOWED_CATEGORIES else \"INVALID\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    df_fb = process_facebook(safe_read_csv(FACEBOOK_CSV))\n",
    "    #df_ig = process_instagram(safe_read_csv(INSTAGRAM_CSV))\n",
    "\n",
    "    log.info(\"Filtre de langue : %d %d\", len(df_fb), len(df_ig))  # avant le filtrage\n",
    "    for df in (df_fb, df_ig):\n",
    "        mask = filter_english(df[CONSTRUCTED_COL].tolist())\n",
    "        df.drop(index=df.index[~pd.Series(mask)], inplace=True)\n",
    "\n",
    "    log.info(\"AprÃ¨s filtre de langue : %d %d\", len(df_fb), len(df_ig))\n",
    "\n",
    "    df_sampled = pd.concat([\n",
    "        df_fb.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE),\n",
    "        df_ig.sample(frac=SAMPLE_FRAC, random_state=RANDOM_STATE)\n",
    "    ], ignore_index=True)\n",
    "    log.info(\"Ã‰chantillon : %s posts\", f\"{len(df_sampled):,}\")\n",
    "\n",
    "    texts = df_sampled[CONSTRUCTED_COL].apply(strip_invisible).tolist()\n",
    "    if not texts:\n",
    "        raise RuntimeError(\"Aucun texte anglais aprÃ¨s filtrage.\")\n",
    "\n",
    "    #log.info(\"Classification Geminiâ€¦\")\n",
    "    #cats = [classify(t) for t in tqdm(texts, desc=\"Gemini\", unit=\"post\")]\n",
    "\n",
    "   #out = pd.DataFrame({\"text\": texts, \"gpt_category\": cats})\n",
    "    #n_inv = (out[\"gpt_category\"] == \"INVALID\").sum()\n",
    "    #if n_inv:\n",
    "    #    log.warning(\"%d rÃ©ponses INVALID\", n_inv)\n",
    "\n",
    "    #out.query(\"gpt_category != 'INVALID'\").to_csv(OUTPUT_FILENAME, index=False)\n",
    "    log.info(\"Fichier enregistrÃ© : %s\", OUTPUT_FILENAME)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ EntrÃ©e script â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    log.info(\"Lancement pipelineâ€¦\")\n",
    "    main()\n",
    "    log.info(\"TerminÃ©.\")\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rxJm-koN9xr9",
   "metadata": {
    "id": "rxJm-koN9xr9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8043ccf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49481,
     "status": "ok",
     "timestamp": 1769035591543,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "b8043ccf",
    "outputId": "d0f7c1d9-b34b-463b-c2a3-cb7bc144dcc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "6,365 exemples chargÃ©s (classes fusionnÃ©es).\n",
      "RÃ©partition des classes : {2: 4021, 0: 1265, 1: 1079}\n",
      "label2id : {'Neutral': 0, 'Pro-Israeli': 1, 'Pro-Palestinian': 2}\n"
     ]
    }
   ],
   "source": [
    "EFF_BATCH_TARGET = 32\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Llama-3-70B-Instruct â€¢ LoRA â€¢ 4-bit NF4 â€¢ Optuna â€¢ Cosine LR â€¢ MCC\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#   â€¢ EntrÃ©e tronquÃ©e Ã  512 tokens\n",
    "#   â€¢ Quantisation bitsandbytes 4-bit NF4\n",
    "#   â€¢ Fusion des classes : Other + Off-topic + Anti-War_Pro-Peace â†’ Neutral\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "import os, gc, random, threading, _thread, json, math, copy\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef\n",
    "\n",
    "import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "SEED = 42\n",
    "set_seed(SEED); random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "USE_GRADIENT_CHECKPOINTING = True\n",
    "MODEL_NAME = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "NUM_EPOCHS, EARLY_STOP_PATIENCE, TIMEOUT_TRAIN = 40, 6, 3 * 36000\n",
    "KFOLD, N_TRIALS = 5, 3\n",
    "artifact_dir = Path(\"./outputs\"); artifact_dir.mkdir(exist_ok=True, parents=True)\n",
    "SINGLE_FOLD_ONLY = False\n",
    "MAX_LENGTH = 512\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lecture du CSV + fusion -------------------------\n",
    "CSV_PATH = \"/content/drive/MyDrive/UTJ2/Memoire/Scripts/Stance/gaza_stance_sampled_classified.csv\"\n",
    "df = pd.read_csv(CSV_PATH)          # colonnes: text, gpt_category / cat\n",
    "\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "# â”€â”€â”€ Fusion des classes mineures â†’ Neutral\n",
    "FUSE_MAP = {\n",
    "    \"Pro-Palestinian\":        \"Pro-Palestinian\",\n",
    "    \"Pro-Israeli\":            \"Pro-Israeli\",\n",
    "    \"Other\":                  \"Neutral\",\n",
    "    \"Off-topic\":              \"Neutral\",\n",
    "    \"Anti-War_Pro-Peace\":     \"Neutral\",\n",
    "}\n",
    "df[\"cat\"] = df[\"cat\"].map(FUSE_MAP)             # applique la fusion\n",
    "df.dropna(subset=[\"cat\"], inplace=True)         # sÃ©curitÃ© : lignes non mappÃ©es\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Mappings label â†” id ------------------------------\n",
    "cats      = sorted(df[\"cat\"].unique())          # ['Neutral', 'Pro-Israeli', 'Pro-Palestinian']\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Diagnostics --------------------------------------\n",
    "print(f\"{len(df):,} exemples chargÃ©s (classes fusionnÃ©es).\")\n",
    "print(\"RÃ©partition des classes :\", df[\"label_id\"].value_counts().to_dict())\n",
    "print(\"label2id :\", label2id)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Listes utiles / constantes -----------------------\n",
    "messages   = df[\"text\"].tolist()\n",
    "labels     = df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)                           # 3\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Split train / val / test -------------------------\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02f90a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d859487e62ae4f1da1284846bc7abcb6",
      "0dbf1d6e218c4930b5beaa843c985fe0",
      "4adc0eadab4b441c9346d96e50a21f6a",
      "b637c5cd40dc450cbb7573e2a8935857",
      "4aacc61fdb374caebed92f9096b21944",
      "25f3c9542b2b4467b8995ba923a5fefd",
      "e12099fda04b4db4ae4405ea17776f74",
      "956baf5852b44eb097ea07b913f0b9cb",
      "364f174037914d769f413641bdc67f08",
      "9715b7d58c334f8b9cb16e17483fdc43",
      "0b7ec87b9c15483f85a55e7610ac2642",
      "1497d4b7748348b6ba6cd32593194e1e",
      "8b0838891b6f46c68400e1ed3c912a81",
      "093949c9ff6c47109c73936559022568",
      "b5341442f83e4b30ac45d26a068caa96",
      "33659f055420477d96ed9536fc933440",
      "1c34568ec32d4ba4a38e495705928d90",
      "70891aac81c64db488a3b38c99d26f74",
      "2debc053862a4c6ea56fd8bd79a73459",
      "b29b016319b14adab629eb2647d3a355",
      "e80a79c956d7489db2f4be66166ce017",
      "b1faa0869bf54d5399433efe4e083f8a",
      "94c5951d08d0450485c607bc737a4770",
      "a08c06618f61423783448d2d965d65db",
      "37f74260cc4b4311b1edf58a3bd6c8d5",
      "1b3d07ab5e6849ccaf1c54ed915db588",
      "d6d08beaf22b43128ecb74b24ae2a353",
      "9134ea58f63345d696bddbfeacf72b37",
      "873e6d9585f0426b88eb064c4c43c73e",
      "89c6b12b18704bf8a1f9a49c32a1cd80",
      "6566f97e6225481f8bb153dd3151dcf6",
      "280b5d5f5b5d4358ba6a4fe7caa8d565",
      "0d3cba73f4fc4fa68037c32ca2035efb"
     ]
    },
    "executionInfo": {
     "elapsed": 5730,
     "status": "ok",
     "timestamp": 1769035603294,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "c02f90a2",
    "outputId": "0052c59b-9099-40d1-b0ab-38dfe90f0137"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d859487e62ae4f1da1284846bc7abcb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1497d4b7748348b6ba6cd32593194e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c5951d08d0450485c607bc737a4770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    }
   ],
   "source": [
    "def enc(txts: List[str]):\n",
    "    prompts = [build_prompt(t) for t in txts]\n",
    "    return tok(                     # â†Â important\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=False    # plus de doubleÂ BOS\n",
    "    )\n",
    "\n",
    "def build_prompt(txt: str) -> str:\n",
    "    \"\"\"Prompt Mistral officiel, sÃ»r pour l'encodeur HF.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": f\"You are an expert assistant. Classify the following text into one of these \"\n",
    "                    f\"categories: {cats_string}. Respond with the category label only.\"},\n",
    "        {\"role\": \"user\", \"content\": txt}\n",
    "    ]\n",
    "    return tok.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,                # on renvoie une string\n",
    "        add_generation_prompt=False     # ajoute automatiquement le tag assistant\n",
    "    ).strip()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt instructif -----------------------------------------\n",
    "cats_string = \", \".join(cats)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Tokenizer --------------------------------------------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")  # â† leftâ€‘pad plus sÃ»r\n",
    "if tok.pad_token_id is None or tok.pad_token_id == tok.eos_token_id:\n",
    "    # 1) on crÃ©e un vrai token pad\n",
    "    tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "    # 2) on pointe officiellement dessus\n",
    "    tok.pad_token = '<pad>'\n",
    "\n",
    "# IMPORTANT : redimensionner lâ€™embedding AVANT dâ€™injecter LoRA\n",
    "VOCAB = len(tok)\n",
    "\n",
    "class TokenizedDS(Dataset):\n",
    "    def __init__(self, e, y):\n",
    "        self.e, self.y = e, y\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, i):\n",
    "        d = {k: torch.tensor(v[i]) for k, v in self.e.items()}\n",
    "        d[\"labels\"] = torch.tensor(self.y[i], dtype=torch.long)\n",
    "        return d\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WeightedCETrainer ----------------------------------------\n",
    "class WeightedCETrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights: torch.Tensor, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights.float()\n",
    "\n",
    "    # â–¿ ajouter le nouvel argument (ou **kwargs) â–¿\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model,\n",
    "        inputs,\n",
    "        return_outputs: bool = False,\n",
    "        num_items_in_batch: int | None = None,   # â† NEW\n",
    "        **kwargs,                                # â† compatibilitÃ© future\n",
    "    ):\n",
    "        labels  = inputs[\"labels\"]\n",
    "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
    "        logits  = outputs.logits\n",
    "\n",
    "        weight = self.class_weights.to(logits.device)\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            logits,\n",
    "            labels,\n",
    "            weight=weight,    # pondÃ©ration des classes\n",
    "            reduction=\"mean\"  # moyenne directe sur le batch\n",
    "        )\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "collator = DataCollatorWithPadding(tok)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Metrics ----------------------------------------------------\n",
    "\n",
    "def cat_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(y_true, y_pred, average=\"micro\"),\n",
    "        \"mcc\": matthews_corrcoef(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def compute_metrics(ev: EvalPrediction):\n",
    "    y_hat = np.argmax(ev.predictions, axis=1)\n",
    "    g = cat_metrics(ev.label_ids, y_hat)\n",
    "    return {\n",
    "        \"eval_accuracy\": g[\"accuracy\"],\n",
    "        \"eval_f1_macro\": g[\"f1_macro\"],\n",
    "        \"eval_f1_micro\": g[\"f1_micro\"],\n",
    "        \"eval_mcc\": g[\"mcc\"],\n",
    "    }\n",
    "\n",
    "\n",
    "te_enc = enc(msg_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eNXHCkl1AhnO",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1769035607515,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "eNXHCkl1AhnO"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RNKzJGhgAhnO",
   "metadata": {
    "id": "RNKzJGhgAhnO"
   },
   "source": [
    "Support for third party widgets will remain active for the duration of the session. To disable support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2-2UaaEAhnP",
   "metadata": {
    "id": "A2-2UaaEAhnP"
   },
   "outputs": [],
   "source": [
    "from google.colab import output\n",
    "output.disable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6cf21ec",
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1769035610082,
     "user": {
      "displayName": "JosÃ© Ignacio Fuentes",
      "userId": "11021149698656128277"
     },
     "user_tz": -60
    },
    "id": "c6cf21ec"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Timeout helper --------------------------------------------\n",
    "\n",
    "def timeout_train(trainer, seconds):\n",
    "    t = threading.Thread(target=trainer.train)\n",
    "    t.start(); t.join(seconds)\n",
    "    if t.is_alive():\n",
    "        _thread.interrupt_main(); t.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d465b4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5ad0ee1341244193a6743bd2a2207833",
      "83db65deecc9408d99574c0c5c74a552",
      "279fba77a40a4a73b5f7ddc30dfcc9e0",
      "4ef56778e4f54c179af64e0ec72750db",
      "8fe1094fb7d546b5977f1f4b75972e16",
      "236a9c2d5ebd485ebce91732a03c2c5f",
      "2e10d96515ad451b87d50265639c2552",
      "a6cbb34af4d04414a0bc2dc11a191298",
      "de97cc7484b74b92905f2d67c184cd0a",
      "6af59cb80b32457ba43ff1135128c83d",
      "c7d43a6ed71f469e88b448a2e681920e",
      "5b15e0aef23345918af904b64a911ed6",
      "cd6a0872a8c8439ca657648a7858834d",
      "f3dd43f426a541ad936365e395cc9dc5",
      "2c102e3e0c4f4d6f99abbf728b8e9e28",
      "179aead8a0d84108bd3d6b68c68c89d8",
      "c62fff4e2c1f4cbd8b376d740e066325",
      "c5f17f05acdb496c869d63a4103bc3ec",
      "7241fcdcbd5d4679866243d592b29094",
      "82bfbabc74e04380a49b6083c8388813",
      "5dd368b1abe74b54a8ef5c23ae2cfeb5",
      "b0f0ccfbe9214be3b4613e6d25d735ce"
     ]
    },
    "id": "8d465b4c",
    "outputId": "041e5c02-95c2-485c-c3a6-1f4889ddee3f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-21 23:01:21,312] A new study created in memory with name: no-name-2abf9fd9-6779-49a3-b9ca-b158158b4d00\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad0ee1341244193a6743bd2a2207833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MinistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Ministral-8B-Instruct-2410 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2592' max='5760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2592/5760 1:53:45 < 2:19:08, 0.38 it/s, Epoch 18/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.693500</td>\n",
       "      <td>1.868276</td>\n",
       "      <td>0.460733</td>\n",
       "      <td>0.352298</td>\n",
       "      <td>0.460733</td>\n",
       "      <td>0.304958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.557400</td>\n",
       "      <td>1.260357</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.542386</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.493513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.174200</td>\n",
       "      <td>1.276580</td>\n",
       "      <td>0.572426</td>\n",
       "      <td>0.403626</td>\n",
       "      <td>0.572426</td>\n",
       "      <td>0.424154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.994000</td>\n",
       "      <td>1.268189</td>\n",
       "      <td>0.677138</td>\n",
       "      <td>0.559448</td>\n",
       "      <td>0.677138</td>\n",
       "      <td>0.540091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.993200</td>\n",
       "      <td>1.058420</td>\n",
       "      <td>0.678883</td>\n",
       "      <td>0.587438</td>\n",
       "      <td>0.678883</td>\n",
       "      <td>0.551001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.813500</td>\n",
       "      <td>1.046070</td>\n",
       "      <td>0.730366</td>\n",
       "      <td>0.603411</td>\n",
       "      <td>0.730366</td>\n",
       "      <td>0.587101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.805100</td>\n",
       "      <td>1.550747</td>\n",
       "      <td>0.735602</td>\n",
       "      <td>0.525211</td>\n",
       "      <td>0.735602</td>\n",
       "      <td>0.567049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.770300</td>\n",
       "      <td>1.436279</td>\n",
       "      <td>0.794066</td>\n",
       "      <td>0.591447</td>\n",
       "      <td>0.794066</td>\n",
       "      <td>0.634763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.626700</td>\n",
       "      <td>1.030996</td>\n",
       "      <td>0.746073</td>\n",
       "      <td>0.613571</td>\n",
       "      <td>0.746073</td>\n",
       "      <td>0.603847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>1.178468</td>\n",
       "      <td>0.658813</td>\n",
       "      <td>0.527824</td>\n",
       "      <td>0.658813</td>\n",
       "      <td>0.519469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>1.216271</td>\n",
       "      <td>0.611693</td>\n",
       "      <td>0.485627</td>\n",
       "      <td>0.611693</td>\n",
       "      <td>0.473796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>1.207158</td>\n",
       "      <td>0.812391</td>\n",
       "      <td>0.640553</td>\n",
       "      <td>0.812391</td>\n",
       "      <td>0.656333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.595500</td>\n",
       "      <td>1.103790</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>0.564085</td>\n",
       "      <td>0.715532</td>\n",
       "      <td>0.568256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>1.210368</td>\n",
       "      <td>0.728621</td>\n",
       "      <td>0.548973</td>\n",
       "      <td>0.728621</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>1.353218</td>\n",
       "      <td>0.801920</td>\n",
       "      <td>0.600254</td>\n",
       "      <td>0.801920</td>\n",
       "      <td>0.636490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.533200</td>\n",
       "      <td>1.133936</td>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.607882</td>\n",
       "      <td>0.767016</td>\n",
       "      <td>0.617927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>1.390875</td>\n",
       "      <td>0.789703</td>\n",
       "      <td>0.615090</td>\n",
       "      <td>0.789703</td>\n",
       "      <td>0.612299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>1.497700</td>\n",
       "      <td>0.811518</td>\n",
       "      <td>0.628827</td>\n",
       "      <td>0.811518</td>\n",
       "      <td>0.647509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 01:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f0ccfbe9214be3b4613e6d25d735ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "colab": {
        "custom_widget_manager": {
         "url": "https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/2b70e893a8ba7c0f/manager.min.js"
        }
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MinistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Ministral-8B-Instruct-2410 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3601' max='5760' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3601/5760 2:36:54 < 1:34:07, 0.38 it/s, Epoch 25/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.271700</td>\n",
       "      <td>2.014221</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.260960</td>\n",
       "      <td>0.439791</td>\n",
       "      <td>0.231848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.352300</td>\n",
       "      <td>1.431186</td>\n",
       "      <td>0.639616</td>\n",
       "      <td>0.435999</td>\n",
       "      <td>0.639616</td>\n",
       "      <td>0.438214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>1.296345</td>\n",
       "      <td>0.605585</td>\n",
       "      <td>0.468024</td>\n",
       "      <td>0.605585</td>\n",
       "      <td>0.458435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.935900</td>\n",
       "      <td>1.170893</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.525782</td>\n",
       "      <td>0.675393</td>\n",
       "      <td>0.522474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>1.446252</td>\n",
       "      <td>0.772251</td>\n",
       "      <td>0.561069</td>\n",
       "      <td>0.772251</td>\n",
       "      <td>0.576455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.790300</td>\n",
       "      <td>1.217962</td>\n",
       "      <td>0.752182</td>\n",
       "      <td>0.591888</td>\n",
       "      <td>0.752182</td>\n",
       "      <td>0.598605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.821900</td>\n",
       "      <td>1.161209</td>\n",
       "      <td>0.680628</td>\n",
       "      <td>0.537156</td>\n",
       "      <td>0.680628</td>\n",
       "      <td>0.534403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.750400</td>\n",
       "      <td>1.314155</td>\n",
       "      <td>0.648342</td>\n",
       "      <td>0.512879</td>\n",
       "      <td>0.648342</td>\n",
       "      <td>0.488066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.776700</td>\n",
       "      <td>1.569360</td>\n",
       "      <td>0.529668</td>\n",
       "      <td>0.503802</td>\n",
       "      <td>0.529668</td>\n",
       "      <td>0.428062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>1.265613</td>\n",
       "      <td>0.760908</td>\n",
       "      <td>0.595778</td>\n",
       "      <td>0.760908</td>\n",
       "      <td>0.596196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.493245</td>\n",
       "      <td>0.703316</td>\n",
       "      <td>0.593344</td>\n",
       "      <td>0.703316</td>\n",
       "      <td>0.568809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.589200</td>\n",
       "      <td>1.318828</td>\n",
       "      <td>0.789703</td>\n",
       "      <td>0.627495</td>\n",
       "      <td>0.789703</td>\n",
       "      <td>0.615728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.531400</td>\n",
       "      <td>1.291620</td>\n",
       "      <td>0.757417</td>\n",
       "      <td>0.572617</td>\n",
       "      <td>0.757417</td>\n",
       "      <td>0.593434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>1.438252</td>\n",
       "      <td>0.720768</td>\n",
       "      <td>0.517619</td>\n",
       "      <td>0.720768</td>\n",
       "      <td>0.553976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>1.343374</td>\n",
       "      <td>0.766143</td>\n",
       "      <td>0.610527</td>\n",
       "      <td>0.766143</td>\n",
       "      <td>0.611731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>1.913720</td>\n",
       "      <td>0.790576</td>\n",
       "      <td>0.542223</td>\n",
       "      <td>0.790576</td>\n",
       "      <td>0.627681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>1.300885</td>\n",
       "      <td>0.779232</td>\n",
       "      <td>0.627788</td>\n",
       "      <td>0.779232</td>\n",
       "      <td>0.612335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>1.433872</td>\n",
       "      <td>0.797557</td>\n",
       "      <td>0.646852</td>\n",
       "      <td>0.797557</td>\n",
       "      <td>0.645305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>1.361240</td>\n",
       "      <td>0.704188</td>\n",
       "      <td>0.572822</td>\n",
       "      <td>0.704188</td>\n",
       "      <td>0.550148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>1.340888</td>\n",
       "      <td>0.757417</td>\n",
       "      <td>0.647620</td>\n",
       "      <td>0.757417</td>\n",
       "      <td>0.620113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.381103</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.657075</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.640248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.393900</td>\n",
       "      <td>1.300296</td>\n",
       "      <td>0.803665</td>\n",
       "      <td>0.652055</td>\n",
       "      <td>0.803665</td>\n",
       "      <td>0.649580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.408700</td>\n",
       "      <td>1.275341</td>\n",
       "      <td>0.780105</td>\n",
       "      <td>0.623221</td>\n",
       "      <td>0.780105</td>\n",
       "      <td>0.626483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.365800</td>\n",
       "      <td>1.316922</td>\n",
       "      <td>0.748691</td>\n",
       "      <td>0.642867</td>\n",
       "      <td>0.748691</td>\n",
       "      <td>0.611227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/36 00:44 < 00:23, 0.51 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©-requis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, gc, copy, json, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import optuna\n",
    "\n",
    "# (on suppose que tous les imports HF, LoRA, votre collator, compute_metrics,\n",
    "#  WeightedCETrainer, enc(), TokenizedDS, timeout_train(), etc. sont dÃ©jÃ  prÃ©sents)\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "artifact_dir = \"./artifacts\"; os.makedirs(artifact_dir, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chargement et dÃ©coupage du corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df = df.rename(columns={\"gpt_category\": \"cat\"})\n",
    "\n",
    "cats       = sorted(df[\"cat\"].unique())\n",
    "label2id   = {c: i for i, c in enumerate(cats)}\n",
    "id2label   = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS       = len(cats)\n",
    "\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Fonctions utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_model(r_lora, lora_dropout, use_bf16):\n",
    "    \"\"\"Construit un modÃ¨le QLoRA + classification, sans entraÃ®nement.\"\"\"\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    "    )\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id      = tok.pad_token_id\n",
    "    base.config.use_cache         = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    l_cfg = LoraConfig(\n",
    "        r=r_lora,\n",
    "        lora_alpha=2 * r_lora,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "            \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "    model = get_peft_model(base, l_cfg)\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad: p.data = p.data.float()\n",
    "    return model\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Optuna objective  (validation croisÃ©e) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def objective(trial):\n",
    "    # Hyper-paramÃ¨tres\n",
    "    lr            = trial.suggest_float(\"lr\", 5e-5, 5e-4, log=True)\n",
    "    bsz           = trial.suggest_categorical(\"bsz\", [16, 32])\n",
    "    r_lora        = trial.suggest_int(\"lora_r\", 8, 32, step=8)\n",
    "    lora_dropout  = trial.suggest_float(\"lora_dropout\", 0.01, 0.1, log=True)\n",
    "\n",
    "    grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "    use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    use_fp16 = not use_bf16\n",
    "    skf      = StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "    fold_mcc, best_fold_state, best_fold_metric = [], None, -1.0\n",
    "    VAL_FOLDS_TO_USE = [0, 2]\n",
    "\n",
    "    for fold_idx, (tr_idx, va_idx) in enumerate(skf.split(msg_tv, lab_tv)):\n",
    "        if fold_idx not in VAL_FOLDS_TO_USE:\n",
    "            continue\n",
    "\n",
    "        # jeux train / val encodÃ©s\n",
    "        msg_tr = [msg_tv[i] for i in tr_idx]; lab_tr = [lab_tv[i] for i in tr_idx]\n",
    "        msg_va = [msg_tv[i] for i in va_idx]; lab_va = [lab_tv[i] for i in va_idx]\n",
    "        train_ds = TokenizedDS(enc(msg_tr), lab_tr)\n",
    "        val_ds   = TokenizedDS(enc(msg_va), lab_va)\n",
    "\n",
    "        # pondÃ©ration des classes\n",
    "        counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "        cls_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "        model = build_model(r_lora, lora_dropout, use_bf16)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"{artifact_dir}/trial_{trial.number}/fold_{fold_idx}\",\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            per_device_train_batch_size=bsz,\n",
    "            per_device_eval_batch_size=32,\n",
    "            gradient_accumulation_steps=grad_acc,\n",
    "            warmup_ratio=0.08,\n",
    "            learning_rate=lr,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            bf16=use_bf16,\n",
    "            fp16=use_fp16,\n",
    "            gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",               # pas besoin de sauvegarde intermÃ©diaire\n",
    "            seed=SEED,\n",
    "            logging_strategy=\"epoch\",\n",
    "            label_names=[\"labels\"],\n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"eval_mcc\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "\n",
    "        trainer = WeightedCETrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "            class_weights=cls_weights,\n",
    "        )\n",
    "\n",
    "        # entraÃ®nement (protÃ©gÃ© par timeout)\n",
    "        timeout_train(trainer, TIMEOUT_TRAIN)\n",
    "        mcc = trainer.evaluate(val_ds)[\"eval_mcc\"]\n",
    "        fold_mcc.append(mcc)\n",
    "\n",
    "        if mcc >= best_fold_metric:\n",
    "            best_fold_metric = mcc\n",
    "            best_fold_state  = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        del trainer, model\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    mean_mcc = float(np.mean(fold_mcc)) if fold_mcc else -1.0\n",
    "    trial.set_user_attr(\"best_state\", best_fold_state)\n",
    "    return mean_mcc\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lancement de la recherche Optuna â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "best_trial   = study.best_trial\n",
    "best_params  = best_trial.params\n",
    "best_state   = best_trial.user_attrs[\"best_state\"]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Reconstruction & chargement du meilleur modÃ¨le â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "best_model   = build_model(\n",
    "    r_lora       = best_params[\"lora_r\"],\n",
    "    lora_dropout = best_params[\"lora_dropout\"],\n",
    "    use_bf16     = use_bf16,\n",
    ")\n",
    "best_model.load_state_dict(best_state)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PrÃ©paration du jeu test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_ds = TokenizedDS(enc(msg_te), lab_te)\n",
    "trainer = WeightedCETrainer(\n",
    "    model=best_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=f\"{artifact_dir}/best_model_mcc\",\n",
    "        per_device_eval_batch_size=32,\n",
    "        dataloader_drop_last=False,\n",
    "        seed=SEED,\n",
    "    ),\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test MCC :\", round(test_metrics[\"eval_mcc\"], 4))\n",
    "print(\"Test F1  :\", round(test_metrics[\"eval_f1_macro\"], 4))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sauvegarde finale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "save_dir = f\"{artifact_dir}/best_model_mcc\"\n",
    "best_model.save_pretrained(save_dir)\n",
    "tok.save_pretrained(save_dir)\n",
    "with open(os.path.join(save_dir, \"best_params.json\"), \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "\n",
    "print(f\"ModÃ¨le et tokenizer sauvegardÃ©s dans Â« {save_dir} Â».\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c17559",
   "metadata": {
    "id": "98c17559"
   },
   "outputs": [],
   "source": [
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42308a5c",
   "metadata": {
    "id": "42308a5c"
   },
   "outputs": [],
   "source": [
    "  Parameters: lr=3.15e-04, bsz=32, lora_r=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b52b47",
   "metadata": {
    "id": "09b52b47"
   },
   "outputs": [],
   "source": [
    "{'lr': 0.000315, 'bsz': 32, 'lora_r': 16, 'lora_dropout': 0.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c166c60",
   "metadata": {
    "id": "7c166c60"
   },
   "outputs": [],
   "source": [
    "# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Phase finale  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
    "# 1)   Re-chargement du CSV  (Ã©tat vierge)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if \"gpt_category\" in df.columns:\n",
    "    df.rename(columns={\"gpt_category\": \"cat\"}, inplace=True)\n",
    "\n",
    "cats      = sorted(df[\"cat\"].unique())\n",
    "label2id  = {c: i for i, c in enumerate(cats)}\n",
    "id2label  = {i: c for c, i in label2id.items()}\n",
    "df[\"label_id\"] = df[\"cat\"].map(label2id)\n",
    "messages, labels = df[\"text\"].tolist(), df[\"label_id\"].tolist()\n",
    "NUM_LABELS = len(cats)\n",
    "\n",
    "# 2)   DÃ©coupage train / val / test\n",
    "msg_tv, msg_te, lab_tv, lab_te = train_test_split(\n",
    "    messages, labels, test_size=0.10, stratify=labels, random_state=SEED\n",
    ")\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.10, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "\n",
    "# 3)   Encodage\n",
    "tr_enc, va_enc, te_enc = enc(msg_tr), enc(msg_va), enc(msg_te)\n",
    "train_ds = TokenizedDS(tr_enc, lab_tr)\n",
    "val_ds   = TokenizedDS(va_enc, lab_va)\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "\n",
    "# 4)   Hyper-paramÃ¨tres Optuna retenus\n",
    "best_params = json.load(open(artifact_dir / \"best_params.json\"))\n",
    "best_params = {'lr': 0.000315, 'bsz': 32, 'lora_r': 64, 'lora_dropout': 0.1}\n",
    "\n",
    "lr            = best_params[\"lr\"]\n",
    "bsz           = best_params[\"bsz\"]\n",
    "r_lora        = best_params[\"lora_r\"]\n",
    "lora_dropout  = best_params[\"lora_dropout\"]\n",
    "\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / bsz))\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "use_fp16 = not use_bf16\n",
    "\n",
    "# 5)   Instanciation modÃ¨le + LoRA\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(len(tok))\n",
    "base.config.pad_token_id = tok.pad_token_id\n",
    "base.config.use_cache = False\n",
    "base.config.use_paged_attention = True\n",
    "if USE_GRADIENT_CHECKPOINTING:\n",
    "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "l_cfg = LoraConfig(\n",
    "    r=r_lora,\n",
    "    lora_alpha=2 * r_lora,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(base, l_cfg)\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        p.data = p.data.float()\n",
    "\n",
    "# 6)   PondÃ©ration des classes\n",
    "counts       = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "class_weights = torch.tensor(counts.sum() / (NUM_LABELS * (counts + 1e-9)), dtype=torch.float)\n",
    "\n",
    "# 7)   Arguments dâ€™entraÃ®nement finaux\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{artifact_dir}/final_run\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=bsz,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    warmup_ratio=0.08,\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16=use_bf16,\n",
    "    fp16=use_fp16,\n",
    "    gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_mcc\",\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    logging_strategy=\"epoch\",\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "trainer = WeightedCETrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=DataCollatorWithPadding(tok),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "# 8)   EntraÃ®nement + Ã©valuation finale\n",
    "timeout_train(trainer, TIMEOUT_TRAIN)          # <-- garde la mÃªme limite que plus haut\n",
    "print(\"Best val MCC:\", trainer.state.best_metric)\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test set :\", {k: round(v, 4) for k, v in test_metrics.items()})\n",
    "\n",
    "# (facultatif) Sauvegarde du modÃ¨le affinÃ©\n",
    "trainer.save_model(f\"{artifact_dir}/final_model\")\n",
    "tok.save_pretrained(f\"{artifact_dir}/final_model\")\n",
    "# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b5e0e",
   "metadata": {
    "id": "819b5e0e"
   },
   "outputs": [],
   "source": [
    "Test set : {'eval_accuracy': 0.7943, 'eval_f1_macro': 0.6463, 'eval_f1_micro': 0.7943, 'eval_mcc': 0.6533, 'eval_loss': 1.4161, 'eval_runtime': 55.0543, 'eval_samples_per_second': 11.57, 'eval_steps_per_second': 0.363, 'epoch': 16.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c070b8",
   "metadata": {
    "id": "88c070b8"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Scores dÃ©taillÃ©s par classe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) PrÃ©dictions brutes sur le test set\n",
    "pred_res = trainer.predict(test_ds)\n",
    "y_true   = pred_res.label_ids\n",
    "y_pred   = np.argmax(pred_res.predictions, axis=1)\n",
    "\n",
    "# 2) Rapport complet (precision / recall / f1 / support)\n",
    "report = classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=[id2label[i] for i in range(NUM_LABELS)],\n",
    "    digits=4,\n",
    "    output_dict=True          # â† pour lâ€™avoir aussi sous forme dict / DataFrame\n",
    ")\n",
    "print(\"\\n=== Classification report ===\")\n",
    "print(pd.DataFrame(report).T)\n",
    "\n",
    "# 3) Matrice de confusion\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index=[f\"true_{id2label[i]}\"  for i in range(NUM_LABELS)],\n",
    "                     columns=[f\"pred_{id2label[i]}\" for i in range(NUM_LABELS)])\n",
    "print(\"\\n=== Confusion matrix ===\")\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc36e5c",
   "metadata": {
    "id": "9cc36e5c"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  Post-Optuna : 2 seeds + SWA, test, puis full-train final\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, gc, math, copy, random, numpy as np, torch\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Dossiers / fichiers Optuna dÃ©jÃ  crÃ©Ã©s\n",
    "best_params  = json.load(open(os.path.join(artifact_dir, \"best_params.json\")))\n",
    "\n",
    "# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def average_states(paths):\n",
    "    avg = OrderedDict()\n",
    "    for p in paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(paths)).to(torch.float16)\n",
    "    return avg\n",
    "\n",
    "# â”€â”€â”€ Configs rÃ©utilisables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "grad_acc = max(1, math.ceil(EFF_BATCH_TARGET / best_params[\"bsz\"]))\n",
    "l_cfg = LoraConfig(\n",
    "    r=best_params[\"lora_r\"],\n",
    "    lora_alpha=2 * best_params[\"lora_r\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  A.   train+val  (90 %)  â†’  deux seeds + SWA\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase A : 2 seeds sur train+val ==========\")\n",
    "\n",
    "# mini-val interne 10 % pour lâ€™early-stop\n",
    "msg_tr, msg_va, lab_tr, lab_va = train_test_split(\n",
    "    msg_tv, lab_tv, test_size=0.1, stratify=lab_tv, random_state=SEED\n",
    ")\n",
    "tr_enc, va_enc = enc(msg_tr), enc(msg_va)\n",
    "train_ds, val_ds = TokenizedDS(tr_enc, lab_tr), TokenizedDS(va_enc, lab_va)\n",
    "\n",
    "cls_counts = np.bincount(lab_tr, minlength=NUM_LABELS)\n",
    "cls_weights = torch.tensor(\n",
    "    cls_counts.sum() / (NUM_LABELS * (cls_counts + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Re-train seed {seed} (train+val) â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_val\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        eval_on_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_mcc\",\n",
    "        greater_is_better=True,\n",
    "        seed=seed,\n",
    "        logging_strategy=\"epoch\",\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=EARLY_STOP_PATIENCE)],\n",
    "        class_weights=cls_weights,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_val.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths.append(state_path)\n",
    "\n",
    "    del trainer, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd4f9f",
   "metadata": {
    "id": "4bdd4f9f"
   },
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a710",
   "metadata": {
    "id": "2d93a710"
   },
   "outputs": [],
   "source": [
    "def eval_loader(loader, model):\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            for k in batch:\n",
    "                batch[k] = batch[k].to(model.device)\n",
    "            logits = model(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            ).logits\n",
    "            preds.extend(torch.argmax(logits, -1).cpu())\n",
    "            true.extend(batch[\"labels\"].cpu())\n",
    "    return preds, true, cat_metrics(true, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589f347",
   "metadata": {
    "id": "3589f347"
   },
   "outputs": [],
   "source": [
    "import copy, torch, os\n",
    "\n",
    "def load_lora_model(state_path: str):\n",
    "    \"\"\"\n",
    "    Reconstruit le backbone + LoRA et recharge les poids sauvegardÃ©s.\n",
    "    Rien dâ€™autre nâ€™est modifiÃ©.\n",
    "    \"\"\"\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "\n",
    "    state = torch.load(state_path, map_location=\"cpu\")\n",
    "    # seules les tÃªtes LoRA ont Ã©tÃ© sauvegardÃ©es ; on les recharge sans toucher au reste\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "# --- Ã‰valuation -------------------------------------------------------------\n",
    "\n",
    "results = {}\n",
    "\n",
    "for state_path in state_paths:          # state_paths = [\"...seed0_val.pt\", \"...seed13_val.pt\"]\n",
    "    model = load_lora_model(state_path)\n",
    "    _, _, metrics = eval_loader(test_dl, model)   # ou val_loader/test_loader, selon ton besoin\n",
    "    results[os.path.basename(state_path)] = metrics\n",
    "\n",
    "# Petit rÃ©capitulatif lisible :\n",
    "for name, m in results.items():\n",
    "    print(f\"{name:25s} | MCC={m['mcc']:.4f} | Acc={m['accuracy']:.4f} | F1={m['f1_macro']:.4f} | F1={m['f1_micro']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574743b3",
   "metadata": {
    "id": "574743b3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# â€” SWA hors-ligne â€”\n",
    "swa_state = average_states(state_paths)\n",
    "torch.save(swa_state, os.path.join(artifact_dir, \"best_state_swa_val.pt\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if tokenizer.pad_token is None:               # câ€™est le cas le plus frÃ©quent\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# â€” Ã©valuation sur test â€”\n",
    "print(\"\\n=========== Phase B : Ã©valuation SWA sur test =========\")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "base.resize_token_embeddings(VOCAB)\n",
    "\n",
    "\n",
    "\n",
    "base.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_swa = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "model_swa.load_state_dict(torch.load(os.path.join(artifact_dir, \"best_state_swa_val.pt\"), map_location=\"cpu\"), strict=False)\n",
    "\n",
    "\n",
    "\n",
    "test_ds = TokenizedDS(te_enc, lab_te)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, test_metrics = eval_loader(test_dl, model_swa)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#  C.   FULL TRAIN (100 %)  â†’  deux seeds + SWA  â†’  production\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n=========== Phase C : full-train 100 % pour la prod =========\")\n",
    "\n",
    "full_enc = enc(msg_tv + msg_te)\n",
    "full_ds  = TokenizedDS(full_enc, lab_tv + lab_te)\n",
    "\n",
    "cls_counts_all = np.bincount(lab_tv + lab_te, minlength=NUM_LABELS)\n",
    "cls_weights_all = torch.tensor(\n",
    "    cls_counts_all.sum() / (NUM_LABELS * (cls_counts_all + 1e-9)), dtype=torch.float\n",
    ")\n",
    "\n",
    "state_paths_full = []\n",
    "for seed in [0, 13]:\n",
    "    print(f\"\\nâ€”â€” Full-train seed {seed} â€”â€”\")\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(VOCAB)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    if USE_GRADIENT_CHECKPOINTING:\n",
    "        base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    model = get_peft_model(base, copy.deepcopy(l_cfg))\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.data = p.data.float()\n",
    "\n",
    "    # pas de validation â†’ epochs fixes\n",
    "    args_prod = TrainingArguments(\n",
    "        output_dir=os.path.join(artifact_dir, f\"seed_{seed}_full\"),\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=best_params[\"bsz\"],\n",
    "        gradient_accumulation_steps=grad_acc,\n",
    "        warmup_ratio=0.08,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        bf16=use_bf16,\n",
    "        fp16=not use_bf16,\n",
    "        gradient_checkpointing=USE_GRADIENT_CHECKPOINTING,\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_strategy=\"epoch\",\n",
    "        seed=seed,\n",
    "        label_names=[\"labels\"],\n",
    "    )\n",
    "\n",
    "    trainer_prod = WeightedCETrainer(\n",
    "        model=model,\n",
    "        args=args_prod,\n",
    "        train_dataset=full_ds,\n",
    "        data_collator=collator,\n",
    "        class_weights=cls_weights_all,\n",
    "    )\n",
    "    trainer_prod.train()\n",
    "\n",
    "    state_path = os.path.join(artifact_dir, f\"best_state_seed{seed}_full.pt\")\n",
    "    torch.save({k: v.cpu() for k, v in model.state_dict().items() if v.requires_grad}, state_path)\n",
    "    state_paths_full.append(state_path)\n",
    "\n",
    "    del trainer_prod, model, base; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "swa_state_full = average_states(state_paths_full)\n",
    "torch.save(swa_state_full, os.path.join(artifact_dir, \"best_state_swa_full.pt\"))\n",
    "print(\"\\nâœ“ ModÃ¨le SWA full-train enregistrÃ© : best_state_swa_full.pt\")\n",
    "\n",
    "# Optionnel : sauvegarde du tokenizer / meta-infos\n",
    "tok.save_pretrained(os.path.join(artifact_dir, \"tokenizer_final\"))\n",
    "json.dump(\n",
    "    {\n",
    "        \"best_params\": best_params,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"notes\": \"adapter SWA train+val, puis SWA full-train pour production\",\n",
    "    },\n",
    "    open(os.path.join(artifact_dir, \"run_summary.json\"), \"w\"),\n",
    "    indent=2,\n",
    ")\n",
    "print(\"RÃ©sumÃ© sauvegardÃ©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51290589",
   "metadata": {
    "id": "51290589"
   },
   "outputs": [],
   "source": [
    "# helpers_swa.py --------------------------------------------------------------\n",
    "import os, copy, torch\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "def load_base(model_name, num_labels, tok, bnb_cfg, lora_cfg):\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    base.resize_token_embeddings(tok.vocab_size)\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "    base.config.use_cache = False\n",
    "    base.config.use_paged_attention = True\n",
    "    return get_peft_model(base, copy.deepcopy(lora_cfg))\n",
    "\n",
    "def average_states(state_paths):\n",
    "    from collections import OrderedDict\n",
    "    avg = OrderedDict()\n",
    "    for p in state_paths:\n",
    "        sd = torch.load(p, map_location=\"cpu\")\n",
    "        for k, v in sd.items():\n",
    "            avg[k] = avg.get(k, 0.) + v.float()\n",
    "    for k in avg:\n",
    "        avg[k] = (avg[k] / len(state_paths)).to(torch.float16)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde7145",
   "metadata": {
    "id": "bdde7145"
   },
   "outputs": [],
   "source": [
    "# run_swa_eval.py -------------------------------------------------------------\n",
    "import os, torch, copy\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig\n",
    "# -----------------------------------------------------------------------------\n",
    "artifact_dir = \"./outputs\"\n",
    "state_paths = [\n",
    "    os.path.join(artifact_dir, \"best_state_seed0_val.pt\"),\n",
    "    os.path.join(artifact_dir, \"best_state_seed13_val.pt\"),\n",
    "]\n",
    "\n",
    "# â”€ config rÃ©utilisÃ©e (exactement celle de ton script initial) â”€\n",
    "l_cfg        = LoraConfig(           # mÃªmes valeurs quâ€™avant\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    bias=\"none\", task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
    "                    \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    ")\n",
    "use_bf16     = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "bnb_cfg      = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,\n",
    ")\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 1. SWA hors-ligne\n",
    "swa_state = average_states(state_paths)\n",
    "swa_path  = os.path.join(artifact_dir, \"best_state_swa_val.pt\")\n",
    "torch.save(swa_state, swa_path)\n",
    "print(f\"âœ“ SWA sauvegardÃ© : {swa_path}\")\n",
    "\n",
    "# 2. Ã‰valuation\n",
    "model = load_base(MODEL_NAME, NUM_LABELS, tok, bnb_cfg, l_cfg)\n",
    "model.load_state_dict(torch.load(swa_path, map_location=\"cpu\"), strict=False)\n",
    "\n",
    "test_ds  = TokenizedDS(te_enc, lab_te)\n",
    "test_dl  = DataLoader(test_ds, batch_size=32, collate_fn=collator)\n",
    "\n",
    "preds, true, metrics = eval_loader(test_dl)\n",
    "print(\"\\n=== RÃ©sultats test (SWA train+val) ===\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c454a2",
   "metadata": {
    "id": "d9c454a2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6904e9f",
   "metadata": {
    "id": "d6904e9f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905183f",
   "metadata": {
    "id": "3905183f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00234a4b",
   "metadata": {
    "id": "00234a4b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4527ef2b",
   "metadata": {
    "id": "4527ef2b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12991eda",
   "metadata": {
    "id": "12991eda"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512820c",
   "metadata": {
    "id": "b512820c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696928db",
   "metadata": {
    "id": "696928db"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db101ddc",
   "metadata": {
    "id": "db101ddc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f903637",
   "metadata": {
    "id": "4f903637"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20abfe5",
   "metadata": {
    "id": "d20abfe5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98539d0c",
   "metadata": {
    "id": "98539d0c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5bfcc5",
   "metadata": {
    "id": "3a5bfcc5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33f14d",
   "metadata": {
    "id": "2d33f14d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89318778",
   "metadata": {
    "id": "89318778"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc995f",
   "metadata": {
    "id": "6bfc995f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c6441",
   "metadata": {
    "id": "582c6441"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a50a170",
   "metadata": {
    "id": "7a50a170"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4a034",
   "metadata": {
    "id": "53e4a034"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e804",
   "metadata": {
    "id": "9593e804"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0511134c",
   "metadata": {
    "id": "0511134c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96b2e",
   "metadata": {
    "id": "08d96b2e"
   },
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MHgOovRy9dDW",
   "metadata": {
    "id": "MHgOovRy9dDW"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Process Facebook & Instagram CSV exports (v2)\n",
    "\n",
    "DiffÃ©rences principales :\n",
    "1. Chemin du modÃ¨le â†’ dossier `outputs/final_model/` issu de trainer.save_model().\n",
    "2. Prompt identique Ã  lâ€™entraÃ®nement : add_generation_prompt=False.\n",
    "3. BitsAndBytes : dtype = bfloat16 si dispo, sinon float16.\n",
    "4. Tout le reste (reprise, sauvegarde incrÃ©mentale, dÃ©tection de langue)\n",
    "   reste inchangÃ©.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Early env tweaks\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"        # supprime lâ€™avertissement\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Standard lib\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import List, Sequence\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Third-party\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# USER CONFIGURATION\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FACEBOOK_CSV   = \"/home/lisst_ai/FACEBOOK_concat.csv\"\n",
    "INSTAGRAM_CSV  = \"/home/lisst_ai/INSTAGRAM_concat.csv\"\n",
    "\n",
    "# â€¼ï¸ nouveau rÃ©pertoire modÃ¨le sauvegardÃ© par trainer.save_model()\n",
    "MODEL_DIR      = \"/home/lisst_ai/qlora_project/outputs/final_model\"\n",
    "\n",
    "OUT_FACEBOOK   = \"/home/lisst_ai/gaza_facebook_cleaned.csv\"\n",
    "OUT_INSTAGRAM  = \"/home/lisst_ai/gaza_instagram_cleaned.csv\"\n",
    "\n",
    "BATCH_SIZE          = 64\n",
    "BATCH_SAVE_EVERY    = 100          # sauvegarde toutes les 100 batchs\n",
    "NUM_WORKERS: int | None = None     # None â‡’ tous les cÅ“urs CPU\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CONSTANTS & LOGGING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SEP               = \" - \"\n",
    "NUM_WORDS_SAMPLE  = 20\n",
    "TARGET_LANG       = \"en\"\n",
    "CONSTRUCTED_COL   = \"constructed_text\"\n",
    "PRED_COL          = \"predicted_category\"\n",
    "MAX_LENGTH        = 512\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format=\"%(asctime)s | %(levelname)-8s | %(message)s\")\n",
    "log = logging.getLogger(\"process_sm\")\n",
    "\n",
    "TRAIN_CSV = \"/home/lisst_ai/gaza_classified.csv\"      # fallback si id2label absent\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def concatenate_fields(values: Sequence[str | float | None],\n",
    "                       *, sep: str = SEP) -> str:\n",
    "    parts: List[str] = []\n",
    "    for val in values:\n",
    "        if not isinstance(val, str):\n",
    "            continue\n",
    "        val_clean = val.strip()\n",
    "        if not val_clean:\n",
    "            continue\n",
    "        current = sep.join(parts).lower()\n",
    "        if val_clean.lower() in current:\n",
    "            continue\n",
    "        parts.append(val_clean)\n",
    "    return sep.join(parts)\n",
    "\n",
    "def detect_lang(sample: str) -> str | None:\n",
    "    try:\n",
    "        return detect(sample)\n",
    "    except LangDetectException:\n",
    "        return None\n",
    "\n",
    "# â€” worker pour ProcessPoolExecutor (picklable) â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "def _detect_lang_worker(args):\n",
    "    idx, txt = args\n",
    "    words = txt.split()\n",
    "    sample = \" \".join(words[:NUM_WORDS_SAMPLE])\n",
    "    if not sample.strip():\n",
    "        return idx, False\n",
    "    return idx, detect_lang(sample) == TARGET_LANG\n",
    "# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "\n",
    "def filter_english(texts: list[str],\n",
    "                   *, workers: int | None = NUM_WORKERS) -> list[bool]:\n",
    "    workers = workers or os.cpu_count() or 4\n",
    "    log.info(\"Detecting language on %d texts with %d workersâ€¦\",\n",
    "             len(texts), workers)\n",
    "\n",
    "    flags = [False] * len(texts)\n",
    "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
    "        for idx, ok in tqdm(ex.map(_detect_lang_worker,\n",
    "                                   enumerate(texts),\n",
    "                                   chunksize=512),\n",
    "                            total=len(texts),\n",
    "                            desc=\"Lang-detect\",\n",
    "                            unit=\"post\"):\n",
    "            flags[idx] = ok\n",
    "    return flags\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Model loading & inference\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_classifier(model_dir: str, *, train_csv: str = TRAIN_CSV):\n",
    "    p_cfg     = PeftConfig.from_pretrained(model_dir)\n",
    "    base_name = p_cfg.base_model_name_or_path\n",
    "\n",
    "    # 1. id2label / num_labels â€“ on tente la config dâ€™abord\n",
    "    id2label = getattr(p_cfg, \"id2label\", None)\n",
    "    if id2label:\n",
    "        num_labels = len(id2label)\n",
    "    else:\n",
    "        try:\n",
    "            with open(Path(model_dir) / \"adapter_config.json\") as f:\n",
    "                raw = json.load(f)\n",
    "            num_labels = raw.get(\"num_labels\")\n",
    "            id2label   = {int(k): v for k, v in raw.get(\"id2label\", {}).items()}\n",
    "        except FileNotFoundError:\n",
    "            num_labels = None\n",
    "            id2label   = None\n",
    "\n",
    "    # 2. Fallback : reconstruction Ã  partir du CSV dâ€™entraÃ®nement\n",
    "    if not id2label:\n",
    "        df         = pd.read_csv(train_csv, header=None, names=[\"text\", \"cat\"])\n",
    "        cats       = sorted(df[\"cat\"].unique())\n",
    "        id2label   = {i: c for i, c in enumerate(cats)}\n",
    "        num_labels = len(id2label)\n",
    "        log.info(\"id2label reconstruit depuis %s\", train_csv)\n",
    "\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    cfg = AutoConfig.from_pretrained(\n",
    "        base_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=(\n",
    "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_name,\n",
    "        config=cfg,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_name, padding_side=\"left\")\n",
    "    if tok.pad_token_id is None:\n",
    "        tok.add_special_tokens({'pad_token': '<pad>'})\n",
    "        tok.pad_token = '<pad>'\n",
    "        base.resize_token_embeddings(len(tok))\n",
    "    base.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    try:\n",
    "        model = PeftModel.from_pretrained(base, model_dir)\n",
    "    except RuntimeError as e:\n",
    "        log.warning(\"TÃªte LoRA incompatible (%s) â†’ ignore_mismatched_sizes=True\", e)\n",
    "        model = PeftModel.from_pretrained(base, model_dir,\n",
    "                                          ignore_mismatched_sizes=True)\n",
    "    model.eval()\n",
    "    model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "    cats_str = \", \".join(id2label.values())\n",
    "    def build_prompt(txt: str) -> str:\n",
    "        messages = [\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": (\"You are an expert assistant. \"\n",
    "                         \"Classify the following text into one of these \"\n",
    "                         f\"categories: {cats_str}. \"\n",
    "                         \"Respond with the category label only.\")},\n",
    "            {\"role\": \"user\", \"content\": txt},\n",
    "        ]\n",
    "        # â€¼ï¸ add_generation_prompt=False pour coller Ã  lâ€™entraÃ®nement\n",
    "        return tok.apply_chat_template(messages,\n",
    "                                       tokenize=False,\n",
    "                                       add_generation_prompt=False).strip()\n",
    "\n",
    "    return tok, model, build_prompt\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Batch prediction with incremental saving\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def incremental_predict(df: pd.DataFrame,\n",
    "                        tok, model, build_prompt,\n",
    "                        *,\n",
    "                        text_col: str = CONSTRUCTED_COL,\n",
    "                        batch_size: int = BATCH_SIZE,\n",
    "                        save_every: int = BATCH_SAVE_EVERY,\n",
    "                        out_path: str | Path | None = None) -> None:\n",
    "    \"\"\"\n",
    "    Remplit df[PRED_COL] in-place pour les lignes oÃ¹ il manque,\n",
    "    en sauvegardant toutes les `save_every` batchs.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # indices Ã  traiter\n",
    "    to_process = df.index[df[PRED_COL].isna() | (df[PRED_COL] == \"\")].tolist()\n",
    "    if not to_process:\n",
    "        log.info(\"Aucune ligne Ã  catÃ©goriser (dÃ©jÃ  complet).\")\n",
    "        return\n",
    "\n",
    "    total_batches = math.ceil(len(to_process) / batch_size)\n",
    "    batch_counter = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(to_process), batch_size),\n",
    "                  desc=\"Batch-predict\",\n",
    "                  total=total_batches,\n",
    "                  unit=\"batch\",\n",
    "                  ncols=300):\n",
    "        batch_idx   = to_process[i: i + batch_size]\n",
    "        batch_texts = df.loc[batch_idx, text_col].tolist()\n",
    "        prompts     = [build_prompt(t) for t in batch_texts]\n",
    "\n",
    "        enc = tok(prompts,\n",
    "                  return_tensors=\"pt\",\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  max_length=MAX_LENGTH)\n",
    "        enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "        ids = torch.argmax(logits, dim=1).tolist()\n",
    "\n",
    "        labels = [\n",
    "            model.config.id2label[i]\n",
    "            if isinstance(model.config.id2label, dict)\n",
    "            else model.config.id2label[i]\n",
    "            for i in ids\n",
    "        ]\n",
    "        df.loc[batch_idx, PRED_COL] = labels\n",
    "\n",
    "        batch_counter += 1\n",
    "        if out_path and batch_counter % save_every == 0:\n",
    "            log.info(\"Interim save â†’ %s\", out_path)\n",
    "            df.to_csv(out_path, index=False)\n",
    "\n",
    "    if out_path:\n",
    "        log.info(\"Final save â†’ %s\", out_path)\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Per-platform helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "def process_instagram(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    text_cols = [\"Description\", \"Image Text\"]\n",
    "    df[CONSTRUCTED_COL] = df.apply(\n",
    "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1)\n",
    "    return df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Robust CSV loader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path, low_memory=False)\n",
    "    except pd.errors.ParserError as err:\n",
    "        log.warning(\"Standard parser failed for %s (%s). \"\n",
    "                    \"Retrying with engine='python'â€¦\", path, err)\n",
    "        return pd.read_csv(path,\n",
    "                           engine=\"python\",\n",
    "                           on_bad_lines=\"skip\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Platform pipeline with resume capability\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pipeline(name: str,\n",
    "                 src_csv: str | Path,\n",
    "                 out_csv: str | Path,\n",
    "                 builder_fn) -> None:\n",
    "    out_csv = Path(out_csv)\n",
    "    src_csv = Path(src_csv)\n",
    "\n",
    "    if out_csv.exists():\n",
    "        log.info(\"%s: found existing output (%s) â€“ resume mode.\", name, out_csv)\n",
    "        df = safe_read_csv(out_csv)\n",
    "        # reconstruit le texte si absent (rare)\n",
    "        if CONSTRUCTED_COL not in df.columns:\n",
    "            raw = safe_read_csv(src_csv)\n",
    "            df_texts = builder_fn(raw)[[CONSTRUCTED_COL]]\n",
    "            df = df.join(df_texts)\n",
    "    else:\n",
    "        log.info(\"%s: loading raw CSVâ€¦\", name)\n",
    "        raw = safe_read_csv(src_csv)\n",
    "        df = builder_fn(raw)\n",
    "        df[PRED_COL] = pd.NA                         # nouvelle colonne\n",
    "\n",
    "    # filtrage langue pour les lignes non catÃ©gorisÃ©es\n",
    "    mask_uncat = df[PRED_COL].isna() | (df[PRED_COL] == \"\")\n",
    "    to_check   = df.loc[mask_uncat, CONSTRUCTED_COL].tolist()\n",
    "    if to_check:\n",
    "        log.info(\"%s: language filtering (%d lignes)â€¦\", name, len(to_check))\n",
    "        flags = filter_english(to_check)\n",
    "        df = df.loc[~mask_uncat | pd.Series(flags,\n",
    "                                            index=df.loc[mask_uncat].index)\n",
    "                    ].reset_index(drop=True)\n",
    "\n",
    "    # classification\n",
    "    tok, model, build_prompt = load_classifier(MODEL_DIR)\n",
    "    incremental_predict(df, tok, model, build_prompt,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        save_every=BATCH_SAVE_EVERY,\n",
    "                        out_path=out_csv)\n",
    "\n",
    "    log.info(\"%s: completed (%d rows total).\", name, len(df))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Main routine\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main() -> None:\n",
    "    run_pipeline(\"Facebook\",\n",
    "                 FACEBOOK_CSV,\n",
    "                 OUT_FACEBOOK,\n",
    "                 process_facebook)\n",
    "\n",
    "    run_pipeline(\"Instagram\",\n",
    "                 INSTAGRAM_CSV,\n",
    "                 OUT_INSTAGRAM,\n",
    "                 process_instagram)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9360076e",
   "metadata": {
    "id": "9360076e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93be0a7a",
   "metadata": {
    "id": "93be0a7a"
   },
   "outputs": [],
   "source": [
    "sklearn.model_selection.cross_val_score(..., return_estimator=True) â†’ bootstrap des scores pour IC 95 % ; on publie toujours : Â« MCC 0.71 Â± 0.03 Â».\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af11ce2",
   "metadata": {
    "id": "9af11ce2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1149fa",
   "metadata": {
    "id": "3f1149fa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "093949c9ff6c47109c73936559022568": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_2debc053862a4c6ea56fd8bd79a73459",
      "max": 17078136,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b29b016319b14adab629eb2647d3a355",
      "tabbable": null,
      "tooltip": null,
      "value": 17078136
     }
    },
    "0b7ec87b9c15483f85a55e7610ac2642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "0d3cba73f4fc4fa68037c32ca2035efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "0dbf1d6e218c4930b5beaa843c985fe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_25f3c9542b2b4467b8995ba923a5fefd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e12099fda04b4db4ae4405ea17776f74",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer_config.json:â€‡"
     }
    },
    "1497d4b7748348b6ba6cd32593194e1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b0838891b6f46c68400e1ed3c912a81",
       "IPY_MODEL_093949c9ff6c47109c73936559022568",
       "IPY_MODEL_b5341442f83e4b30ac45d26a068caa96"
      ],
      "layout": "IPY_MODEL_33659f055420477d96ed9536fc933440",
      "tabbable": null,
      "tooltip": null
     }
    },
    "179aead8a0d84108bd3d6b68c68c89d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b3d07ab5e6849ccaf1c54ed915db588": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_280b5d5f5b5d4358ba6a4fe7caa8d565",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0d3cba73f4fc4fa68037c32ca2035efb",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡414/414â€‡[00:00&lt;00:00,â€‡55.2kB/s]"
     }
    },
    "1c34568ec32d4ba4a38e495705928d90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "236a9c2d5ebd485ebce91732a03c2c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25f3c9542b2b4467b8995ba923a5fefd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "279fba77a40a4a73b5f7ddc30dfcc9e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_a6cbb34af4d04414a0bc2dc11a191298",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de97cc7484b74b92905f2d67c184cd0a",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "280b5d5f5b5d4358ba6a4fe7caa8d565": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c102e3e0c4f4d6f99abbf728b8e9e28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "2debc053862a4c6ea56fd8bd79a73459": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e10d96515ad451b87d50265639c2552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "33659f055420477d96ed9536fc933440": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "364f174037914d769f413641bdc67f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37f74260cc4b4311b1edf58a3bd6c8d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_89c6b12b18704bf8a1f9a49c32a1cd80",
      "max": 414,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6566f97e6225481f8bb153dd3151dcf6",
      "tabbable": null,
      "tooltip": null,
      "value": 414
     }
    },
    "4aacc61fdb374caebed92f9096b21944": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4adc0eadab4b441c9346d96e50a21f6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_956baf5852b44eb097ea07b913f0b9cb",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_364f174037914d769f413641bdc67f08",
      "tabbable": null,
      "tooltip": null,
      "value": 1
     }
    },
    "4ef56778e4f54c179af64e0ec72750db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_6af59cb80b32457ba43ff1135128c83d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7d43a6ed71f469e88b448a2e681920e",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡4/4â€‡[00:16&lt;00:00,â€‡â€‡5.69s/it]"
     }
    },
    "5ad0ee1341244193a6743bd2a2207833": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83db65deecc9408d99574c0c5c74a552",
       "IPY_MODEL_279fba77a40a4a73b5f7ddc30dfcc9e0",
       "IPY_MODEL_4ef56778e4f54c179af64e0ec72750db"
      ],
      "layout": "IPY_MODEL_8fe1094fb7d546b5977f1f4b75972e16",
      "tabbable": null,
      "tooltip": null
     }
    },
    "5b15e0aef23345918af904b64a911ed6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5dd368b1abe74b54a8ef5c23ae2cfeb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6566f97e6225481f8bb153dd3151dcf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6af59cb80b32457ba43ff1135128c83d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70891aac81c64db488a3b38c99d26f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "7241fcdcbd5d4679866243d592b29094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_5b15e0aef23345918af904b64a911ed6",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd6a0872a8c8439ca657648a7858834d",
      "tabbable": null,
      "tooltip": null,
      "value": 4
     }
    },
    "82bfbabc74e04380a49b6083c8388813": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_179aead8a0d84108bd3d6b68c68c89d8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c62fff4e2c1f4cbd8b376d740e066325",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡4/4â€‡[00:16&lt;00:00,â€‡â€‡5.76s/it]"
     }
    },
    "83db65deecc9408d99574c0c5c74a552": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_236a9c2d5ebd485ebce91732a03c2c5f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2e10d96515ad451b87d50265639c2552",
      "tabbable": null,
      "tooltip": null,
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "873e6d9585f0426b88eb064c4c43c73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "89c6b12b18704bf8a1f9a49c32a1cd80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b0838891b6f46c68400e1ed3c912a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_1c34568ec32d4ba4a38e495705928d90",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70891aac81c64db488a3b38c99d26f74",
      "tabbable": null,
      "tooltip": null,
      "value": "tokenizer.json:â€‡100%"
     }
    },
    "8fe1094fb7d546b5977f1f4b75972e16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9134ea58f63345d696bddbfeacf72b37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94c5951d08d0450485c607bc737a4770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a08c06618f61423783448d2d965d65db",
       "IPY_MODEL_37f74260cc4b4311b1edf58a3bd6c8d5",
       "IPY_MODEL_1b3d07ab5e6849ccaf1c54ed915db588"
      ],
      "layout": "IPY_MODEL_d6d08beaf22b43128ecb74b24ae2a353",
      "tabbable": null,
      "tooltip": null
     }
    },
    "956baf5852b44eb097ea07b913f0b9cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9715b7d58c334f8b9cb16e17483fdc43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a08c06618f61423783448d2d965d65db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_9134ea58f63345d696bddbfeacf72b37",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_873e6d9585f0426b88eb064c4c43c73e",
      "tabbable": null,
      "tooltip": null,
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "a6cbb34af4d04414a0bc2dc11a191298": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0f0ccfbe9214be3b4613e6d25d735ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c5f17f05acdb496c869d63a4103bc3ec",
       "IPY_MODEL_7241fcdcbd5d4679866243d592b29094",
       "IPY_MODEL_82bfbabc74e04380a49b6083c8388813"
      ],
      "layout": "IPY_MODEL_5dd368b1abe74b54a8ef5c23ae2cfeb5",
      "tabbable": null,
      "tooltip": null
     }
    },
    "b1faa0869bf54d5399433efe4e083f8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "b29b016319b14adab629eb2647d3a355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b5341442f83e4b30ac45d26a068caa96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_e80a79c956d7489db2f4be66166ce017",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b1faa0869bf54d5399433efe4e083f8a",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡17.1M/17.1Mâ€‡[00:01&lt;00:00,â€‡13.3MB/s]"
     }
    },
    "b637c5cd40dc450cbb7573e2a8935857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_9715b7d58c334f8b9cb16e17483fdc43",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0b7ec87b9c15483f85a55e7610ac2642",
      "tabbable": null,
      "tooltip": null,
      "value": "â€‡181k/?â€‡[00:00&lt;00:00,â€‡19.9MB/s]"
     }
    },
    "c5f17f05acdb496c869d63a4103bc3ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_allow_html": false,
      "layout": "IPY_MODEL_f3dd43f426a541ad936365e395cc9dc5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2c102e3e0c4f4d6f99abbf728b8e9e28",
      "tabbable": null,
      "tooltip": null,
      "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
     }
    },
    "c62fff4e2c1f4cbd8b376d740e066325": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "c7d43a6ed71f469e88b448a2e681920e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "cd6a0872a8c8439ca657648a7858834d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6d08beaf22b43128ecb74b24ae2a353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d859487e62ae4f1da1284846bc7abcb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "2.0.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0dbf1d6e218c4930b5beaa843c985fe0",
       "IPY_MODEL_4adc0eadab4b441c9346d96e50a21f6a",
       "IPY_MODEL_b637c5cd40dc450cbb7573e2a8935857"
      ],
      "layout": "IPY_MODEL_4aacc61fdb374caebed92f9096b21944",
      "tabbable": null,
      "tooltip": null
     }
    },
    "de97cc7484b74b92905f2d67c184cd0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e12099fda04b4db4ae4405ea17776f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "2.0.0",
     "model_name": "HTMLStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "2.0.0",
      "_model_name": "HTMLStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "StyleView",
      "background": null,
      "description_width": "",
      "font_size": null,
      "text_color": null
     }
    },
    "e80a79c956d7489db2f4be66166ce017": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f3dd43f426a541ad936365e395cc9dc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "2.0.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "2.0.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "2.0.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border_bottom": null,
      "border_left": null,
      "border_right": null,
      "border_top": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
