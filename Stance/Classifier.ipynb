{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facebook Post Stance Classifier\n",
        "\n",
        "Classify Facebook posts using the trained Ministral-8B stance classifier.\n",
        "\n",
        "This notebook loads the trained model from `best_adapter/` and applies it to a CSV file containing Facebook posts. It uses the same 3-class system:\n",
        "- **Pro-Palestinian**\n",
        "- **Pro-Israeli**  \n",
        "- **Neutral** (merged from Other, Off-topic, Anti-War_Pro-Peace)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Sequence\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Optional: for language filtering\n",
        "try:\n",
        "    from langdetect import detect, LangDetectException\n",
        "    HAS_LANGDETECT = True\n",
        "except ImportError:\n",
        "    HAS_LANGDETECT = False\n",
        "    logging.warning(\"langdetect not installed. Language filtering disabled.\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your paths and parameters here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and data paths\n",
        "# Update these paths to point to your files in Google Drive\n",
        "MODEL_DIR = \"/content/drive/MyDrive/best_adapter\"  # Path to trained model directory (update this!)\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/Stance/gaza_stance_sampled_classified.csv\"  # For label mapping fallback (update this!)\n",
        "\n",
        "# Input/Output paths\n",
        "INPUT_CSV = \"/content/drive/MyDrive/Data/Final data to use/finaldataSample20251127.csv\"  # Set your input CSV path here (update this!)\n",
        "OUTPUT_CSV = None  # Will be auto-generated if None (input_name_classified.csv)\n",
        "\n",
        "# Processing settings\n",
        "BATCH_SIZE = 32\n",
        "BATCH_SAVE_EVERY = 100  # Save every N batches\n",
        "MAX_LENGTH = 512\n",
        "SEP = \" - \"\n",
        "CONSTRUCTED_COL = \"constructed_text\"\n",
        "PRED_COL = \"predicted_category\"\n",
        "\n",
        "# Language filtering (optional)\n",
        "FILTER_ENGLISH = True\n",
        "TARGET_LANG = \"en\"\n",
        "NUM_WORDS_SAMPLE = 100\n",
        "NUM_WORKERS = None  # None = use all CPU cores\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\"\n",
        ")\n",
        "log = logging.getLogger(\"stance_classifier\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def strip_invisible(text: str) -> str:\n",
        "    \"\"\"Remove zero-width characters.\"\"\"\n",
        "    zero_width_re = re.compile(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF]\")\n",
        "    return zero_width_re.sub(\"\", text)\n",
        "\n",
        "\n",
        "def concatenate_fields(values: Sequence[str | float | None], *, sep: str = SEP) -> str:\n",
        "    \"\"\"Concatenate text fields, avoiding duplicates.\"\"\"\n",
        "    parts: List[str] = []\n",
        "    for val in values:\n",
        "        if not isinstance(val, str):\n",
        "            continue\n",
        "        val_clean = val.strip()\n",
        "        if not val_clean:\n",
        "            continue\n",
        "        current = sep.join(parts).lower()\n",
        "        if val_clean.lower() in current:\n",
        "            continue\n",
        "        parts.append(val_clean)\n",
        "    return sep.join(parts)\n",
        "\n",
        "\n",
        "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reconstruct full text from Facebook CSV columns.\"\"\"\n",
        "    text_cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
        "    df[CONSTRUCTED_COL] = df.apply(\n",
        "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
        "    \"\"\"Robust CSV reader with fallback.\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    except pd.errors.ParserError as err:\n",
        "        log.warning(f\"Standard parser failed for {path}. Retrying with engine='python'...\")\n",
        "        return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language Filtering (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _detect_lang_worker(args):\n",
        "    \"\"\"Worker for parallel language detection.\"\"\"\n",
        "    idx, txt = args\n",
        "    if not HAS_LANGDETECT:\n",
        "        return idx, True  # Skip filtering if langdetect not available\n",
        "    words = txt.split()\n",
        "    sample = \" \".join(words[:NUM_WORDS_SAMPLE])\n",
        "    if not sample.strip():\n",
        "        return idx, False\n",
        "    try:\n",
        "        return idx, detect(sample) == TARGET_LANG\n",
        "    except (LangDetectException, Exception):\n",
        "        return idx, False\n",
        "\n",
        "\n",
        "def filter_english(texts: list[str], *, workers: int | None = NUM_WORKERS) -> list[bool]:\n",
        "    \"\"\"Filter texts to keep only English ones.\"\"\"\n",
        "    if not HAS_LANGDETECT:\n",
        "        log.warning(\"langdetect not available. Skipping language filtering.\")\n",
        "        return [True] * len(texts)\n",
        "    \n",
        "    workers = workers or os.cpu_count() or 4\n",
        "    log.info(f\"Detecting language on {len(texts)} texts with {workers} workers...\")\n",
        "    \n",
        "    flags = [False] * len(texts)\n",
        "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
        "        for idx, ok in tqdm(\n",
        "            ex.map(_detect_lang_worker, enumerate(texts), chunksize=512),\n",
        "            total=len(texts),\n",
        "            desc=\"Lang-detect\",\n",
        "            unit=\"post\"\n",
        "        ):\n",
        "            flags[idx] = ok\n",
        "    return flags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classifier(model_dir: str, *, train_csv: str = TRAIN_CSV):\n",
        "    \"\"\"Load the trained classifier model and tokenizer.\"\"\"\n",
        "    log.info(f\"Loading model from {model_dir}...\")\n",
        "    \n",
        "    # Load PEFT config\n",
        "    p_cfg = PeftConfig.from_pretrained(model_dir)\n",
        "    base_name = p_cfg.base_model_name_or_path\n",
        "    \n",
        "    # Determine label mapping\n",
        "    id2label = getattr(p_cfg, \"id2label\", None)\n",
        "    num_labels = None\n",
        "    \n",
        "    # Try to get from PEFT config\n",
        "    if id2label:\n",
        "        num_labels = len(id2label)\n",
        "        # Convert to dict if needed\n",
        "        if isinstance(id2label, list):\n",
        "            id2label = {i: label for i, label in enumerate(id2label)}\n",
        "    \n",
        "    # If not found, try adapter_config.json\n",
        "    if not id2label or not num_labels:\n",
        "        try:\n",
        "            with open(Path(model_dir) / \"adapter_config.json\") as f:\n",
        "                raw = json.load(f)\n",
        "            num_labels = raw.get(\"num_labels\")\n",
        "            id2label_dict = raw.get(\"id2label\", {})\n",
        "            if id2label_dict:\n",
        "                id2label = {int(k): v for k, v in id2label_dict.items()}\n",
        "                num_labels = len(id2label)\n",
        "        except (FileNotFoundError, KeyError, ValueError):\n",
        "            pass\n",
        "    \n",
        "    # Fallback: reconstruct from training CSV\n",
        "    if not id2label or not num_labels:\n",
        "        log.info(f\"Reconstructing labels from {train_csv}...\")\n",
        "        try:\n",
        "            df = pd.read_csv(train_csv)\n",
        "            if \"gpt_category\" in df.columns:\n",
        "                cat_col = \"gpt_category\"\n",
        "            elif \"cat\" in df.columns:\n",
        "                cat_col = \"cat\"\n",
        "            else:\n",
        "                raise ValueError(\"Cannot find category column in training CSV\")\n",
        "            \n",
        "            # Apply same class merging as training\n",
        "            FUSE_MAP = {\n",
        "                \"Pro-Palestinian\": \"Pro-Palestinian\",\n",
        "                \"Pro-Israeli\": \"Pro-Israeli\",\n",
        "                \"Other\": \"Neutral\",\n",
        "                \"Off-topic\": \"Neutral\",\n",
        "                \"Anti-War_Pro-Peace\": \"Neutral\",\n",
        "            }\n",
        "            cats = sorted(df[cat_col].map(FUSE_MAP).dropna().unique())\n",
        "            id2label = {i: c for i, c in enumerate(cats)}\n",
        "            num_labels = len(id2label)\n",
        "        except Exception as e:\n",
        "            log.error(f\"Failed to reconstruct labels: {e}\")\n",
        "            raise ValueError(f\"Could not determine label mapping. Please check {train_csv} exists and has the correct columns.\")\n",
        "    \n",
        "    # Validate we have labels\n",
        "    if not id2label or num_labels is None or num_labels == 0:\n",
        "        raise ValueError(f\"Invalid label mapping: id2label={id2label}, num_labels={num_labels}\")\n",
        "    \n",
        "    label2id = {v: k for k, v in id2label.items()}\n",
        "    log.info(f\"Label mapping: {id2label}\")\n",
        "    log.info(f\"Number of labels: {num_labels}\")\n",
        "    \n",
        "    # Load config - only pass num_labels and id2label if they're valid\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        base_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "    \n",
        "    # Setup quantization (same as training)\n",
        "    bnb_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=(\n",
        "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    # Load base model\n",
        "    log.info(f\"Loading base model: {base_name}\")\n",
        "    base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_name,\n",
        "        config=cfg,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=bnb_cfg,\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir, padding_side=\"left\")\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.add_special_tokens({'pad_token': '<pad>'})\n",
        "        tok.pad_token = '<pad>'\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.pad_token_id = tok.pad_token_id\n",
        "    \n",
        "    # Load PEFT adapter\n",
        "    try:\n",
        "        model = PeftModel.from_pretrained(base, model_dir)\n",
        "    except RuntimeError as e:\n",
        "        log.warning(f\"LoRA head incompatible ({e}) → using ignore_mismatched_sizes=True\")\n",
        "        model = PeftModel.from_pretrained(base, model_dir, ignore_mismatched_sizes=True)\n",
        "    \n",
        "    model.eval()\n",
        "    model.config.pad_token_id = tok.pad_token_id\n",
        "    \n",
        "    # Build prompt function (same as training)\n",
        "    cats_str = \", \".join(id2label.values())\n",
        "    def build_prompt(txt: str) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are an expert assistant. \"\n",
        "                    \"Classify the following text into one of these \"\n",
        "                    f\"categories: {cats_str}. \"\n",
        "                    \"Respond with the category label only.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": txt},\n",
        "        ]\n",
        "        return tok.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        ).strip()\n",
        "    \n",
        "    return tok, model, build_prompt, id2label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def incremental_predict(\n",
        "    df: pd.DataFrame,\n",
        "    tok,\n",
        "    model,\n",
        "    build_prompt,\n",
        "    id2label: dict,\n",
        "    *,\n",
        "    text_col: str = CONSTRUCTED_COL,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    save_every: int = BATCH_SAVE_EVERY,\n",
        "    out_path: str | Path | None = None,\n",
        ") -> None:\n",
        "    \"\"\"Predict categories for texts, saving incrementally.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Find rows to process\n",
        "    to_process = df.index[df[PRED_COL].isna() | (df[PRED_COL] == \"\")].tolist()\n",
        "    if not to_process:\n",
        "        log.info(\"No rows to categorize (already complete).\")\n",
        "        return\n",
        "    \n",
        "    total_batches = math.ceil(len(to_process) / batch_size)\n",
        "    batch_counter = 0\n",
        "    \n",
        "    log.info(f\"Processing {len(to_process)} rows in {total_batches} batches...\")\n",
        "    \n",
        "    for i in tqdm(\n",
        "        range(0, len(to_process), batch_size),\n",
        "        desc=\"Batch-predict\",\n",
        "        total=total_batches,\n",
        "        unit=\"batch\",\n",
        "    ):\n",
        "        batch_idx = to_process[i : i + batch_size]\n",
        "        batch_texts = df.loc[batch_idx, text_col].apply(strip_invisible).tolist()\n",
        "        prompts = [build_prompt(t) for t in batch_texts]\n",
        "        \n",
        "        # Tokenize\n",
        "        enc = tok(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            add_special_tokens=False,  # Important: same as training\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "        ids = torch.argmax(logits, dim=1).tolist()\n",
        "        \n",
        "        # Map to labels\n",
        "        labels = [id2label[i] for i in ids]\n",
        "        df.loc[batch_idx, PRED_COL] = labels\n",
        "        \n",
        "        batch_counter += 1\n",
        "        if out_path and batch_counter % save_every == 0:\n",
        "            log.info(f\"Interim save → {out_path}\")\n",
        "            df.to_csv(out_path, index=False)\n",
        "    \n",
        "    if out_path:\n",
        "        log.info(f\"Final save → {out_path}\")\n",
        "        df.to_csv(out_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Classification Pipeline\n",
        "\n",
        "Run the classification by executing the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare paths\n",
        "input_csv = Path(INPUT_CSV)\n",
        "if not input_csv.exists():\n",
        "    raise FileNotFoundError(f\"Input CSV not found: {input_csv}\")\n",
        "\n",
        "if OUTPUT_CSV is None:\n",
        "    output_csv = input_csv.parent / f\"{input_csv.stem}_classified{input_csv.suffix}\"\n",
        "else:\n",
        "    output_csv = Path(OUTPUT_CSV)\n",
        "\n",
        "print(f\"Input CSV: {input_csv}\")\n",
        "print(f\"Output CSV: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load or create dataframe\n",
        "if output_csv.exists():\n",
        "    log.info(f\"Found existing output ({output_csv}) – resume mode.\")\n",
        "    df = safe_read_csv(output_csv)\n",
        "    if CONSTRUCTED_COL not in df.columns:\n",
        "        raw = safe_read_csv(input_csv)\n",
        "        df_texts = process_facebook(raw)[[CONSTRUCTED_COL]]\n",
        "        df = df.join(df_texts)\n",
        "else:\n",
        "    log.info(f\"Loading raw CSV: {input_csv}\")\n",
        "    df = process_facebook(safe_read_csv(input_csv))\n",
        "    df[PRED_COL] = pd.NA\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Language filtering (if enabled)\n",
        "if FILTER_ENGLISH:\n",
        "    mask_uncat = df[PRED_COL].isna() | (df[PRED_COL] == \"\")\n",
        "    to_check = df.loc[mask_uncat, CONSTRUCTED_COL].tolist()\n",
        "    if to_check:\n",
        "        log.info(f\"Language filtering ({len(to_check)} rows)...\")\n",
        "        flags = filter_english(to_check)\n",
        "        df = df.loc[\n",
        "            ~mask_uncat | pd.Series(flags, index=df.loc[mask_uncat].index)\n",
        "        ].reset_index(drop=True)\n",
        "        print(f\"After language filtering: {len(df)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tok, model, build_prompt, id2label = load_classifier(MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run predictions\n",
        "incremental_predict(\n",
        "    df,\n",
        "    tok,\n",
        "    model,\n",
        "    build_prompt,\n",
        "    id2label,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    save_every=BATCH_SAVE_EVERY,\n",
        "    out_path=output_csv,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results summary\n",
        "log.info(f\"Completed! Results saved to {output_csv}\")\n",
        "log.info(f\"Total rows: {len(df)}\")\n",
        "\n",
        "if PRED_COL in df.columns:\n",
        "    counts = df[PRED_COL].value_counts()\n",
        "    log.info(\"Classification summary:\")\n",
        "    for cat, count in counts.items():\n",
        "        log.info(f\"  {cat}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Also display as a nice table\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Classification Summary\")\n",
        "    print(\"=\"*50)\n",
        "    display(counts.to_frame(\"Count\").assign(Percentage=lambda x: (x['Count'] / len(df) * 100).round(1)))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
