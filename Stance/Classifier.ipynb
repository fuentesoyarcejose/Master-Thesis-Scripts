{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Facebook Post Stance Classifier\n",
        "\n",
        "Classify Facebook posts using the trained Ministral-8B stance classifier.\n",
        "\n",
        "This notebook loads the trained model from `best_adapter/` and applies it to a CSV file containing Facebook posts. It uses the same 3-class system:\n",
        "- **Pro-Palestinian**\n",
        "- **Pro-Israeli**  \n",
        "- **Neutral** (merged from Other, Off-topic, Anti-War_Pro-Peace)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Sequence\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Optional: for language filtering\n",
        "try:\n",
        "    from langdetect import detect, LangDetectException\n",
        "    HAS_LANGDETECT = True\n",
        "except ImportError:\n",
        "    HAS_LANGDETECT = False\n",
        "    logging.warning(\"langdetect not installed. Language filtering disabled.\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.DtypeWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q transformers accelerate peft langdetect tqdm pandas huggingface_hub\n",
        "\n",
        "# Install bitsandbytes (required for 4-bit quantization)\n",
        "print(\"Installing bitsandbytes...\")\n",
        "!pip install -q bitsandbytes\n",
        "print(\"✓ bitsandbytes installed\")\n",
        "\n",
        "# Install flash-attn (optional but recommended for faster inference)\n",
        "# Note: This may take a few minutes to compile\n",
        "print(\"\\nInstalling flash-attn (this may take a few minutes)...\")\n",
        "try:\n",
        "    !pip install -q flash-attn --no-build-isolation\n",
        "    print(\"✓ flash-attn installed\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ flash-attn installation failed (will use default attention): {e}\")\n",
        "    print(\"This is optional - the model will still work with default attention.\")\n",
        "\n",
        "# Verify installations\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(\"✓ bitsandbytes imported successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Warning: bitsandbytes import failed: {e}\")\n",
        "    print(\"You may need to restart the runtime after installation.\")\n",
        "\n",
        "try:\n",
        "    import flash_attn\n",
        "    print(\"✓ flash-attn imported successfully\")\n",
        "except ImportError:\n",
        "    print(\"⚠ flash-attn not available (will use default attention)\")\n",
        "\n",
        "print(\"\\n✓ All dependencies installed!\")\n",
        "print(\"\\n⚠️ NEXT STEP: Restart runtime (Runtime → Restart runtime), then run all cells from the beginning.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hugging Face Authentication\n",
        "\n",
        "Set your Hugging Face token to access gated models or private repositories:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Enter your token directly (not recommended for sharing - token will be visible)\n",
        "# HF_TOKEN = \"your_huggingface_token_here\"\n",
        "\n",
        "# Option 2: Use getpass to securely input token (recommended)\n",
        "from getpass import getpass\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face token: \")\n",
        "\n",
        "# Option 3: Read from environment variable (if already set)\n",
        "# HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "\n",
        "# Set the token for Hugging Face Hub\n",
        "if HF_TOKEN:\n",
        "    from huggingface_hub import login\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"✓ Hugging Face authentication successful!\")\n",
        "else:\n",
        "    print(\"⚠ No Hugging Face token provided. This may fail if accessing gated/private models.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your paths and parameters here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and data paths\n",
        "# Update these paths to point to your files in Google Drive\n",
        "MODEL_DIR = \"/content/drive/MyDrive/UTJ2/Mémoire/Mémoire Scripts/Stance/best_adapter\"  # Path to trained model directory (update this!)\n",
        "TRAIN_CSV = \"/content/drive/MyDrive/UTJ2/Mémoire/Mémoire Scripts/Stance/gaza_stance_sampled_classified.csv\"  # For label mapping fallback (update this!)\n",
        "\n",
        "# Input/Output paths\n",
        "INPUT_CSV = \"/content/drive/MyDrive/UTJ2/Mémoire/Data/Final data to use/finaldataSample20251127.csv\"  # Set your input CSV path here (update this!)\n",
        "OUTPUT_CSV = None  # Will be auto-generated if None (input_name_classified.csv)\n",
        "\n",
        "# Processing settings\n",
        "BATCH_SIZE = 32\n",
        "BATCH_SAVE_EVERY = 100  # Save every N batches\n",
        "MAX_LENGTH = 512\n",
        "SEP = \" - \"\n",
        "CONSTRUCTED_COL = \"constructed_text\"\n",
        "PRED_COL = \"predicted_category\"\n",
        "\n",
        "# Language filtering (optional)\n",
        "FILTER_ENGLISH = True\n",
        "TARGET_LANG = \"en\"\n",
        "NUM_WORDS_SAMPLE = 100\n",
        "NUM_WORKERS = None  # None = use all CPU cores\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)-8s | %(message)s\"\n",
        ")\n",
        "log = logging.getLogger(\"stance_classifier\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def strip_invisible(text: str) -> str:\n",
        "    \"\"\"Remove zero-width characters.\"\"\"\n",
        "    zero_width_re = re.compile(r\"[\\u200B-\\u200F\\u202A-\\u202E\\u2060-\\u206F\\uFEFF]\")\n",
        "    return zero_width_re.sub(\"\", text)\n",
        "\n",
        "\n",
        "def concatenate_fields(values: Sequence[str | float | None], *, sep: str = SEP) -> str:\n",
        "    \"\"\"Concatenate text fields, avoiding duplicates.\"\"\"\n",
        "    parts: List[str] = []\n",
        "    for val in values:\n",
        "        if not isinstance(val, str):\n",
        "            continue\n",
        "        val_clean = val.strip()\n",
        "        if not val_clean:\n",
        "            continue\n",
        "        current = sep.join(parts).lower()\n",
        "        if val_clean.lower() in current:\n",
        "            continue\n",
        "        parts.append(val_clean)\n",
        "    return sep.join(parts)\n",
        "\n",
        "\n",
        "def process_facebook(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reconstruct full text from Facebook CSV columns.\"\"\"\n",
        "    text_cols = [\"Message\", \"Description\", \"Image Text\", \"Link Text\"]\n",
        "    df[CONSTRUCTED_COL] = df.apply(\n",
        "        lambda row: concatenate_fields([row.get(c) for c in text_cols]), axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def safe_read_csv(path: str | Path) -> pd.DataFrame:\n",
        "    \"\"\"Robust CSV reader with fallback.\"\"\"\n",
        "    try:\n",
        "        return pd.read_csv(path, low_memory=False)\n",
        "    except pd.errors.ParserError as err:\n",
        "        log.warning(f\"Standard parser failed for {path}. Retrying with engine='python'...\")\n",
        "        return pd.read_csv(path, engine=\"python\", on_bad_lines=\"skip\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language Filtering (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _detect_lang_worker(args):\n",
        "    \"\"\"Worker for parallel language detection.\"\"\"\n",
        "    idx, txt = args\n",
        "    if not HAS_LANGDETECT:\n",
        "        return idx, True  # Skip filtering if langdetect not available\n",
        "    words = txt.split()\n",
        "    sample = \" \".join(words[:NUM_WORDS_SAMPLE])\n",
        "    if not sample.strip():\n",
        "        return idx, False\n",
        "    try:\n",
        "        return idx, detect(sample) == TARGET_LANG\n",
        "    except (LangDetectException, Exception):\n",
        "        return idx, False\n",
        "\n",
        "\n",
        "def filter_english(texts: list[str], *, workers: int | None = NUM_WORKERS) -> list[bool]:\n",
        "    \"\"\"Filter texts to keep only English ones.\"\"\"\n",
        "    if not HAS_LANGDETECT:\n",
        "        log.warning(\"langdetect not available. Skipping language filtering.\")\n",
        "        return [True] * len(texts)\n",
        "    \n",
        "    workers = workers or os.cpu_count() or 4\n",
        "    log.info(f\"Detecting language on {len(texts)} texts with {workers} workers...\")\n",
        "    \n",
        "    flags = [False] * len(texts)\n",
        "    with ProcessPoolExecutor(max_workers=workers) as ex:\n",
        "        for idx, ok in tqdm(\n",
        "            ex.map(_detect_lang_worker, enumerate(texts), chunksize=512),\n",
        "            total=len(texts),\n",
        "            desc=\"Lang-detect\",\n",
        "            unit=\"post\"\n",
        "        ):\n",
        "            flags[idx] = ok\n",
        "    return flags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_classifier(model_dir: str):\n",
        "    \"\"\"\n",
        "    Load the trained 2-class stance classifier (Pro-Palestinian vs Pro-Israeli).\n",
        "    \n",
        "    The adapter was trained with 2 classes:\n",
        "    - 0: Pro-Palestinian\n",
        "    - 1: Pro-Israeli\n",
        "    \"\"\"\n",
        "    log.info(f\"Loading 2-class classifier from {model_dir}...\")\n",
        "    \n",
        "    # Convert to Path and ensure absolute path for local files\n",
        "    model_dir_path = Path(model_dir).absolute()\n",
        "    model_dir_str = str(model_dir_path)\n",
        "    \n",
        "    # Load PEFT config by reading adapter_config.json directly\n",
        "    adapter_config_path = model_dir_path / \"adapter_config.json\"\n",
        "    if not adapter_config_path.exists():\n",
        "        raise FileNotFoundError(f\"adapter_config.json not found in {model_dir_path}\")\n",
        "    \n",
        "    with open(adapter_config_path) as f:\n",
        "        adapter_config = json.load(f)\n",
        "    \n",
        "    base_name = adapter_config.get(\"base_model_name_or_path\")\n",
        "    if not base_name:\n",
        "        raise ValueError(\"base_model_name_or_path not found in adapter_config.json\")\n",
        "    \n",
        "    # Hard-code 2-class system based on the trained adapter\n",
        "    # The adapter has shape [2, 4096] which confirms 2 classes\n",
        "    id2label = {\n",
        "        0: \"Pro-Palestinian\",\n",
        "        1: \"Pro-Israeli\",\n",
        "    }\n",
        "    label2id = {v: k for k, v in id2label.items()}\n",
        "    num_labels = 2\n",
        "    \n",
        "    log.info(f\"Using 2-class mapping: {id2label}\")\n",
        "    log.info(f\"Base model: {base_name}\")\n",
        "    \n",
        "    # Load config - only pass num_labels and id2label if they're valid\n",
        "    cfg = AutoConfig.from_pretrained(\n",
        "        base_name,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "    )\n",
        "    \n",
        "    # Setup quantization (same as training)\n",
        "    bnb_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=(\n",
        "            torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "        ),\n",
        "    )\n",
        "    \n",
        "    # Determine attention implementation (use flash_attention_2 if available)\n",
        "    try:\n",
        "        import flash_attn\n",
        "        attn_impl = \"flash_attention_2\"\n",
        "        log.info(\"Using Flash Attention 2 for faster inference\")\n",
        "    except ImportError:\n",
        "        attn_impl = None  # Use default attention\n",
        "        log.info(\"Flash Attention 2 not available, using default attention\")\n",
        "    \n",
        "    # Load base model\n",
        "    log.info(f\"Loading base model: {base_name}\")\n",
        "    model_kwargs = {\n",
        "        \"config\": cfg,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"quantization_config\": bnb_cfg,\n",
        "    }\n",
        "    if attn_impl:\n",
        "        model_kwargs[\"attn_implementation\"] = attn_impl\n",
        "    \n",
        "    base = AutoModelForSequenceClassification.from_pretrained(\n",
        "        base_name,\n",
        "        **model_kwargs\n",
        "    )\n",
        "    \n",
        "    # Load tokenizer from base model (NOT from adapter directory)\n",
        "    # PEFT/LoRA adapters don't contain tokenizers - always use base model tokenizer\n",
        "    log.info(f\"Loading tokenizer from base model: {base_name}\")\n",
        "    tok = AutoTokenizer.from_pretrained(base_name, padding_side=\"left\")\n",
        "    if tok.pad_token_id is None:\n",
        "        tok.add_special_tokens({'pad_token': '<pad>'})\n",
        "        tok.pad_token = '<pad>'\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.pad_token_id = tok.pad_token_id\n",
        "    \n",
        "    # Load PEFT adapter from local directory\n",
        "    model_dir_str = str(model_dir_path.absolute())\n",
        "\n",
        "    # If this path is wrong / not mounted, PEFT will treat it like a Hub repo id and crash.\n",
        "    if not model_dir_path.exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"Adapter folder does not exist: {model_dir_str}\\n\"\n",
        "            \"Make sure Drive is mounted and MODEL_DIR points to the adapter directory.\"\n",
        "        )\n",
        "    if not (model_dir_path / \"adapter_config.json\").exists():\n",
        "        raise FileNotFoundError(\n",
        "            f\"adapter_config.json not found in: {model_dir_str}\\n\"\n",
        "            \"MODEL_DIR must point to the PEFT adapter folder.\"\n",
        "        )\n",
        "    if not (\n",
        "        (model_dir_path / \"adapter_model.safetensors\").exists()\n",
        "        or (model_dir_path / \"adapter_model.bin\").exists()\n",
        "        or (model_dir_path / \"adapter_model.pt\").exists()\n",
        "    ):\n",
        "        log.warning(\n",
        "            \"No adapter_model.(safetensors|bin|pt) found in %s. \"\n",
        "            \"If your adapter files are named differently, PEFT may not load them.\",\n",
        "            model_dir_str,\n",
        "        )\n",
        "\n",
        "    log.info(f\"Loading PEFT adapter from local path: {model_dir_str}\")\n",
        "    try:\n",
        "        model = PeftModel.from_pretrained(base, model_dir_str, local_files_only=True)\n",
        "    except RuntimeError as e:\n",
        "        log.warning(f\"LoRA head incompatible ({e}) → using ignore_mismatched_sizes=True\")\n",
        "        model = PeftModel.from_pretrained(\n",
        "            base,\n",
        "            model_dir_str,\n",
        "            ignore_mismatched_sizes=True,\n",
        "            local_files_only=True,\n",
        "        )\n",
        "    \n",
        "    model.eval()\n",
        "    model.config.pad_token_id = tok.pad_token_id\n",
        "    \n",
        "    # Build prompt function (same as training)\n",
        "    cats_str = \", \".join(id2label.values())\n",
        "    def build_prompt(txt: str) -> str:\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are an expert assistant. \"\n",
        "                    \"Classify the following text into one of these \"\n",
        "                    f\"categories: {cats_str}. \"\n",
        "                    \"Respond with the category label only.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": txt},\n",
        "        ]\n",
        "        return tok.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        ).strip()\n",
        "    \n",
        "    return tok, model, build_prompt, id2label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def incremental_predict(\n",
        "    df: pd.DataFrame,\n",
        "    tok,\n",
        "    model,\n",
        "    build_prompt,\n",
        "    id2label: dict,\n",
        "    *,\n",
        "    text_col: str = CONSTRUCTED_COL,\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "    save_every: int = BATCH_SAVE_EVERY,\n",
        "    out_path: str | Path | None = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Predict categories for texts with 2-class scores (Pro-Palestinian and Pro-Israeli).\n",
        "    Saves both predicted category and individual class scores.\n",
        "    \"\"\"\n",
        "    import torch.nn.functional as F\n",
        "    \n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Find rows to process\n",
        "    to_process = df.index[df[PRED_COL].isna() | (df[PRED_COL] == \"\")].tolist()\n",
        "    if not to_process:\n",
        "        log.info(\"No rows to categorize (already complete).\")\n",
        "        return\n",
        "    \n",
        "    total_batches = math.ceil(len(to_process) / batch_size)\n",
        "    batch_counter = 0\n",
        "    \n",
        "    # Ensure score columns exist\n",
        "    if \"score_pro_palestinian\" not in df.columns:\n",
        "        df[\"score_pro_palestinian\"] = pd.NA\n",
        "    if \"score_pro_israeli\" not in df.columns:\n",
        "        df[\"score_pro_israeli\"] = pd.NA\n",
        "    \n",
        "    log.info(f\"Processing {len(to_process)} rows in {total_batches} batches...\")\n",
        "    \n",
        "    for i in tqdm(\n",
        "        range(0, len(to_process), batch_size),\n",
        "        desc=\"Batch-predict\",\n",
        "        total=total_batches,\n",
        "        unit=\"batch\",\n",
        "    ):\n",
        "        batch_idx = to_process[i : i + batch_size]\n",
        "        batch_texts = df.loc[batch_idx, text_col].apply(strip_invisible).tolist()\n",
        "        prompts = [build_prompt(t) for t in batch_texts]\n",
        "        \n",
        "        # Tokenize\n",
        "        enc = tok(\n",
        "            prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "            add_special_tokens=False,  # Important: same as training\n",
        "        )\n",
        "        enc = {k: v.to(device) for k, v in enc.items()}\n",
        "        \n",
        "        # Predict\n",
        "        with torch.no_grad():\n",
        "            logits = model(**enc).logits\n",
        "            # Convert logits to probabilities using softmax\n",
        "            probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "        \n",
        "        # Get predicted class (argmax)\n",
        "        ids = probs.argmax(axis=1).tolist()\n",
        "        labels = [id2label[i] for i in ids]\n",
        "        df.loc[batch_idx, PRED_COL] = labels\n",
        "        \n",
        "        # Store individual class scores\n",
        "        df.loc[batch_idx, \"score_pro_palestinian\"] = probs[:, 0]  # Class 0\n",
        "        df.loc[batch_idx, \"score_pro_israeli\"] = probs[:, 1]  # Class 1\n",
        "        \n",
        "        batch_counter += 1\n",
        "        if out_path and batch_counter % save_every == 0:\n",
        "            log.info(f\"Interim save → {out_path}\")\n",
        "            df.to_csv(out_path, index=False)\n",
        "    \n",
        "    if out_path:\n",
        "        log.info(f\"Final save → {out_path}\")\n",
        "        df.to_csv(out_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Classification Pipeline\n",
        "\n",
        "Run the classification by executing the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare paths\n",
        "input_csv = Path(INPUT_CSV)\n",
        "if not input_csv.exists():\n",
        "    raise FileNotFoundError(f\"Input CSV not found: {input_csv}\")\n",
        "\n",
        "if OUTPUT_CSV is None:\n",
        "    output_csv = input_csv.parent / f\"{input_csv.stem}_classified{input_csv.suffix}\"\n",
        "else:\n",
        "    output_csv = Path(OUTPUT_CSV)\n",
        "\n",
        "print(f\"Input CSV: {input_csv}\")\n",
        "print(f\"Output CSV: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load or create dataframe\n",
        "if output_csv.exists():\n",
        "    log.info(f\"Found existing output ({output_csv}) – resume mode.\")\n",
        "    df = safe_read_csv(output_csv)\n",
        "    if CONSTRUCTED_COL not in df.columns:\n",
        "        raw = safe_read_csv(input_csv)\n",
        "        df_texts = process_facebook(raw)[[CONSTRUCTED_COL]]\n",
        "        df = df.join(df_texts)\n",
        "else:\n",
        "    log.info(f\"Loading raw CSV: {input_csv}\")\n",
        "    df = process_facebook(safe_read_csv(input_csv))\n",
        "    df[PRED_COL] = pd.NA\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Language filtering (if enabled)\n",
        "if FILTER_ENGLISH:\n",
        "    mask_uncat = df[PRED_COL].isna() | (df[PRED_COL] == \"\")\n",
        "    to_check = df.loc[mask_uncat, CONSTRUCTED_COL].tolist()\n",
        "    if to_check:\n",
        "        log.info(f\"Language filtering ({len(to_check)} rows)...\")\n",
        "        flags = filter_english(to_check)\n",
        "        df = df.loc[\n",
        "            ~mask_uncat | pd.Series(flags, index=df.loc[mask_uncat].index)\n",
        "        ].reset_index(drop=True)\n",
        "        print(f\"After language filtering: {len(df)} rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tok, model, build_prompt, id2label = load_classifier(MODEL_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run predictions\n",
        "incremental_predict(\n",
        "    df,\n",
        "    tok,\n",
        "    model,\n",
        "    build_prompt,\n",
        "    id2label,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    save_every=BATCH_SAVE_EVERY,\n",
        "    out_path=output_csv,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results summary\n",
        "log.info(f\"Completed! Results saved to {output_csv}\")\n",
        "log.info(f\"Total rows: {len(df)}\")\n",
        "\n",
        "if PRED_COL in df.columns:\n",
        "    counts = df[PRED_COL].value_counts()\n",
        "    log.info(\"Classification summary:\")\n",
        "    for cat, count in counts.items():\n",
        "        log.info(f\"  {cat}: {count} ({count/len(df)*100:.1f}%)\")\n",
        "    \n",
        "    # Also display as a nice table\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Classification Summary\")\n",
        "    print(\"=\"*50)\n",
        "    display(counts.to_frame(\"Count\").assign(Percentage=lambda x: (x['Count'] / len(df) * 100).round(1)))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
